{
  
    
        "post0": {
            "title": "Worldwide Natural Gas Reserve Data Analysis.",
            "content": "import requests import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns url = &#39;https://www.worldometers.info/gas/&#39; html = requests.get(url).content df_list = pd.read_html(html) Oil_dataset = df_list[-1] print(Oil_dataset) Oil_dataset.to_csv(&#39;my data.csv&#39;) . # Country Gas Reserves (MMcf) World Share 0 1 Russia 1688228000 24.3% 1 2 Iran 1201382000 17.3% 2 3 Qatar 871585000 12.5% 3 4 United States 368704000 5.3% 4 5 Saudi Arabia 294205000 4.2% .. .. ... ... ... 94 95 Benin 40000 0.0006% 95 96 Greece 35000 0.0005% 96 97 DR Congo 35000 0.0005% 97 98 Albania 29000 0.0004% 98 99 Barbados 5000 0.0001% [99 rows x 4 columns] . Oil_dataset.columns . Index([&#39;#&#39;, &#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;, &#39;World Share&#39;], dtype=&#39;object&#39;) . Oil_dataset.head() . # Country Gas Reserves (MMcf) World Share . 0 1 | Russia | 1688228000 | 24.3% | . 1 2 | Iran | 1201382000 | 17.3% | . 2 3 | Qatar | 871585000 | 12.5% | . 3 4 | United States | 368704000 | 5.3% | . 4 5 | Saudi Arabia | 294205000 | 4.2% | . Oil_dataset.tail() . # Country Gas Reserves (MMcf) World Share . 94 95 | Benin | 40000 | 0.0006% | . 95 96 | Greece | 35000 | 0.0005% | . 96 97 | DR Congo | 35000 | 0.0005% | . 97 98 | Albania | 29000 | 0.0004% | . 98 99 | Barbados | 5000 | 0.0001% | . Oil_dataset.shape . (99, 3) . y = Oil_dataset[&quot;Gas Reserves (MMcf)&quot;] . . TopTen_GasReserve = y.head(10) . TopTen_GasReserve . 0 1688228000 1 1201382000 2 871585000 3 368704000 4 294205000 5 265000000 6 215098000 7 197087000 8 180490000 9 163959000 Name: Gas Reserves (MMcf), dtype: int64 . PercentShare = Oil_dataset[&quot;World Share&quot;] . PercentShare = x.head(10) . PercentShare . 0 24.3% 1 17.3% 2 12.5% 3 5.3% 4 4.2% 5 3.8% 6 3.1% 7 2.8% 8 2.6% 9 2.4% Name: World Share, dtype: object . TopCountries = Oil_dataset[&quot;Country&quot;] . TopTenCountries = TopCountries.head(10) . TopTenCountries . 0 Russia 1 Iran 2 Qatar 3 United States 4 Saudi Arabia 5 Turkmenistan 6 United Arab Emirates 7 Venezuela 8 Nigeria 9 China Name: Country, dtype: object . Let&#39;s split the dataset into 2 different dataset for analysis. . 1 . Countries Per their Gas reserve 2. Countries pee their percentile Gas reserve . CtryPerShare = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;World Share&#39;]) CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerReserve = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;]) . CtryPerReserve . Country Gas Reserves (MMcf) . 0 Russia | 1688228000 | . 1 Iran | 1201382000 | . 2 Qatar | 871585000 | . 3 United States | 368704000 | . 4 Saudi Arabia | 294205000 | . 5 Turkmenistan | 265000000 | . 6 United Arab Emirates | 215098000 | . 7 Venezuela | 197087000 | . 8 Nigeria | 180490000 | . 9 China | 163959000 | . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerReserve[&#39;Country&#39;], CtryPerReserve[&quot;Gas Reserves (MMcf)&quot;]) . CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerShare.dtypes . Country object World Share object dtype: object . s = CtryPerShare[&#39;World Share&#39;].str.replace(r&#39;%&#39;, r&#39;&#39;).astype(&#39;float&#39;)/100 CtryPerShare[&#39;World Share&#39;] = pd.to_numeric(CtryPerShare[&#39;World Share&#39;], errors=&#39;coerce&#39;).fillna(s) . CtryPerShare . Country World Share . 0 Russia | 0.243 | . 1 Iran | 0.173 | . 2 Qatar | 0.125 | . 3 United States | 0.053 | . 4 Saudi Arabia | 0.042 | . 5 Turkmenistan | 0.038 | . 6 United Arab Emirates | 0.031 | . 7 Venezuela | 0.028 | . 8 Nigeria | 0.026 | . 9 China | 0.024 | . fig = plt.figure(figsize = (10, 7)) plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&quot;Country&quot;], shadow = True) plt.show() . C: Users Admin AppData Local Temp/ipykernel_3308/3530909758.py:4: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&#34;Country&#34;], shadow = True) . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerShare[&#39;Country&#39;], CtryPerShare[&quot;World Share&quot;]) . name = CtryPerShare[&#39;Country&#39;] price = CtryPerShare[&#39;World Share&#39;] # Figure Size fig, ax = plt.subplots(figsize =(16, 9)) # Horizontal Bar Plot ax.barh(name, price) #Remove axes splines for s in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[s].set_visible(False) #Remove x, y Ticks ax.xaxis.set_ticks_position(&#39;none&#39;) ax.yaxis.set_ticks_position(&#39;none&#39;) # Add padding between axes and labels ax.xaxis.set_tick_params(pad = 5) ax.yaxis.set_tick_params(pad = 10) #Add x, y gridlines ax.grid(b = True, color =&#39;grey&#39;, linestyle =&#39;-.&#39;, linewidth = 0.5, alpha = 0.2) # Show top values ax.invert_yaxis() # Add annotation to bars for i in ax.patches: plt.text(i.get_width(), i.get_y()+0.5, str(round((i.get_width()), 2)), fontsize = 10, fontweight =&#39;bold&#39;, color =&#39;grey&#39;) #Add Plot Title ax.set_title(&#39;Top 10 Countries Per % Natural Gas Reseve &#39;, loc =&#39;left&#39;, ) # # # Add Text watermark # fig.text(0.9, 0.15, &#39;Jeeteshgavande30&#39;, fontsize = 12, # color =&#39;grey&#39;, ha =&#39;right&#39;, va =&#39;bottom&#39;, # alpha = 0.7) # Show Plot plt.show() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/03/12/Worldwide-Natural-Gas-Reserve.html",
            "relUrl": "/2022/03/12/Worldwide-Natural-Gas-Reserve.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Worldwide Natural Gas Reserve Data Analysis.",
            "content": "import requests import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns url = &#39;https://www.worldometers.info/gas/&#39; html = requests.get(url).content df_list = pd.read_html(html) Oil_dataset = df_list[-1] print(Oil_dataset) Oil_dataset.to_csv(&#39;my data.csv&#39;) . # Country Gas Reserves (MMcf) World Share 0 1 Russia 1688228000 24.3% 1 2 Iran 1201382000 17.3% 2 3 Qatar 871585000 12.5% 3 4 United States 368704000 5.3% 4 5 Saudi Arabia 294205000 4.2% .. .. ... ... ... 94 95 Benin 40000 0.0006% 95 96 Greece 35000 0.0005% 96 97 DR Congo 35000 0.0005% 97 98 Albania 29000 0.0004% 98 99 Barbados 5000 0.0001% [99 rows x 4 columns] . Oil_dataset.columns . Index([&#39;#&#39;, &#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;, &#39;World Share&#39;], dtype=&#39;object&#39;) . Oil_dataset.head() . # Country Gas Reserves (MMcf) World Share . 0 1 | Russia | 1688228000 | 24.3% | . 1 2 | Iran | 1201382000 | 17.3% | . 2 3 | Qatar | 871585000 | 12.5% | . 3 4 | United States | 368704000 | 5.3% | . 4 5 | Saudi Arabia | 294205000 | 4.2% | . Oil_dataset.tail() . # Country Gas Reserves (MMcf) World Share . 94 95 | Benin | 40000 | 0.0006% | . 95 96 | Greece | 35000 | 0.0005% | . 96 97 | DR Congo | 35000 | 0.0005% | . 97 98 | Albania | 29000 | 0.0004% | . 98 99 | Barbados | 5000 | 0.0001% | . Oil_dataset.shape . (99, 3) . y = Oil_dataset[&quot;Gas Reserves (MMcf)&quot;] . . TopTen_GasReserve = y.head(10) . TopTen_GasReserve . 0 1688228000 1 1201382000 2 871585000 3 368704000 4 294205000 5 265000000 6 215098000 7 197087000 8 180490000 9 163959000 Name: Gas Reserves (MMcf), dtype: int64 . PercentShare = Oil_dataset[&quot;World Share&quot;] . PercentShare = x.head(10) . PercentShare . 0 24.3% 1 17.3% 2 12.5% 3 5.3% 4 4.2% 5 3.8% 6 3.1% 7 2.8% 8 2.6% 9 2.4% Name: World Share, dtype: object . TopCountries = Oil_dataset[&quot;Country&quot;] . TopTenCountries = TopCountries.head(10) . TopTenCountries . 0 Russia 1 Iran 2 Qatar 3 United States 4 Saudi Arabia 5 Turkmenistan 6 United Arab Emirates 7 Venezuela 8 Nigeria 9 China Name: Country, dtype: object . Let&#39;s split the dataset into 2 different dataset for analysis. . 1 . Countries Per their Gas reserve 2. Countries pee their percentile Gas reserve . CtryPerShare = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;World Share&#39;]) CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerReserve = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;]) . CtryPerReserve . Country Gas Reserves (MMcf) . 0 Russia | 1688228000 | . 1 Iran | 1201382000 | . 2 Qatar | 871585000 | . 3 United States | 368704000 | . 4 Saudi Arabia | 294205000 | . 5 Turkmenistan | 265000000 | . 6 United Arab Emirates | 215098000 | . 7 Venezuela | 197087000 | . 8 Nigeria | 180490000 | . 9 China | 163959000 | . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerReserve[&#39;Country&#39;], CtryPerReserve[&quot;Gas Reserves (MMcf)&quot;]) . CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerShare.dtypes . Country object World Share object dtype: object . s = CtryPerShare[&#39;World Share&#39;].str.replace(r&#39;%&#39;, r&#39;&#39;).astype(&#39;float&#39;)/100 CtryPerShare[&#39;World Share&#39;] = pd.to_numeric(CtryPerShare[&#39;World Share&#39;], errors=&#39;coerce&#39;).fillna(s) . CtryPerShare . Country World Share . 0 Russia | 0.243 | . 1 Iran | 0.173 | . 2 Qatar | 0.125 | . 3 United States | 0.053 | . 4 Saudi Arabia | 0.042 | . 5 Turkmenistan | 0.038 | . 6 United Arab Emirates | 0.031 | . 7 Venezuela | 0.028 | . 8 Nigeria | 0.026 | . 9 China | 0.024 | . fig = plt.figure(figsize = (10, 7)) plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&quot;Country&quot;], shadow = True) plt.show() . C: Users Admin AppData Local Temp/ipykernel_3308/3530909758.py:4: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&#34;Country&#34;], shadow = True) . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerShare[&#39;Country&#39;], CtryPerShare[&quot;World Share&quot;]) . name = CtryPerShare[&#39;Country&#39;] price = CtryPerShare[&#39;World Share&#39;] # Figure Size fig, ax = plt.subplots(figsize =(16, 9)) # Horizontal Bar Plot ax.barh(name, price) #Remove axes splines for s in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[s].set_visible(False) #Remove x, y Ticks ax.xaxis.set_ticks_position(&#39;none&#39;) ax.yaxis.set_ticks_position(&#39;none&#39;) # Add padding between axes and labels ax.xaxis.set_tick_params(pad = 5) ax.yaxis.set_tick_params(pad = 10) #Add x, y gridlines ax.grid(b = True, color =&#39;grey&#39;, linestyle =&#39;-.&#39;, linewidth = 0.5, alpha = 0.2) # Show top values ax.invert_yaxis() # Add annotation to bars for i in ax.patches: plt.text(i.get_width(), i.get_y()+0.5, str(round((i.get_width()), 2)), fontsize = 10, fontweight =&#39;bold&#39;, color =&#39;grey&#39;) #Add Plot Title ax.set_title(&#39;Top 10 Countries Per % Natural Gas Reseve &#39;, loc =&#39;left&#39;, ) # # # Add Text watermark # fig.text(0.9, 0.15, &#39;Jeeteshgavande30&#39;, fontsize = 12, # color =&#39;grey&#39;, ha =&#39;right&#39;, va =&#39;bottom&#39;, # alpha = 0.7) # Show Plot plt.show() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/02/28/Worldwide-Natural-Gas-Reserve.html",
            "relUrl": "/2022/02/28/Worldwide-Natural-Gas-Reserve.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "We will be performing EDA on FIFA 2018 DATASET. We will be using both SQL and Python combination to do the analysis in Jupyter Notebook. The right, right from Jupyter IDE. . To be able to run SQL in Jupyter Notebook, we need to download the necessary modules and access the dataset from a local database. . The Dataset we are using can be accessed from: https://public.tableau.com/en-us/s/resources . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import pyodbc conn_str = ( r&#39;DRIVER={SQL Server};&#39; r&#39;SERVER=SEIFEDIN SQLEXPRESS;&#39; r&#39;DATABASE=Fifa 18;&#39; r&#39;Trusted_Connection=yes;&#39; ) cnxn = pyodbc.connect(conn_str) query = &quot;SELECT * FROM Fifa18&quot; cursor = cnxn.cursor() # cursor.execute(&quot;SELECT * FROM Fifa18&quot;) df = pd.read_sql(query, cnxn) df . Wage (€) Value (€) Name Age Photo Nationality Flag Overall Potential Club ... RB RCB RCM RDM RF RM RS RW RWB ST . 0 565000.0 | 95500000.0 | Cristiano Ronaldo | 32.0 | https://cdn.sofifa.org/48/18/players/20801.png | Portugal | https://cdn.sofifa.org/flags/38.png | 94.0 | 94.0 | Real Madrid CF | ... | 61.0 | 53.0 | 82.0 | 62.0 | 91.0 | 89.0 | 92.0 | 91.0 | 66.0 | 92.0 | . 1 565000.0 | 105000000.0 | L. Messi | 30.0 | https://cdn.sofifa.org/48/18/players/158023.png | Argentina | https://cdn.sofifa.org/flags/52.png | 93.0 | 93.0 | FC Barcelona | ... | 57.0 | 45.0 | 84.0 | 59.0 | 92.0 | 90.0 | 88.0 | 91.0 | 62.0 | 88.0 | . 2 280000.0 | 123000000.0 | Neymar | 25.0 | https://cdn.sofifa.org/48/18/players/190871.png | Brazil | https://cdn.sofifa.org/flags/54.png | 92.0 | 94.0 | Paris Saint-Germain | ... | 59.0 | 46.0 | 79.0 | 59.0 | 88.0 | 87.0 | 84.0 | 89.0 | 64.0 | 84.0 | . 3 510000.0 | 97000000.0 | L. Suárez | 30.0 | https://cdn.sofifa.org/48/18/players/176580.png | Uruguay | https://cdn.sofifa.org/flags/60.png | 92.0 | 92.0 | FC Barcelona | ... | 64.0 | 58.0 | 80.0 | 65.0 | 88.0 | 85.0 | 88.0 | 87.0 | 68.0 | 88.0 | . 4 230000.0 | 61000000.0 | M. Neuer | 31.0 | https://cdn.sofifa.org/48/18/players/167495.png | Germany | https://cdn.sofifa.org/flags/21.png | 92.0 | 92.0 | FC Bayern Munich | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 17976 5000.0 | 625000.0 | A. Muric | 18.0 | https://cdn.sofifa.org/48/18/players/233164.png | Switzerland | https://cdn.sofifa.org/flags/47.png | 63.0 | 81.0 | Manchester City | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 17977 4000.0 | 375000.0 | W. Atkinson | 28.0 | https://cdn.sofifa.org/48/18/players/186572.png | England | https://cdn.sofifa.org/flags/14.png | 63.0 | 63.0 | Mansfield Town | ... | 60.0 | 57.0 | 59.0 | 60.0 | 59.0 | 62.0 | 58.0 | 60.0 | 62.0 | 58.0 | . 17978 4000.0 | 675000.0 | M. Beerman | 18.0 | https://cdn.sofifa.org/48/18/players/233165.png | Malta | https://cdn.sofifa.org/flags/32.png | 63.0 | 80.0 | Rangers | ... | 62.0 | 57.0 | 56.0 | 58.0 | 53.0 | 59.0 | 48.0 | 58.0 | 63.0 | 48.0 | . 17979 4000.0 | 625000.0 | P. Burner | 21.0 | https://cdn.sofifa.org/48/18/players/235981.png | France | https://cdn.sofifa.org/flags/18.png | 63.0 | 76.0 | OGC Nice | ... | 62.0 | 60.0 | 60.0 | 62.0 | 56.0 | 60.0 | 53.0 | 58.0 | 63.0 | 53.0 | . 17980 4000.0 | 220000.0 | A. Al Mazaidi | 31.0 | https://cdn.sofifa.org/48/18/players/191437.png | Saudi Arabia | https://cdn.sofifa.org/flags/183.png | 63.0 | 63.0 | Al Fateh | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 17981 rows × 74 columns . df.head(5) . Wage (€) Value (€) Name Age Photo Nationality Flag Overall Potential Club ... RB RCB RCM RDM RF RM RS RW RWB ST . 0 565000.0 | 95500000.0 | Cristiano Ronaldo | 32.0 | https://cdn.sofifa.org/48/18/players/20801.png | Portugal | https://cdn.sofifa.org/flags/38.png | 94.0 | 94.0 | Real Madrid CF | ... | 61.0 | 53.0 | 82.0 | 62.0 | 91.0 | 89.0 | 92.0 | 91.0 | 66.0 | 92.0 | . 1 565000.0 | 105000000.0 | L. Messi | 30.0 | https://cdn.sofifa.org/48/18/players/158023.png | Argentina | https://cdn.sofifa.org/flags/52.png | 93.0 | 93.0 | FC Barcelona | ... | 57.0 | 45.0 | 84.0 | 59.0 | 92.0 | 90.0 | 88.0 | 91.0 | 62.0 | 88.0 | . 2 280000.0 | 123000000.0 | Neymar | 25.0 | https://cdn.sofifa.org/48/18/players/190871.png | Brazil | https://cdn.sofifa.org/flags/54.png | 92.0 | 94.0 | Paris Saint-Germain | ... | 59.0 | 46.0 | 79.0 | 59.0 | 88.0 | 87.0 | 84.0 | 89.0 | 64.0 | 84.0 | . 3 510000.0 | 97000000.0 | L. Suárez | 30.0 | https://cdn.sofifa.org/48/18/players/176580.png | Uruguay | https://cdn.sofifa.org/flags/60.png | 92.0 | 92.0 | FC Barcelona | ... | 64.0 | 58.0 | 80.0 | 65.0 | 88.0 | 85.0 | 88.0 | 87.0 | 68.0 | 88.0 | . 4 230000.0 | 61000000.0 | M. Neuer | 31.0 | https://cdn.sofifa.org/48/18/players/167495.png | Germany | https://cdn.sofifa.org/flags/21.png | 92.0 | 92.0 | FC Bayern Munich | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 74 columns . Let&#39;s check how many countries are represented in the dataset. . Nationality = &quot;SELECT COUNT(DISTINCT Nationality) AS Nationality FROM Fifa18&quot; df= pd.read_sql(Nationality, cnxn) df . Nationality . 0 165 | . Let&#39;s chech how many players are in the FIFA . Num_players = &quot;SELECT COUNT(*) AS Num_Of_Players FROM Fifa18&quot; df= pd.read_sql(Num_players, cnxn) df . Num_Of_Players . 0 17981 | . There are 17981 players in the FIFA . Find out how many players are from each country... . NumOfPlayerPerCuntry = &quot;SELECT COUNT(*) AS NumOfPlayerPerCuntry, Nationality FROM Fifa18 GROUP BY Nationality ORDER BY NumOfPlayerPerCuntry DESC&quot; df= pd.read_sql(NumOfPlayerPerCuntry, cnxn) df.head(10) . NumOfPlayerPerCuntry Nationality . 0 1630 | England | . 1 1140 | Germany | . 2 1019 | Spain | . 3 978 | France | . 4 965 | Argentina | . 5 812 | Brazil | . 6 799 | Italy | . 7 592 | Colombia | . 8 469 | Japan | . 9 429 | Netherlands | . What&#39;s the highest Salary paid to a player? . Highest_Salary = &quot;SELECT MAX([Wage (€)]) AS Max_Salary FROM Fifa18&quot; df= pd.read_sql(Highest_Salary, cnxn) df . Max_Salary . 0 565000.0 | . Which player is getting paid the highest and where is he from? . HighestPaidPlayer = &quot;SELECT Name, Nationality FROM Fifa18 WHERE [Wage (€)] = 565000&quot; df= pd.read_sql(HighestPaidPlayer, cnxn) df . Name Nationality . 0 Cristiano Ronaldo | Portugal | . 1 L. Messi | Argentina | . It appears we have ties. The two GOATS(CR7 and L.Messi Make the same amount) . HighestPaidPlayer = &quot;SELECT Name, [Wage (€)], Nationality FROM Fifa18 WHERE [Wage (€)] = (SELECT MAX([Wage (€)]) FROM Fifa18)&quot; df= pd.read_sql(HighestPaidPlayer, cnxn) df . Name Wage (€) Nationality . 0 Cristiano Ronaldo | 565000.0 | Portugal | . 1 L. Messi | 565000.0 | Argentina | . What&#39;s the minimum Salary paid? . Min_Salary = &quot;SELECT Min([Wage (€)]) FROM Fifa18&quot; df= pd.read_sql(Min_Salary, cnxn) df . . 0 0.0 | . It appears the are some people who are just volunteers. . Which player is got the highest rating and from which club? USE THE NESTED QUERY METHOD FOR THIS. . Top_player = &quot;SELECT Name, Overall, Club FROM Fifa18 WHERE Overall = (SELECT MAX(Overall) FROM Fifa18)&quot; df = pd.read_sql(Top_player, cnxn) df . Name Overall Club . 0 Cristiano Ronaldo | 94.0 | Real Madrid CF | . Cristiano Ronaldo is the highest rated player in FIFA . Which are top 3 clubs based on overall rating ? . Top_Clubs = &quot;SELECT TOP 4 AVG(Overall) AS Avg_Overall_Rating, Club FROM Fifa18 GROUP BY Club ORDER BY AVG(Overall) DESC&quot; df = pd.read_sql(Top_Clubs, cnxn) df . Avg_Overall_Rating Club . 0 82.560000 | FC Barcelona | . 1 81.653846 | Juventus | . 2 81.038462 | Real Madrid CF | . 3 79.423077 | FC Bayern Munich | . We can even combine with Viz from here. We can run both Matplotlib or Seaborn right here at the same time . fig = plt.figure() ax = fig.add_axes([5,5,1,1]) x_ax = df[&quot;Club&quot;] y_ax = df[&quot;Avg_Overall_Rating&quot;] ax.bar(x_ax, y_ax) ax.set_xlabel(&quot;Clubs&quot;) ax.set_ylabel(&quot;Overall Rating&quot;) ax.set_title(&quot;Highest Rated Clubs in Fifa&quot;) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . IF YOU WANT TO USE JUST SQL TO PERFORM YOUR EDA, BELOW IS THE CODE. . /* We will be performing Explatory Data Analysis on a FIFA 2018 DataSet. . DateSet can be found here if further research/Exploration needs to be done. . Data Source: https://public.tableau.com/en-us/s/resources . */ . -- Check our dataset. . SELECT * . FROM . Fifa18 . --Check how many countries are represented . SELECT COUNT(DISTINCT Nationality) . FROM . DBO.Fifa18 . --Check the number of players in FIFA . SELECT COUNT(*) as Number_of_Players FROM Fifa18 . --How many players from each country. Check the top 5 . SELECT TOP 5 COUNT(*) AS NumOfPlayer_FromEach_Country, Nationality . FROM Fifa18 . GROUP BY nationality ORDER BY NumOfPlayer_FromEach_Country DESC . --England appears to have the most players in FIFA. . --What&#39;s the highest Salary paid to a player . SELECT . MAX([Wage (€)]) AS Maximum_Salary . FROM Fifa18 . -- Which player is getting paid the highest and where is he from? . SELECT Name, Nationality . FROM Fifa18 . WHERE [Wage (€)] = 565000 --Interestingly, we have two players getting the highest salary. The two goats(CR7, and Messi) . --We can also do it this way. Nesting a query within a query(Outer query) . SELECT name, [Wage (€)] . FROM Fifa18 WHERE [Wage (€)] = (SELECT max([Wage (€)]) FROM Fifa18) . What&#39;s the minimum Salary paid? . SELECT min([Wage (€)]) FROM Fifa18 . --It appears some people work in FIFA volunterly. . --Which player is got the highest rating and from which club? USE THE NESTED QUERY METHOD FOR THIS. . SELECT NAME, OVERALL, CLUB FROM Fifa18 WHERE OVERALL = (SELECT MAX(OVERALL) FROM Fifa18) --My main man Critiano Ronaldo(CR7) Comes on top. . --Which are top 3 clubs based on overall rating ? . SELECT TOP 5 AVG(overall) as Average_Overall_rating, CLUB FROM Fifa18 GROUP BY CLUB ORDER BY Average_Overall_rating DESC .",
            "url": "https://seifhh.github.io/DS_Projects/2022/01/23/SQL-IN-PYTHON.html",
            "relUrl": "/2022/01/23/SQL-IN-PYTHON.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "COVID-19 WORLWIDE DATASET ANALYSIS.",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt print(&quot;Modules are imported&quot;) . Modules are imported . corona_dataset_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set/covid19_Confirmed_dataset.csv&quot;) . corona_dataset_csv.head(100) #Let&#39;s check what our data looks like. . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 NaN | Djibouti | 11.8251 | 42.5903 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 945 | 974 | 986 | 999 | 1008 | 1023 | 1035 | 1072 | 1077 | 1089 | . 96 NaN | Dominican Republic | 18.7357 | -70.1627 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5044 | 5300 | 5543 | 5749 | 5926 | 6135 | 6293 | 6416 | 6652 | 6972 | . 97 NaN | Ecuador | -1.8312 | -78.1834 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10398 | 10850 | 11183 | 22719 | 22719 | 22719 | 23240 | 24258 | 24675 | 24934 | . 98 NaN | Egypt | 26.0000 | 30.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3490 | 3659 | 3891 | 4092 | 4319 | 4534 | 4782 | 5042 | 5268 | 5537 | . 99 NaN | El Salvador | 13.7942 | -88.8965 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 225 | 237 | 250 | 274 | 274 | 298 | 323 | 345 | 377 | 395 | . 100 rows × 104 columns . corona_dataset_csv.shape #Always good to check the shape( #of rows and column) of our data_set. . (266, 104) . . Adjusted_df = corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;], axis = 1) #Using drop method, we can drop the two col we don&#39;t need. Make sure to identify the axis. . Adjusted_df . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 261 NaN | Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . 262 NaN | Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8 | 14 | . 263 NaN | Yemen | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . 264 NaN | Comoros | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 265 NaN | Tajikistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | . 266 rows × 102 columns . # Using drop method will not delete the two column from the original data set. It just won&#39;t show in our new data from. to remove it from the original data set, # we can do as follows: corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;],axis = 1,inplace = True) #Inplace will drop the two unwatnted col. . corona_dataset_csv.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . 5 rows × 102 columns . Aggregating the rows by country: . Instead of having multiple data from the same country based on the province/region, we can combine them and get a signle dataset for each country. . corona_dataset_aggregated= corona_dataset_csv.groupby(&#39;Country/Region&#39;).sum() # This will group data from each region and sum up the total to result in a single output for each country corona_dataset_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 466 | 474 | 480 | 484 | 342 | 342 | 342 | 343 | 344 | 344 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 70 | 74 | 76 | 84 | 84 | 88 | 88 | 95 | 97 | 106 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 29 | 31 | 31 | 32 | 32 | 32 | 40 | . 187 rows × 100 columns . corona_dataset_aggregated.shape . (187, 100) . . Performing Visualisations . corona_dataset_aggregated.loc[&#39;Australia&#39;] #Showing data for Australia . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 4 ... 4/26/20 6714 4/27/20 6721 4/28/20 6744 4/29/20 6752 4/30/20 6766 Name: Australia, Length: 100, dtype: int64 . . corona_dataset_aggregated.loc[&#39;Australia&#39;].plot() . &lt;AxesSubplot:&gt; . corona_dataset_aggregated.loc[&#39;China&#39;].plot() corona_dataset_aggregated.loc[&#39;Italy&#39;].plot() corona_dataset_aggregated.loc[&#39;Spain&#39;].plot() plt.legend() #This will add the legend to make it easy to identify . &lt;matplotlib.legend.Legend at 0x2985f665f10&gt; . Calculating a good measure to do the analysis. Let&#39;s findout the spread of the virus in each country . . corona_dataset_aggregated.loc[&#39;China&#39;][:7].plot() . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;Australia&#39;].diff().plot() #This will show the change in infection rate day by day. . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;China&#39;].diff().max() #This is 24 hours change in China. . 15136.0 . corona_dataset_aggregated.loc[&#39;Italy&#39;].diff().max() . 6557.0 . corona_dataset_aggregated.loc[&#39;Spain&#39;].diff().max() . 9630.0 . #Since the indexes of our dataset is a list of coutries,we can do as follow: countries = list(corona_dataset_aggregated.index) #Create an empty list and append the result of each countries infection rate into the new list max_infection_rates = [] for c in countries: max_infection_rates.append(corona_dataset_aggregated.loc[c].diff().max()) max_infection_rates . [232.0, 34.0, 199.0, 43.0, 5.0, 6.0, 291.0, 134.0, 497.0, 1321.0, 105.0, 7.0, 301.0, 641.0, 12.0, 1485.0, 2454.0, 4.0, 19.0, 1.0, 104.0, 92.0, 7.0, 7502.0, 26.0, 137.0, 41.0, 21.0, 6.0, 45.0, 31.0, 203.0, 2778.0, 31.0, 21.0, 1138.0, 15136.0, 353.0, 1.0, 57.0, 81.0, 37.0, 113.0, 96.0, 63.0, 58.0, 381.0, 391.0, 99.0, 156.0, 5.0, 371.0, 11536.0, 269.0, 32.0, 130.0, 7.0, 134.0, 20.0, 9.0, 5.0, 267.0, 26849.0, 38.0, 5.0, 42.0, 6933.0, 403.0, 156.0, 6.0, 68.0, 167.0, 132.0, 12.0, 10.0, 3.0, 72.0, 210.0, 99.0, 1893.0, 436.0, 3186.0, 91.0, 1515.0, 1131.0, 6557.0, 52.0, 1161.0, 40.0, 264.0, 29.0, 851.0, 289.0, 300.0, 69.0, 3.0, 48.0, 61.0, 17.0, 13.0, 21.0, 90.0, 234.0, 7.0, 14.0, 10.0, 235.0, 190.0, 58.0, 52.0, 2.0, 41.0, 1425.0, 222.0, 12.0, 13.0, 30.0, 281.0, 19.0, 3.0, 14.0, 1346.0, 89.0, 2.0, 69.0, 208.0, 107.0, 386.0, 144.0, 1292.0, 357.0, 5.0, 27.0, 3683.0, 538.0, 545.0, 1516.0, 957.0, 523.0, 7099.0, 22.0, 5.0, 6.0, 4.0, 54.0, 6.0, 1351.0, 87.0, 2379.0, 2.0, 20.0, 1426.0, 114.0, 70.0, 73.0, 354.0, 28.0, 9630.0, 65.0, 67.0, 3.0, 812.0, 1321.0, 6.0, 27.0, 15.0, 181.0, 188.0, 10.0, 14.0, 40.0, 82.0, 5138.0, 36188.0, 11.0, 578.0, 552.0, 8733.0, 48.0, 167.0, 29.0, 19.0, 66.0, 4.0, 5.0, 9.0, 8.0] . . corona_dataset_aggregated[&#39;max_infection_rate&#39;] = max_infection_rates . corona_dataset_aggregated.columns #Our new col is included in the end . Index([&#39;1/22/20&#39;, &#39;1/23/20&#39;, &#39;1/24/20&#39;, &#39;1/25/20&#39;, &#39;1/26/20&#39;, &#39;1/27/20&#39;, &#39;1/28/20&#39;, &#39;1/29/20&#39;, &#39;1/30/20&#39;, &#39;1/31/20&#39;, ... &#39;4/22/20&#39;, &#39;4/23/20&#39;, &#39;4/24/20&#39;, &#39;4/25/20&#39;, &#39;4/26/20&#39;, &#39;4/27/20&#39;, &#39;4/28/20&#39;, &#39;4/29/20&#39;, &#39;4/30/20&#39;, &#39;max_infection_rate&#39;], dtype=&#39;object&#39;, length=101) . corona_dataset_aggregated.index . Index([&#39;Afghanistan&#39;, &#39;Albania&#39;, &#39;Algeria&#39;, &#39;Andorra&#39;, &#39;Angola&#39;, &#39;Antigua and Barbuda&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;, &#39;Austria&#39;, ... &#39;United Kingdom&#39;, &#39;Uruguay&#39;, &#39;Uzbekistan&#39;, &#39;Venezuela&#39;, &#39;Vietnam&#39;, &#39;West Bank and Gaza&#39;, &#39;Western Sahara&#39;, &#39;Yemen&#39;, &#39;Zambia&#39;, &#39;Zimbabwe&#39;], dtype=&#39;object&#39;, name=&#39;Country/Region&#39;, length=187) . corona_dataset_aggregated.head() . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 max_infection_rate . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | 232.0 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | 34.0 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | 199.0 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | 43.0 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | 5.0 | . 5 rows × 101 columns . . corona_data = pd.DataFrame(corona_dataset_aggregated[&#39;max_infection_rate&#39;]) corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . . happiness_report_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set worldwide_happiness_report.csv&quot;) . happiness_report_csv.head() . Overall rank Country or region Score GDP per capita Social support Healthy life expectancy Freedom to make life choices Generosity Perceptions of corruption . 0 1 | Finland | 7.769 | 1.340 | 1.587 | 0.986 | 0.596 | 0.153 | 0.393 | . 1 2 | Denmark | 7.600 | 1.383 | 1.573 | 0.996 | 0.592 | 0.252 | 0.410 | . 2 3 | Norway | 7.554 | 1.488 | 1.582 | 1.028 | 0.603 | 0.271 | 0.341 | . 3 4 | Iceland | 7.494 | 1.380 | 1.624 | 1.026 | 0.591 | 0.354 | 0.118 | . 4 5 | Netherlands | 7.488 | 1.396 | 1.522 | 0.999 | 0.557 | 0.322 | 0.298 | . happiness_report_csv.head() . Overall rank Country or region Score GDP per capita Social support Healthy life expectancy Freedom to make life choices Generosity Perceptions of corruption . 0 1 | Finland | 7.769 | 1.340 | 1.587 | 0.986 | 0.596 | 0.153 | 0.393 | . 1 2 | Denmark | 7.600 | 1.383 | 1.573 | 0.996 | 0.592 | 0.252 | 0.410 | . 2 3 | Norway | 7.554 | 1.488 | 1.582 | 1.028 | 0.603 | 0.271 | 0.341 | . 3 4 | Iceland | 7.494 | 1.380 | 1.624 | 1.026 | 0.591 | 0.354 | 0.118 | . 4 5 | Netherlands | 7.488 | 1.396 | 1.522 | 0.999 | 0.557 | 0.322 | 0.298 | . . useless_cols = [&quot;Overall rank&quot;, &quot;Score&quot;,&quot;Generosity&quot;, &quot;Perceptions of corruption&quot;] . happiness_report_csv.drop(useless_cols, axis = 1, inplace = True) . happiness_report_csv.head() . Country or region GDP per capita Social support Healthy life expectancy Freedom to make life choices . 0 Finland | 1.340 | 1.587 | 0.986 | 0.596 | . 1 Denmark | 1.383 | 1.573 | 0.996 | 0.592 | . 2 Norway | 1.488 | 1.582 | 1.028 | 0.603 | . 3 Iceland | 1.380 | 1.624 | 1.026 | 0.591 | . 4 Netherlands | 1.396 | 1.522 | 0.999 | 0.557 | . . happiness_report_csv.set_index(&quot;Country or region&quot;, inplace = True) . happiness_report_csv.head() . GDP per capita Social support Healthy life expectancy Freedom to make life choices . Country or region . Finland 1.340 | 1.587 | 0.986 | 0.596 | . Denmark 1.383 | 1.573 | 0.996 | 0.592 | . Norway 1.488 | 1.582 | 1.028 | 0.603 | . Iceland 1.380 | 1.624 | 1.026 | 0.591 | . Netherlands 1.396 | 1.522 | 0.999 | 0.557 | . Now let&#39;s join the world happiness dataset with the corona dataset.* . corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . corona_data.shape . (187, 1) . happiness_report_csv . GDP per capita Social support Healthy life expectancy Freedom to make life choices . Country or region . Finland 1.340 | 1.587 | 0.986 | 0.596 | . Denmark 1.383 | 1.573 | 0.996 | 0.592 | . Norway 1.488 | 1.582 | 1.028 | 0.603 | . Iceland 1.380 | 1.624 | 1.026 | 0.591 | . Netherlands 1.396 | 1.522 | 0.999 | 0.557 | . ... ... | ... | ... | ... | . Rwanda 0.359 | 0.711 | 0.614 | 0.555 | . Tanzania 0.476 | 0.885 | 0.499 | 0.417 | . Afghanistan 0.350 | 0.517 | 0.361 | 0.000 | . Central African Republic 0.026 | 0.000 | 0.105 | 0.225 | . South Sudan 0.306 | 0.575 | 0.295 | 0.010 | . 156 rows × 4 columns . happiness_report_csv.shape . (156, 4) . Since the number of countries in the happiness dataset is smaller than that of Corona dataset, we need to perform inner join to combine them on country col. . data = corona_data.join(happiness_report_csv, how = &quot;inner&quot;) #Performing inner join. data.head() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . Afghanistan 232.0 | 0.350 | 0.517 | 0.361 | 0.000 | . Albania 34.0 | 0.947 | 0.848 | 0.874 | 0.383 | . Algeria 199.0 | 1.002 | 1.160 | 0.785 | 0.086 | . Argentina 291.0 | 1.092 | 1.432 | 0.881 | 0.471 | . Armenia 134.0 | 0.850 | 1.055 | 0.815 | 0.283 | . Let&#39;s check if there&#39;s any correlation between the factors in the above dataset. . data.corr() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . max_infection_rate 1.000000 | 0.250118 | 0.191958 | 0.289263 | 0.078196 | . GDP per capita 0.250118 | 1.000000 | 0.759468 | 0.863062 | 0.394603 | . Social support 0.191958 | 0.759468 | 1.000000 | 0.765286 | 0.456246 | . Healthy life expectancy 0.289263 | 0.863062 | 0.765286 | 1.000000 | 0.427892 | . Freedom to make life choices 0.078196 | 0.394603 | 0.456246 | 0.427892 | 1.000000 | . Now let&#39;s visualize the correlation and the bigger picture. Better way to do this is using seaborn. . data.head() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . Afghanistan 232.0 | 0.350 | 0.517 | 0.361 | 0.000 | . Albania 34.0 | 0.947 | 0.848 | 0.874 | 0.383 | . Algeria 199.0 | 1.002 | 1.160 | 0.785 | 0.086 | . Argentina 291.0 | 1.092 | 1.432 | 0.881 | 0.471 | . Armenia 134.0 | 0.850 | 1.055 | 0.815 | 0.283 | . Let&#39;s plot GDP vs Maximum infection rate. . x = data[&quot;GDP per capita&quot;] y = data[&quot;max_infection_rate&quot;] sns.scatterplot(x = x, y = y) . &lt;AxesSubplot:xlabel=&#39;GDP per capita&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . . sns.scatterplot(x = x, y = np.log(y)) #This will apply log scaling into the y-axis. . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . sns.regplot(x = x, y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . . x = data[&quot;Healthy life expectancy&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x = x,y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . x = data[&quot;Social support&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x = x, y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Social support&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . x = data[&quot;Freedom to make life choices&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x=x, y= np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Freedom to make life choices&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . Surprisingly, there seems to be a positive correlation in developed country than the rest, which indicates, people in a developed countries are prone to catching Covid than those in the developing country. In order to prove this, let&#39;s compare the result to the confirmed death dataset . covid_death_dataset = pd.read_csv(&quot;D: Data Science Covid_Data_Set covid19_deaths_dataset.csv&quot;) . covid_death_dataset.head(10) . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . 5 NaN | Antigua and Barbuda | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 6 NaN | Argentina | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 147 | 152 | 165 | 176 | 185 | 192 | 197 | 207 | 214 | 218 | . 7 NaN | Armenia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 24 | 24 | 27 | 28 | 28 | 29 | 30 | 30 | 32 | . 8 Australian Capital Territory | Australia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 9 New South Wales | Australia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 26 | 31 | 33 | 33 | 34 | 34 | 39 | 40 | 41 | . 10 rows × 102 columns . covid_death_dataset.shape . (266, 104) . covid_death_dataset.drop([&quot;Lat&quot;, &quot;Long&quot;], axis = 1, inplace = True) . covid_death_dataset.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . 5 rows × 102 columns . Let&#39;s perform some cleanup on death dataset. . covid_death_dataset.shape . (266, 102) . Let&#39;s aggregate data into a country. . covid_death_aggregated = covid_death_dataset.groupby(&quot;Country/Region&quot;).sum() covid_death_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 2 | 2 | 2 | 2 | 2 | 2 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | . 187 rows × 100 columns . covid_death_aggregated.shape . (187, 100) . . Covid_death_aggregated.loc[&quot;Australia&quot;] . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 0 .. 4/26/20 83 4/27/20 83 4/28/20 89 4/29/20 91 4/30/20 93 Name: Australia, Length: 100, dtype: int64 . Let&#39;s perform visualisations on death dataset. . Covid_death_aggregated.loc[&quot;Australia&quot;].plot() . &lt;AxesSubplot:&gt; . Covid_death_aggregated.loc[&quot;Australia&quot;].plot() Covid_death_aggregated.loc[&quot;Spain&quot;].plot() Covid_death_aggregated.loc[&quot;China&quot;].plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x2986139b850&gt; . Let&#39;s calculate change in death rate . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().plot() . &lt;AxesSubplot:&gt; . Let&#39;s calculate the max death rate in 24 hrs . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().max() . 8.0 . Covid_death_aggregated.loc[&quot;Germany&quot;].diff().max() . 510.0 . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().max() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/01/22/COVID-19-Data-Analysis.html",
            "relUrl": "/2022/01/22/COVID-19-Data-Analysis.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "COVID-19 WORLWIDE DATASET ANALYSIS.",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt print(&quot;Modules are imported&quot;) . Modules are imported . corona_dataset_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set/covid19_Confirmed_dataset.csv&quot;) . corona_dataset_csv.head(100) #Let&#39;s check what our data looks like. . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 NaN | Djibouti | 11.8251 | 42.5903 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 945 | 974 | 986 | 999 | 1008 | 1023 | 1035 | 1072 | 1077 | 1089 | . 96 NaN | Dominican Republic | 18.7357 | -70.1627 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5044 | 5300 | 5543 | 5749 | 5926 | 6135 | 6293 | 6416 | 6652 | 6972 | . 97 NaN | Ecuador | -1.8312 | -78.1834 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10398 | 10850 | 11183 | 22719 | 22719 | 22719 | 23240 | 24258 | 24675 | 24934 | . 98 NaN | Egypt | 26.0000 | 30.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3490 | 3659 | 3891 | 4092 | 4319 | 4534 | 4782 | 5042 | 5268 | 5537 | . 99 NaN | El Salvador | 13.7942 | -88.8965 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 225 | 237 | 250 | 274 | 274 | 298 | 323 | 345 | 377 | 395 | . 100 rows × 104 columns . corona_dataset_csv.shape #Always good to check the shape( #of rows and column) of our data_set. . (266, 104) . . Adjusted_df = corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;], axis = 1) #Using drop method, we can drop the two col we don&#39;t need. Make sure to identify the axis. . Adjusted_df . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 261 NaN | Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . 262 NaN | Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8 | 14 | . 263 NaN | Yemen | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . 264 NaN | Comoros | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 265 NaN | Tajikistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | . 266 rows × 102 columns . # Using drop method will not delete the two column from the original data set. It just won&#39;t show in our new data from. to remove it from the original data set, # we can do as follows: corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;],axis = 1,inplace = True) #Inplace will drop the two unwatnted col. . corona_dataset_csv.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . 5 rows × 102 columns . Aggregating the rows by country: . Instead of having multiple data from the same country based on the province/region, we can combine them and get a signle dataset for each country. . corona_dataset_aggregated= corona_dataset_csv.groupby(&#39;Country/Region&#39;).sum() # This will group data from each region and sum up the total to result in a single output for each country corona_dataset_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 466 | 474 | 480 | 484 | 342 | 342 | 342 | 343 | 344 | 344 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 70 | 74 | 76 | 84 | 84 | 88 | 88 | 95 | 97 | 106 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 29 | 31 | 31 | 32 | 32 | 32 | 40 | . 187 rows × 100 columns . corona_dataset_aggregated.shape . (187, 100) . . Performing Visualisations . corona_dataset_aggregated.loc[&#39;Australia&#39;] #Showing data for Australia . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 4 ... 4/26/20 6714 4/27/20 6721 4/28/20 6744 4/29/20 6752 4/30/20 6766 Name: Australia, Length: 100, dtype: int64 . . corona_dataset_aggregated.loc[&#39;Australia&#39;].plot() . &lt;AxesSubplot:&gt; . corona_dataset_aggregated.loc[&#39;China&#39;].plot() corona_dataset_aggregated.loc[&#39;Italy&#39;].plot() corona_dataset_aggregated.loc[&#39;Spain&#39;].plot() plt.legend() #This will add the legend to make it easy to identify . &lt;matplotlib.legend.Legend at 0x2985f665f10&gt; . Calculating a good measure to do the analysis. Let&#39;s findout the spread of the virus in each country . . corona_dataset_aggregated.loc[&#39;China&#39;][:7].plot() . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;Australia&#39;].diff().plot() #This will show the change in infection rate day by day. . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;China&#39;].diff().max() #This is 24 hours change in China. . 15136.0 . corona_dataset_aggregated.loc[&#39;Italy&#39;].diff().max() . 6557.0 . corona_dataset_aggregated.loc[&#39;Spain&#39;].diff().max() . 9630.0 . #Since the indexes of our dataset is a list of coutries,we can do as follow: countries = list(corona_dataset_aggregated.index) #Create an empty list and append the result of each countries infection rate into the new list max_infection_rates = [] for c in countries: max_infection_rates.append(corona_dataset_aggregated.loc[c].diff().max()) max_infection_rates . [232.0, 34.0, 199.0, 43.0, 5.0, 6.0, 291.0, 134.0, 497.0, 1321.0, 105.0, 7.0, 301.0, 641.0, 12.0, 1485.0, 2454.0, 4.0, 19.0, 1.0, 104.0, 92.0, 7.0, 7502.0, 26.0, 137.0, 41.0, 21.0, 6.0, 45.0, 31.0, 203.0, 2778.0, 31.0, 21.0, 1138.0, 15136.0, 353.0, 1.0, 57.0, 81.0, 37.0, 113.0, 96.0, 63.0, 58.0, 381.0, 391.0, 99.0, 156.0, 5.0, 371.0, 11536.0, 269.0, 32.0, 130.0, 7.0, 134.0, 20.0, 9.0, 5.0, 267.0, 26849.0, 38.0, 5.0, 42.0, 6933.0, 403.0, 156.0, 6.0, 68.0, 167.0, 132.0, 12.0, 10.0, 3.0, 72.0, 210.0, 99.0, 1893.0, 436.0, 3186.0, 91.0, 1515.0, 1131.0, 6557.0, 52.0, 1161.0, 40.0, 264.0, 29.0, 851.0, 289.0, 300.0, 69.0, 3.0, 48.0, 61.0, 17.0, 13.0, 21.0, 90.0, 234.0, 7.0, 14.0, 10.0, 235.0, 190.0, 58.0, 52.0, 2.0, 41.0, 1425.0, 222.0, 12.0, 13.0, 30.0, 281.0, 19.0, 3.0, 14.0, 1346.0, 89.0, 2.0, 69.0, 208.0, 107.0, 386.0, 144.0, 1292.0, 357.0, 5.0, 27.0, 3683.0, 538.0, 545.0, 1516.0, 957.0, 523.0, 7099.0, 22.0, 5.0, 6.0, 4.0, 54.0, 6.0, 1351.0, 87.0, 2379.0, 2.0, 20.0, 1426.0, 114.0, 70.0, 73.0, 354.0, 28.0, 9630.0, 65.0, 67.0, 3.0, 812.0, 1321.0, 6.0, 27.0, 15.0, 181.0, 188.0, 10.0, 14.0, 40.0, 82.0, 5138.0, 36188.0, 11.0, 578.0, 552.0, 8733.0, 48.0, 167.0, 29.0, 19.0, 66.0, 4.0, 5.0, 9.0, 8.0] . . corona_dataset_aggregated[&#39;max_infection_rate&#39;] = max_infection_rates . corona_dataset_aggregated.columns #Our new col is included in the end . Index([&#39;1/22/20&#39;, &#39;1/23/20&#39;, &#39;1/24/20&#39;, &#39;1/25/20&#39;, &#39;1/26/20&#39;, &#39;1/27/20&#39;, &#39;1/28/20&#39;, &#39;1/29/20&#39;, &#39;1/30/20&#39;, &#39;1/31/20&#39;, ... &#39;4/22/20&#39;, &#39;4/23/20&#39;, &#39;4/24/20&#39;, &#39;4/25/20&#39;, &#39;4/26/20&#39;, &#39;4/27/20&#39;, &#39;4/28/20&#39;, &#39;4/29/20&#39;, &#39;4/30/20&#39;, &#39;max_infection_rate&#39;], dtype=&#39;object&#39;, length=101) . corona_dataset_aggregated.index . Index([&#39;Afghanistan&#39;, &#39;Albania&#39;, &#39;Algeria&#39;, &#39;Andorra&#39;, &#39;Angola&#39;, &#39;Antigua and Barbuda&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;, &#39;Austria&#39;, ... &#39;United Kingdom&#39;, &#39;Uruguay&#39;, &#39;Uzbekistan&#39;, &#39;Venezuela&#39;, &#39;Vietnam&#39;, &#39;West Bank and Gaza&#39;, &#39;Western Sahara&#39;, &#39;Yemen&#39;, &#39;Zambia&#39;, &#39;Zimbabwe&#39;], dtype=&#39;object&#39;, name=&#39;Country/Region&#39;, length=187) . corona_dataset_aggregated.head() . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 max_infection_rate . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | 232.0 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | 34.0 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | 199.0 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | 43.0 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | 5.0 | . 5 rows × 101 columns . . corona_data = pd.DataFrame(corona_dataset_aggregated[&#39;max_infection_rate&#39;]) corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . print(len(corona_dataset_csv)) . 266 .",
            "url": "https://seifhh.github.io/DS_Projects/2021/05/28/Data-Analysis.html",
            "relUrl": "/2021/05/28/Data-Analysis.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://seifhh.github.io/DS_Projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Used Car  Data Analysis",
            "content": ". import pandas as pd #Two dimentional table import numpy as np #Array and metrices import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . path = (r&quot;D: Data Science Python IBM Data_Analysis automobileEDA.csv&quot;) df = pd.read_csv(path) . df.head(5) . symboling normalized-losses make aspiration num-of-doors body-style drive-wheels engine-location wheel-base length ... compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km horsepower-binned diesel gas . 0 3 | 122 | alfa-romero | std | two | convertible | rwd | front | 88.6 | 0.811148 | ... | 9.0 | 111.0 | 5000.0 | 21 | 27 | 13495.0 | 11.190476 | Medium | 0 | 1 | . 1 3 | 122 | alfa-romero | std | two | convertible | rwd | front | 88.6 | 0.811148 | ... | 9.0 | 111.0 | 5000.0 | 21 | 27 | 16500.0 | 11.190476 | Medium | 0 | 1 | . 2 1 | 122 | alfa-romero | std | two | hatchback | rwd | front | 94.5 | 0.822681 | ... | 9.0 | 154.0 | 5000.0 | 19 | 26 | 16500.0 | 12.368421 | Medium | 0 | 1 | . 3 2 | 164 | audi | std | four | sedan | fwd | front | 99.8 | 0.848630 | ... | 10.0 | 102.0 | 5500.0 | 24 | 30 | 13950.0 | 9.791667 | Medium | 0 | 1 | . 4 2 | 164 | audi | std | four | sedan | 4wd | front | 99.4 | 0.848630 | ... | 8.0 | 115.0 | 5500.0 | 18 | 22 | 17450.0 | 13.055556 | Medium | 0 | 1 | . 5 rows × 29 columns . Analyzing Individual Feature Patterns Using Visualization . df.dtypes . symboling int64 normalized-losses int64 make object aspiration object num-of-doors object body-style object drive-wheels object engine-location object wheel-base float64 length float64 width float64 height float64 curb-weight int64 engine-type object num-of-cylinders object engine-size int64 fuel-system object bore float64 stroke float64 compression-ratio float64 horsepower float64 peak-rpm float64 city-mpg int64 highway-mpg int64 price float64 city-L/100km float64 horsepower-binned object diesel int64 gas int64 dtype: object . df.dtypes[[&quot;peak-rpm&quot;, &quot;horsepower&quot;, &quot;engine-type&quot;]] . peak-rpm float64 horsepower float64 engine-type object dtype: object . Calculate the correlation between variables of type &quot;int64&quot; or &quot;float64&quot; using the method &quot;corr&quot;: . df.corr() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . symboling 1.000000 | 0.466264 | -0.535987 | -0.365404 | -0.242423 | -0.550160 | -0.233118 | -0.110581 | -0.140019 | -0.008245 | -0.182196 | 0.075819 | 0.279740 | -0.035527 | 0.036233 | -0.082391 | 0.066171 | -0.196735 | 0.196735 | . normalized-losses 0.466264 | 1.000000 | -0.056661 | 0.019424 | 0.086802 | -0.373737 | 0.099404 | 0.112360 | -0.029862 | 0.055563 | -0.114713 | 0.217299 | 0.239543 | -0.225016 | -0.181877 | 0.133999 | 0.238567 | -0.101546 | 0.101546 | . wheel-base -0.535987 | -0.056661 | 1.000000 | 0.876024 | 0.814507 | 0.590742 | 0.782097 | 0.572027 | 0.493244 | 0.158502 | 0.250313 | 0.371147 | -0.360305 | -0.470606 | -0.543304 | 0.584642 | 0.476153 | 0.307237 | -0.307237 | . length -0.365404 | 0.019424 | 0.876024 | 1.000000 | 0.857170 | 0.492063 | 0.880665 | 0.685025 | 0.608971 | 0.124139 | 0.159733 | 0.579821 | -0.285970 | -0.665192 | -0.698142 | 0.690628 | 0.657373 | 0.211187 | -0.211187 | . width -0.242423 | 0.086802 | 0.814507 | 0.857170 | 1.000000 | 0.306002 | 0.866201 | 0.729436 | 0.544885 | 0.188829 | 0.189867 | 0.615077 | -0.245800 | -0.633531 | -0.680635 | 0.751265 | 0.673363 | 0.244356 | -0.244356 | . height -0.550160 | -0.373737 | 0.590742 | 0.492063 | 0.306002 | 1.000000 | 0.307581 | 0.074694 | 0.180449 | -0.062704 | 0.259737 | -0.087027 | -0.309974 | -0.049800 | -0.104812 | 0.135486 | 0.003811 | 0.281578 | -0.281578 | . curb-weight -0.233118 | 0.099404 | 0.782097 | 0.880665 | 0.866201 | 0.307581 | 1.000000 | 0.849072 | 0.644060 | 0.167562 | 0.156433 | 0.757976 | -0.279361 | -0.749543 | -0.794889 | 0.834415 | 0.785353 | 0.221046 | -0.221046 | . engine-size -0.110581 | 0.112360 | 0.572027 | 0.685025 | 0.729436 | 0.074694 | 0.849072 | 1.000000 | 0.572609 | 0.209523 | 0.028889 | 0.822676 | -0.256733 | -0.650546 | -0.679571 | 0.872335 | 0.745059 | 0.070779 | -0.070779 | . bore -0.140019 | -0.029862 | 0.493244 | 0.608971 | 0.544885 | 0.180449 | 0.644060 | 0.572609 | 1.000000 | -0.055390 | 0.001263 | 0.566936 | -0.267392 | -0.582027 | -0.591309 | 0.543155 | 0.554610 | 0.054458 | -0.054458 | . stroke -0.008245 | 0.055563 | 0.158502 | 0.124139 | 0.188829 | -0.062704 | 0.167562 | 0.209523 | -0.055390 | 1.000000 | 0.187923 | 0.098462 | -0.065713 | -0.034696 | -0.035201 | 0.082310 | 0.037300 | 0.241303 | -0.241303 | . compression-ratio -0.182196 | -0.114713 | 0.250313 | 0.159733 | 0.189867 | 0.259737 | 0.156433 | 0.028889 | 0.001263 | 0.187923 | 1.000000 | -0.214514 | -0.435780 | 0.331425 | 0.268465 | 0.071107 | -0.299372 | 0.985231 | -0.985231 | . horsepower 0.075819 | 0.217299 | 0.371147 | 0.579821 | 0.615077 | -0.087027 | 0.757976 | 0.822676 | 0.566936 | 0.098462 | -0.214514 | 1.000000 | 0.107885 | -0.822214 | -0.804575 | 0.809575 | 0.889488 | -0.169053 | 0.169053 | . peak-rpm 0.279740 | 0.239543 | -0.360305 | -0.285970 | -0.245800 | -0.309974 | -0.279361 | -0.256733 | -0.267392 | -0.065713 | -0.435780 | 0.107885 | 1.000000 | -0.115413 | -0.058598 | -0.101616 | 0.115830 | -0.475812 | 0.475812 | . city-mpg -0.035527 | -0.225016 | -0.470606 | -0.665192 | -0.633531 | -0.049800 | -0.749543 | -0.650546 | -0.582027 | -0.034696 | 0.331425 | -0.822214 | -0.115413 | 1.000000 | 0.972044 | -0.686571 | -0.949713 | 0.265676 | -0.265676 | . highway-mpg 0.036233 | -0.181877 | -0.543304 | -0.698142 | -0.680635 | -0.104812 | -0.794889 | -0.679571 | -0.591309 | -0.035201 | 0.268465 | -0.804575 | -0.058598 | 0.972044 | 1.000000 | -0.704692 | -0.930028 | 0.198690 | -0.198690 | . price -0.082391 | 0.133999 | 0.584642 | 0.690628 | 0.751265 | 0.135486 | 0.834415 | 0.872335 | 0.543155 | 0.082310 | 0.071107 | 0.809575 | -0.101616 | -0.686571 | -0.704692 | 1.000000 | 0.789898 | 0.110326 | -0.110326 | . city-L/100km 0.066171 | 0.238567 | 0.476153 | 0.657373 | 0.673363 | 0.003811 | 0.785353 | 0.745059 | 0.554610 | 0.037300 | -0.299372 | 0.889488 | 0.115830 | -0.949713 | -0.930028 | 0.789898 | 1.000000 | -0.241282 | 0.241282 | . diesel -0.196735 | -0.101546 | 0.307237 | 0.211187 | 0.244356 | 0.281578 | 0.221046 | 0.070779 | 0.054458 | 0.241303 | 0.985231 | -0.169053 | -0.475812 | 0.265676 | 0.198690 | 0.110326 | -0.241282 | 1.000000 | -1.000000 | . gas 0.196735 | 0.101546 | -0.307237 | -0.211187 | -0.244356 | -0.281578 | -0.221046 | -0.070779 | -0.054458 | -0.241303 | -0.985231 | 0.169053 | 0.475812 | -0.265676 | -0.198690 | -0.110326 | 0.241282 | -1.000000 | 1.000000 | . Let&#39;s find the correlation between the following columns: bore, stroke, compression-ratio, and horsepower. . df[[&#39;bore&#39;, &#39;stroke&#39;, &#39;compression-ratio&#39;, &#39;horsepower&#39;]].corr() . bore stroke compression-ratio horsepower . bore 1.000000 | -0.055390 | 0.001263 | 0.566936 | . stroke -0.055390 | 1.000000 | 0.187923 | 0.098462 | . compression-ratio 0.001263 | 0.187923 | 1.000000 | -0.214514 | . horsepower 0.566936 | 0.098462 | -0.214514 | 1.000000 | . #Fitted regression line on the data #Calculate linear relationship between engine-size and pirce . sns.regplot(x = &quot;engine-size&quot;, y = &quot;price&quot;, data = df) plt.ylim(0,) . (0.0, 53653.66212817468) . As the slope of the regression line is positive, this correlation indicate there exist positive relationship between engine-size and price. The higher the engine-size, the higher the price. . df[[&quot;engine-size&quot;, &quot;price&quot;]].corr() . engine-size price . engine-size 1.000000 | 0.872335 | . price 0.872335 | 1.000000 | . . sns.regplot(x = &quot;highway-mpg&quot;, y = &quot;price&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;highway-mpg&#39;, ylabel=&#39;price&#39;&gt; . As highway-mpg goes up, the price goes down: this indicates an inverse/negative relationship between these two variables. Highway mpg could potentially be a predictor of price. Let&#39;s examine the correlation. . df[[&quot;highway-mpg&quot;,&quot;price&quot;]].corr() . highway-mpg price . highway-mpg 1.000000 | -0.704692 | . price -0.704692 | 1.000000 | . As we can see, the correlation between &#39;highway-mpg&#39; and &#39;price&#39; and see it&#39;s approximately -0.704. . . sns.regplot(x = &quot;peak-rpm&quot;, y = &quot;price&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;peak-rpm&#39;, ylabel=&#39;price&#39;&gt; . Peak rpm does not seem like a good predictor of the price at all since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore, it&#39;s not a reliable variable. This is a weak linear relationship. The slope of the regression line is almost the same(zero) at all point and it can&#39;t be used to predictive analysis. . df[[&#39;peak-rpm&#39;,&#39;price&#39;]].corr() . peak-rpm price . peak-rpm 1.000000 | -0.101616 | . price -0.101616 | 1.000000 | . sns.regplot(x = &#39;stroke&#39;, y = &#39;price&#39;, data = df) . &lt;AxesSubplot:xlabel=&#39;stroke&#39;, ylabel=&#39;price&#39;&gt; . df[[&#39;stroke&#39;,&#39;price&#39;]].corr() . stroke price . stroke 1.00000 | 0.08231 | . price 0.08231 | 1.00000 | . . Let&#39;s examine categorical variables. The categorical variables can have the type &quot;object&quot; or &quot;int64&quot;. Best way to visualize this is using boxplot. . sns.boxplot(x=&quot;body-style&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;body-style&#39;, ylabel=&#39;price&#39;&gt; . We see that the distributions of price between the different body-style categories have a significant overlap, so body-style would not be a good predictor of price. Let&#39;s examine engine &quot;engine-location&quot; and &quot;price&quot; . sns.boxplot(x=&quot;engine-location&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;engine-location&#39;, ylabel=&#39;price&#39;&gt; . Here we see that the distribution of price between these two engine-location categories, front and rear, are distinct enough to take engine-location as a potential good predictor of price. . Let&#39;s examine &quot;drive-wheels&quot; and &quot;price&quot; . sns.boxplot(x=&quot;drive-wheels&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;drive-wheels&#39;, ylabel=&#39;price&#39;&gt; . Here we see that the distribution of price between the different drive-wheels categories differs. As such, drive-wheels could potentially be a predictor of price. . Let&#39;s compute descriptive Statistical Analysis. NaN value will be skipped when we do descriptive analysis. . df.describe() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . count 201.000000 | 201.00000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 197.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | . mean 0.840796 | 122.00000 | 98.797015 | 0.837102 | 0.915126 | 53.766667 | 2555.666667 | 126.875622 | 3.330692 | 3.256904 | 10.164279 | 103.405534 | 5117.665368 | 25.179104 | 30.686567 | 13207.129353 | 9.944145 | 0.099502 | 0.900498 | . std 1.254802 | 31.99625 | 6.066366 | 0.059213 | 0.029187 | 2.447822 | 517.296727 | 41.546834 | 0.268072 | 0.319256 | 4.004965 | 37.365700 | 478.113805 | 6.423220 | 6.815150 | 7947.066342 | 2.534599 | 0.300083 | 0.300083 | . min -2.000000 | 65.00000 | 86.600000 | 0.678039 | 0.837500 | 47.800000 | 1488.000000 | 61.000000 | 2.540000 | 2.070000 | 7.000000 | 48.000000 | 4150.000000 | 13.000000 | 16.000000 | 5118.000000 | 4.795918 | 0.000000 | 0.000000 | . 25% 0.000000 | 101.00000 | 94.500000 | 0.801538 | 0.890278 | 52.000000 | 2169.000000 | 98.000000 | 3.150000 | 3.110000 | 8.600000 | 70.000000 | 4800.000000 | 19.000000 | 25.000000 | 7775.000000 | 7.833333 | 0.000000 | 1.000000 | . 50% 1.000000 | 122.00000 | 97.000000 | 0.832292 | 0.909722 | 54.100000 | 2414.000000 | 120.000000 | 3.310000 | 3.290000 | 9.000000 | 95.000000 | 5125.369458 | 24.000000 | 30.000000 | 10295.000000 | 9.791667 | 0.000000 | 1.000000 | . 75% 2.000000 | 137.00000 | 102.400000 | 0.881788 | 0.925000 | 55.500000 | 2926.000000 | 141.000000 | 3.580000 | 3.410000 | 9.400000 | 116.000000 | 5500.000000 | 30.000000 | 34.000000 | 16500.000000 | 12.368421 | 0.000000 | 1.000000 | . max 3.000000 | 256.00000 | 120.900000 | 1.000000 | 1.000000 | 59.800000 | 4066.000000 | 326.000000 | 3.940000 | 4.170000 | 23.000000 | 262.000000 | 6600.000000 | 49.000000 | 54.000000 | 45400.000000 | 18.076923 | 1.000000 | 1.000000 | . df.describe(include=[&#39;object&#39;]) . make aspiration num-of-doors body-style drive-wheels engine-location engine-type num-of-cylinders fuel-system horsepower-binned . count 201 | 201 | 201 | 201 | 201 | 201 | 201 | 201 | 201 | 200 | . unique 22 | 2 | 2 | 5 | 3 | 2 | 6 | 7 | 8 | 3 | . top toyota | std | four | sedan | fwd | front | ohc | four | mpfi | Low | . freq 32 | 165 | 115 | 94 | 118 | 198 | 145 | 157 | 92 | 115 | . Value counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the &quot;value_counts&quot; method on the column &quot;drive-wheels&quot;. N.B, the method &quot;value_counts&quot; only works on pandas series, not pandas dataframes. As a result, we only include one bracket df[&#39;drive-wheels&#39;], not two brackets df[[&#39;drive-wheels&#39;]]. . df[&#39;drive-wheels&#39;].value_counts() . fwd 118 rwd 75 4wd 8 Name: drive-wheels, dtype: int64 . df[&#39;drive-wheels&#39;].value_counts().to_frame() . drive-wheels . fwd 118 | . rwd 75 | . 4wd 8 | . . drive_wheels_counts = df[&#39;drive-wheels&#39;].value_counts().to_frame() drive_wheels_counts.rename(columns={&#39;drive-wheels&#39;: &#39;value_counts&#39;}, inplace=True) drive_wheels_counts . value_counts . fwd 118 | . rwd 75 | . 4wd 8 | . . drive_wheels_counts.index.name = &quot;drive-wheels&quot; drive_wheels_counts . value_counts . drive-wheels . fwd 118 | . rwd 75 | . 4wd 8 | . . engine_loc_counts = df[&#39;engine-location&#39;].value_counts().to_frame() engine_loc_counts.rename(columns={&#39;engine-location&#39;: &#39;value_counts&#39;}, inplace=True) engine_loc_counts.index.name = &#39;engine-location&#39; engine_loc_counts.head() . value_counts . engine-location . front 198 | . rear 3 | . After examining the value counts of the engine location, we see that engine location would not be a good predictor variable for the price. This is because we only have three cars with a rear engine and 198 with an engine in the front, so this result is skewed. Thus, we are not able to draw any conclusions about the engine location. . #Groupby method groups data into different categories. . df[&#39;drive-wheels&#39;].unique() . array([&#39;rwd&#39;, &#39;fwd&#39;, &#39;4wd&#39;], dtype=object) . . . df_group_one = df[[&#39;drive-wheels&#39;,&#39;body-style&#39;,&#39;price&#39;]] df_group_one.head(10) . drive-wheels body-style price . 0 rwd | convertible | 13495.0 | . 1 rwd | convertible | 16500.0 | . 2 rwd | hatchback | 16500.0 | . 3 fwd | sedan | 13950.0 | . 4 4wd | sedan | 17450.0 | . 5 fwd | sedan | 15250.0 | . 6 fwd | sedan | 17710.0 | . 7 fwd | wagon | 18920.0 | . 8 fwd | sedan | 23875.0 | . 9 rwd | sedan | 16430.0 | . df_group_one = df_group_one.groupby([&#39;drive-wheels&#39;],as_index=False).mean() df_group_one . drive-wheels price . 0 4wd | 10241.000000 | . 1 fwd | 9244.779661 | . 2 rwd | 19757.613333 | . From our data, it seems rear-wheel drive vehicles are, on average, the most expensive, while 4-wheel and front-wheel are approximately the same in price. . df_gptest = df[[&#39;drive-wheels&#39;,&#39;body-style&#39;,&#39;price&#39;]] grouped_data = df_gptest.groupby([&#39;drive-wheels&#39;,&#39;body-style&#39;],as_index=False).mean() grouped_data . drive-wheels body-style price . 0 4wd | hatchback | 7603.000000 | . 1 4wd | sedan | 12647.333333 | . 2 4wd | wagon | 9095.750000 | . 3 fwd | convertible | 11595.000000 | . 4 fwd | hardtop | 8249.000000 | . 5 fwd | hatchback | 8396.387755 | . 6 fwd | sedan | 9811.800000 | . 7 fwd | wagon | 9997.333333 | . 8 rwd | convertible | 23949.600000 | . 9 rwd | hardtop | 24202.714286 | . 10 rwd | hatchback | 14337.777778 | . 11 rwd | sedan | 21711.833333 | . 12 rwd | wagon | 16994.222222 | . . grouped_data_pivoted = grouped_data.pivot(index=&#39;drive-wheels&#39;,columns=&#39;body-style&#39;) grouped_data_pivoted . price . body-style convertible hardtop hatchback sedan wagon . drive-wheels . 4wd NaN | NaN | 7603.000000 | 12647.333333 | 9095.750000 | . fwd 11595.0 | 8249.000000 | 8396.387755 | 9811.800000 | 9997.333333 | . rwd 23949.6 | 24202.714286 | 14337.777778 | 21711.833333 | 16994.222222 | . grouped_data_pivoted = grouped_data_pivoted.fillna(0) grouped_data_pivoted . price . body-style convertible hardtop hatchback sedan wagon . drive-wheels . 4wd 0.0 | 0.000000 | 7603.000000 | 12647.333333 | 9095.750000 | . fwd 11595.0 | 8249.000000 | 8396.387755 | 9811.800000 | 9997.333333 | . rwd 23949.6 | 24202.714286 | 14337.777778 | 21711.833333 | 16994.222222 | . df_avg_bodystyle = df[[&#39;body-style&#39;,&#39;price&#39;]] grouped_data_bodystyle = df_avg_bodystyle.groupby([&#39;body-style&#39;],as_index= False).mean() grouped_data_bodystyle . body-style price . 0 convertible | 21890.500000 | . 1 hardtop | 22208.500000 | . 2 hatchback | 9957.441176 | . 3 sedan | 14459.755319 | . 4 wagon | 12371.960000 | . Let&#39;s use a heat map to visualize the relationship between Body Style vs Price. . fig, ax = plt.subplots() im = ax.pcolor(grouped_data_pivoted, cmap=&#39;RdBu&#39;) #label names row_labels = grouped_data_pivoted.columns.levels[1] col_labels = grouped_data_pivoted.index #move ticks and labels to the center ax.set_xticks(np.arange(grouped_data_pivoted.shape[1]) + 0.5, minor=False) ax.set_yticks(np.arange(grouped_data_pivoted.shape[0]) + 0.5, minor=False) #insert labels ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(col_labels, minor=False) #rotate label if too long plt.xticks(rotation=90) fig.colorbar(im) plt.show() . The heatmap plots the target variable (price) proportional to colour with respect to the variables &#39;drive-wheel&#39; and &#39;body-style&#39; on the vertical and horizontal axis, respectively. This allows us to visualize how the price is related to &#39;drive-wheel&#39; and &#39;body-style. . . df.corr() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . symboling 1.000000 | 0.466264 | -0.535987 | -0.365404 | -0.242423 | -0.550160 | -0.233118 | -0.110581 | -0.140019 | -0.008245 | -0.182196 | 0.075819 | 0.279740 | -0.035527 | 0.036233 | -0.082391 | 0.066171 | -0.196735 | 0.196735 | . normalized-losses 0.466264 | 1.000000 | -0.056661 | 0.019424 | 0.086802 | -0.373737 | 0.099404 | 0.112360 | -0.029862 | 0.055563 | -0.114713 | 0.217299 | 0.239543 | -0.225016 | -0.181877 | 0.133999 | 0.238567 | -0.101546 | 0.101546 | . wheel-base -0.535987 | -0.056661 | 1.000000 | 0.876024 | 0.814507 | 0.590742 | 0.782097 | 0.572027 | 0.493244 | 0.158502 | 0.250313 | 0.371147 | -0.360305 | -0.470606 | -0.543304 | 0.584642 | 0.476153 | 0.307237 | -0.307237 | . length -0.365404 | 0.019424 | 0.876024 | 1.000000 | 0.857170 | 0.492063 | 0.880665 | 0.685025 | 0.608971 | 0.124139 | 0.159733 | 0.579821 | -0.285970 | -0.665192 | -0.698142 | 0.690628 | 0.657373 | 0.211187 | -0.211187 | . width -0.242423 | 0.086802 | 0.814507 | 0.857170 | 1.000000 | 0.306002 | 0.866201 | 0.729436 | 0.544885 | 0.188829 | 0.189867 | 0.615077 | -0.245800 | -0.633531 | -0.680635 | 0.751265 | 0.673363 | 0.244356 | -0.244356 | . height -0.550160 | -0.373737 | 0.590742 | 0.492063 | 0.306002 | 1.000000 | 0.307581 | 0.074694 | 0.180449 | -0.062704 | 0.259737 | -0.087027 | -0.309974 | -0.049800 | -0.104812 | 0.135486 | 0.003811 | 0.281578 | -0.281578 | . curb-weight -0.233118 | 0.099404 | 0.782097 | 0.880665 | 0.866201 | 0.307581 | 1.000000 | 0.849072 | 0.644060 | 0.167562 | 0.156433 | 0.757976 | -0.279361 | -0.749543 | -0.794889 | 0.834415 | 0.785353 | 0.221046 | -0.221046 | . engine-size -0.110581 | 0.112360 | 0.572027 | 0.685025 | 0.729436 | 0.074694 | 0.849072 | 1.000000 | 0.572609 | 0.209523 | 0.028889 | 0.822676 | -0.256733 | -0.650546 | -0.679571 | 0.872335 | 0.745059 | 0.070779 | -0.070779 | . bore -0.140019 | -0.029862 | 0.493244 | 0.608971 | 0.544885 | 0.180449 | 0.644060 | 0.572609 | 1.000000 | -0.055390 | 0.001263 | 0.566936 | -0.267392 | -0.582027 | -0.591309 | 0.543155 | 0.554610 | 0.054458 | -0.054458 | . stroke -0.008245 | 0.055563 | 0.158502 | 0.124139 | 0.188829 | -0.062704 | 0.167562 | 0.209523 | -0.055390 | 1.000000 | 0.187923 | 0.098462 | -0.065713 | -0.034696 | -0.035201 | 0.082310 | 0.037300 | 0.241303 | -0.241303 | . compression-ratio -0.182196 | -0.114713 | 0.250313 | 0.159733 | 0.189867 | 0.259737 | 0.156433 | 0.028889 | 0.001263 | 0.187923 | 1.000000 | -0.214514 | -0.435780 | 0.331425 | 0.268465 | 0.071107 | -0.299372 | 0.985231 | -0.985231 | . horsepower 0.075819 | 0.217299 | 0.371147 | 0.579821 | 0.615077 | -0.087027 | 0.757976 | 0.822676 | 0.566936 | 0.098462 | -0.214514 | 1.000000 | 0.107885 | -0.822214 | -0.804575 | 0.809575 | 0.889488 | -0.169053 | 0.169053 | . peak-rpm 0.279740 | 0.239543 | -0.360305 | -0.285970 | -0.245800 | -0.309974 | -0.279361 | -0.256733 | -0.267392 | -0.065713 | -0.435780 | 0.107885 | 1.000000 | -0.115413 | -0.058598 | -0.101616 | 0.115830 | -0.475812 | 0.475812 | . city-mpg -0.035527 | -0.225016 | -0.470606 | -0.665192 | -0.633531 | -0.049800 | -0.749543 | -0.650546 | -0.582027 | -0.034696 | 0.331425 | -0.822214 | -0.115413 | 1.000000 | 0.972044 | -0.686571 | -0.949713 | 0.265676 | -0.265676 | . highway-mpg 0.036233 | -0.181877 | -0.543304 | -0.698142 | -0.680635 | -0.104812 | -0.794889 | -0.679571 | -0.591309 | -0.035201 | 0.268465 | -0.804575 | -0.058598 | 0.972044 | 1.000000 | -0.704692 | -0.930028 | 0.198690 | -0.198690 | . price -0.082391 | 0.133999 | 0.584642 | 0.690628 | 0.751265 | 0.135486 | 0.834415 | 0.872335 | 0.543155 | 0.082310 | 0.071107 | 0.809575 | -0.101616 | -0.686571 | -0.704692 | 1.000000 | 0.789898 | 0.110326 | -0.110326 | . city-L/100km 0.066171 | 0.238567 | 0.476153 | 0.657373 | 0.673363 | 0.003811 | 0.785353 | 0.745059 | 0.554610 | 0.037300 | -0.299372 | 0.889488 | 0.115830 | -0.949713 | -0.930028 | 0.789898 | 1.000000 | -0.241282 | 0.241282 | . diesel -0.196735 | -0.101546 | 0.307237 | 0.211187 | 0.244356 | 0.281578 | 0.221046 | 0.070779 | 0.054458 | 0.241303 | 0.985231 | -0.169053 | -0.475812 | 0.265676 | 0.198690 | 0.110326 | -0.241282 | 1.000000 | -1.000000 | . gas 0.196735 | 0.101546 | -0.307237 | -0.211187 | -0.244356 | -0.281578 | -0.221046 | -0.070779 | -0.054458 | -0.241303 | -0.985231 | 0.169053 | 0.475812 | -0.265676 | -0.198690 | -0.110326 | 0.241282 | -1.000000 | 1.000000 | . Let&#39;s calculate the significant of correlation(P-value) here. Remember, the P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant. p-value is &lt; 0.001: we say there is strong evidence that the correlation is significant. p-value is &lt; 0.05: there is moderate evidence that the correlation is significant. p-value is &lt; 0.1: there is weak evidence that the correlation is significant. p-value is &gt; 0.1: there is no evidence that the correlation is significant. . from scipy import stats . pearson_coef, p_value = stats.pearsonr(df[&#39;wheel-base&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.5846418222655081 with a P-value of P = 8.076488270732989e-20 . Since the p-value is &lt; 0.001, the correlation between wheel-base and price is statistically significant, although the linear relationship isn&#39;t extremely strong (~0.585). . pearson_coef, p_value = stats.pearsonr(df[&#39;horsepower&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.809574567003656 with a P-value of P = 6.369057428259557e-48 . Since the p-value is &lt; 0.001, the correlation between horsepower and price is statistically significant, and the linear relationship is quite strong (~0.809, close to 1). . pearson_coef, p_value = stats.pearsonr(df[&#39;length&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.690628380448364 with a P-value of P = 8.016477466158986e-30 . Since the p-value is &lt; 0.001, the correlation between length and price is statistically significant, and the linear relationship is moderately strong (~0.691). . pearson_coef, p_value = stats.pearsonr(df[&#39;width&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value ) . The Pearson Correlation Coefficient is 0.7512653440522674 with a P-value of P = 9.200335510481516e-38 . Since the p-value is &lt; 0.001, the correlation between width and price is statistically significant, and the linear relationship is quite strong (~0.751). . pearson_coef, p_value = stats.pearsonr(df[&#39;curb-weight&#39;], df[&#39;price&#39;]) print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.8344145257702846 with a P-value of P = 2.1895772388936914e-53 . Since the p-value is &lt; 0.001, the correlation between curb-weight and price is statistically significant, and the linear relationship is quite strong (~0.834). . pearson_coef, p_value = stats.pearsonr(df[&#39;engine-size&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.8723351674455185 with a P-value of P = 9.265491622198389e-64 . Since the p-value is &lt; 0.001, the correlation between engine-size and price is statistically significant, and the linear relationship is very strong (~0.872). . pearson_coef, p_value = stats.pearsonr(df[&#39;bore&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value ) . The Pearson Correlation Coefficient is 0.5431553832626602 with a P-value of P = 8.049189483935489e-17 . Since the p-value is &lt; 0.001, the correlation between bore and price is statistically significant, but the linear relationship is only moderate (~0.521). . pearson_coef, p_value = stats.pearsonr(df[&#39;city-mpg&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is -0.6865710067844677 with a P-value of P = 2.321132065567674e-29 . Since the p-value is &lt; 0.001, the correlation between city-mpg and price is statistically significant, and the coefficient of about -0.687 shows that the relationship is negative and moderately strong. . pearson_coef, p_value = stats.pearsonr(df[&#39;highway-mpg&#39;], df[&#39;price&#39;]) print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value ) . The Pearson Correlation Coefficient is -0.7046922650589529 with a P-value of P = 1.7495471144477352e-31 . Since the p-value is &lt; 0.001, the correlation between highway-mpg and price is statistically significant, and the coefficient of about -0.705 shows that the relationship is negative and moderately strong. . We can use ANOVA(Analysis of Variance) to test whether there are significant differences between the means of two more groups.F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means. . P-value:P-value tells how statistically significant our calculated score value is. If our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to return a sizeable F-test score and a small p-value. . grouped_test2=df_gptest[[&#39;drive-wheels&#39;, &#39;price&#39;]].groupby([&#39;drive-wheels&#39;]) grouped_test2.head(2) . drive-wheels price . 0 rwd | 13495.0 | . 1 rwd | 16500.0 | . 3 fwd | 13950.0 | . 4 4wd | 17450.0 | . 5 fwd | 15250.0 | . 136 4wd | 7603.0 | . df_gptest . drive-wheels body-style price . 0 rwd | convertible | 13495.0 | . 1 rwd | convertible | 16500.0 | . 2 rwd | hatchback | 16500.0 | . 3 fwd | sedan | 13950.0 | . 4 4wd | sedan | 17450.0 | . ... ... | ... | ... | . 196 rwd | sedan | 16845.0 | . 197 rwd | sedan | 19045.0 | . 198 rwd | sedan | 21485.0 | . 199 rwd | sedan | 22470.0 | . 200 rwd | sedan | 22625.0 | . 201 rows × 3 columns . grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;] . 4 17450.0 136 7603.0 140 9233.0 141 11259.0 144 8013.0 145 11694.0 150 7898.0 151 8778.0 Name: price, dtype: float64 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 67.95406500780399 , P = 3.3945443577151245e-23 . This is a great result with a large F-test score showing a strong correlation and a P-value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated? . Let&#39;s examine them separately. . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val ) . ANOVA results: F= 130.5533160959111 , P = 2.2355306355677845e-23 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 8.580681368924756 , P = 0.004411492211225333 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;]) print(&quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 0.665465750252303 , P = 0.41620116697845666 . We now have a better idea of what our data looks like and which variables are important to take into account when predicting the car price. We have narrowed it down to the following variables: . Continuous numerical variables: . Length Width Curb-weight Engine-size Horsepower City-mpg Highway-mpg Wheel-base Bore Categorical variables: . Drive-wheels As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model&#39;s prediction performance. .",
            "url": "https://seifhh.github.io/DS_Projects/2020/01/28/Used-Car-Data-Analysis-with-Python.html",
            "relUrl": "/2020/01/28/Used-Car-Data-Analysis-with-Python.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://seifhh.github.io/DS_Projects/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "An Electrical Engineer, passionate about the field of Data Science and Blockchain Technology. .",
          "url": "https://seifhh.github.io/DS_Projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://seifhh.github.io/DS_Projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}