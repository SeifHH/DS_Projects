{
  
    
        "post0": {
            "title": "Worldwide Natural Gas Reserve Data Analysis.",
            "content": "import requests import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns url = &#39;https://www.worldometers.info/gas/&#39; html = requests.get(url).content df_list = pd.read_html(html) Oil_dataset = df_list[-1] print(Oil_dataset) Oil_dataset.to_csv(&#39;my data.csv&#39;) . # Country Gas Reserves (MMcf) World Share 0 1 Russia 1688228000 24.3% 1 2 Iran 1201382000 17.3% 2 3 Qatar 871585000 12.5% 3 4 United States 368704000 5.3% 4 5 Saudi Arabia 294205000 4.2% .. .. ... ... ... 94 95 Benin 40000 0.0006% 95 96 Greece 35000 0.0005% 96 97 DR Congo 35000 0.0005% 97 98 Albania 29000 0.0004% 98 99 Barbados 5000 0.0001% [99 rows x 4 columns] . Oil_dataset.columns . Index([&#39;#&#39;, &#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;, &#39;World Share&#39;], dtype=&#39;object&#39;) . Oil_dataset.head() . # Country Gas Reserves (MMcf) World Share . 0 1 | Russia | 1688228000 | 24.3% | . 1 2 | Iran | 1201382000 | 17.3% | . 2 3 | Qatar | 871585000 | 12.5% | . 3 4 | United States | 368704000 | 5.3% | . 4 5 | Saudi Arabia | 294205000 | 4.2% | . Oil_dataset.tail() . # Country Gas Reserves (MMcf) World Share . 94 95 | Benin | 40000 | 0.0006% | . 95 96 | Greece | 35000 | 0.0005% | . 96 97 | DR Congo | 35000 | 0.0005% | . 97 98 | Albania | 29000 | 0.0004% | . 98 99 | Barbados | 5000 | 0.0001% | . Oil_dataset.shape . (99, 3) . y = Oil_dataset[&quot;Gas Reserves (MMcf)&quot;] . . TopTen_GasReserve = y.head(10) . TopTen_GasReserve . 0 1688228000 1 1201382000 2 871585000 3 368704000 4 294205000 5 265000000 6 215098000 7 197087000 8 180490000 9 163959000 Name: Gas Reserves (MMcf), dtype: int64 . PercentShare = Oil_dataset[&quot;World Share&quot;] . PercentShare = x.head(10) . PercentShare . 0 24.3% 1 17.3% 2 12.5% 3 5.3% 4 4.2% 5 3.8% 6 3.1% 7 2.8% 8 2.6% 9 2.4% Name: World Share, dtype: object . TopCountries = Oil_dataset[&quot;Country&quot;] . TopTenCountries = TopCountries.head(10) . TopTenCountries . 0 Russia 1 Iran 2 Qatar 3 United States 4 Saudi Arabia 5 Turkmenistan 6 United Arab Emirates 7 Venezuela 8 Nigeria 9 China Name: Country, dtype: object . Let&#39;s split the dataset into 2 different dataset for analysis. . 1 . Countries Per their Gas reserve 2. Countries pee their percentile Gas reserve . CtryPerShare = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;World Share&#39;]) CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerReserve = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;]) . CtryPerReserve . Country Gas Reserves (MMcf) . 0 Russia | 1688228000 | . 1 Iran | 1201382000 | . 2 Qatar | 871585000 | . 3 United States | 368704000 | . 4 Saudi Arabia | 294205000 | . 5 Turkmenistan | 265000000 | . 6 United Arab Emirates | 215098000 | . 7 Venezuela | 197087000 | . 8 Nigeria | 180490000 | . 9 China | 163959000 | . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerReserve[&#39;Country&#39;], CtryPerReserve[&quot;Gas Reserves (MMcf)&quot;]) . CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerShare.dtypes . Country object World Share object dtype: object . s = CtryPerShare[&#39;World Share&#39;].str.replace(r&#39;%&#39;, r&#39;&#39;).astype(&#39;float&#39;)/100 CtryPerShare[&#39;World Share&#39;] = pd.to_numeric(CtryPerShare[&#39;World Share&#39;], errors=&#39;coerce&#39;).fillna(s) . CtryPerShare . Country World Share . 0 Russia | 0.243 | . 1 Iran | 0.173 | . 2 Qatar | 0.125 | . 3 United States | 0.053 | . 4 Saudi Arabia | 0.042 | . 5 Turkmenistan | 0.038 | . 6 United Arab Emirates | 0.031 | . 7 Venezuela | 0.028 | . 8 Nigeria | 0.026 | . 9 China | 0.024 | . fig = plt.figure(figsize = (10, 7)) plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&quot;Country&quot;], shadow = True) plt.show() . C: Users Admin AppData Local Temp/ipykernel_3308/3530909758.py:4: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&#34;Country&#34;], shadow = True) . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerShare[&#39;Country&#39;], CtryPerShare[&quot;World Share&quot;]) . name = CtryPerShare[&#39;Country&#39;] price = CtryPerShare[&#39;World Share&#39;] # Figure Size fig, ax = plt.subplots(figsize =(16, 9)) # Horizontal Bar Plot ax.barh(name, price) #Remove axes splines for s in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[s].set_visible(False) #Remove x, y Ticks ax.xaxis.set_ticks_position(&#39;none&#39;) ax.yaxis.set_ticks_position(&#39;none&#39;) # Add padding between axes and labels ax.xaxis.set_tick_params(pad = 5) ax.yaxis.set_tick_params(pad = 10) #Add x, y gridlines ax.grid(b = True, color =&#39;grey&#39;, linestyle =&#39;-.&#39;, linewidth = 0.5, alpha = 0.2) # Show top values ax.invert_yaxis() # Add annotation to bars for i in ax.patches: plt.text(i.get_width(), i.get_y()+0.5, str(round((i.get_width()), 2)), fontsize = 10, fontweight =&#39;bold&#39;, color =&#39;grey&#39;) #Add Plot Title ax.set_title(&#39;Top 10 Countries Per % Natural Gas Reseve &#39;, loc =&#39;left&#39;, ) # # # Add Text watermark # fig.text(0.9, 0.15, &#39;Jeeteshgavande30&#39;, fontsize = 12, # color =&#39;grey&#39;, ha =&#39;right&#39;, va =&#39;bottom&#39;, # alpha = 0.7) # Show Plot plt.show() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/03/16/Worldwide-Natural-Gas-Reserve.html",
            "relUrl": "/2022/03/16/Worldwide-Natural-Gas-Reserve.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Cities by accident",
            "content": "US Car Accident Explotary Data Aanalysis . TODO: Talk about EDA . TODO: Talk about the dataset(source, how it will be used, and general information and how it can be used to prevent accident. . pip install opendatasets --upgrade --quiet . Download the dataset from Kaggle . import opendatasets as od download_url = &quot;https://www.kaggle.com/sobhanmoosavi/us-accidents&quot; od.download(download_url) . Skipping, found downloaded files in &#34;./us-accidents&#34; (use force=True to force download) . data_filename = &quot;./us-accidents/US_Accidents_Dec21_updated.csv&quot; . Data Preparation and Cleaning . Load the files using Pandas | Look at some information about the dataset. Get generation stat about the dataset | Fix any mnissing or incorrect values. | import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(data_filename) . df . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description ... Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-1 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.108910 | -83.092860 | 40.112060 | -83.031870 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | ... | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.865420 | -84.062800 | 39.865010 | -84.048730 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | ... | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-3 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.102660 | -84.524680 | 39.102090 | -84.523960 | 0.055 | At I-71/US-50/Exit 1 - Accident. | ... | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-4 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.062130 | -81.537840 | 41.062170 | -81.535470 | 0.123 | At Dart Ave/Exit 21 - Accident. | ... | False | False | False | False | False | False | Night | Night | Day | Day | . 4 A-5 | 3 | 2016-02-08 07:53:43 | 2016-02-08 13:53:43 | 39.172393 | -84.492792 | 39.170476 | -84.501798 | 0.500 | At Mitchell Ave/Exit 6 - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2845337 A-2845338 | 2 | 2019-08-23 18:03:25 | 2019-08-23 18:32:01 | 34.002480 | -117.379360 | 33.998880 | -117.370940 | 0.543 | At Market St - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845338 A-2845339 | 2 | 2019-08-23 19:11:30 | 2019-08-23 19:38:23 | 32.766960 | -117.148060 | 32.765550 | -117.153630 | 0.338 | At Camino Del Rio/Mission Center Rd - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845339 A-2845340 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:28:49 | 33.775450 | -117.847790 | 33.777400 | -117.857270 | 0.561 | At Glassell St/Grand Ave - Accident. in the ri... | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845340 A-2845341 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:29:42 | 33.992460 | -118.403020 | 33.983110 | -118.395650 | 0.772 | At CA-90/Marina Fwy/Jefferson Blvd - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845341 A-2845342 | 2 | 2019-08-23 18:52:06 | 2019-08-23 19:21:31 | 34.133930 | -117.230920 | 34.137360 | -117.239340 | 0.537 | At Highland Ave/Arden Ave - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845342 rows × 47 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Our datset contains 2.8M rows of data with 47 variables . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2845342 entries, 0 to 2845341 Data columns (total 47 columns): # Column Dtype -- 0 ID object 1 Severity int64 2 Start_Time object 3 End_Time object 4 Start_Lat float64 5 Start_Lng float64 6 End_Lat float64 7 End_Lng float64 8 Distance(mi) float64 9 Description object 10 Number float64 11 Street object 12 Side object 13 City object 14 County object 15 State object 16 Zipcode object 17 Country object 18 Timezone object 19 Airport_Code object 20 Weather_Timestamp object 21 Temperature(F) float64 22 Wind_Chill(F) float64 23 Humidity(%) float64 24 Pressure(in) float64 25 Visibility(mi) float64 26 Wind_Direction object 27 Wind_Speed(mph) float64 28 Precipitation(in) float64 29 Weather_Condition object 30 Amenity bool 31 Bump bool 32 Crossing bool 33 Give_Way bool 34 Junction bool 35 No_Exit bool 36 Railway bool 37 Roundabout bool 38 Station bool 39 Stop bool 40 Traffic_Calming bool 41 Traffic_Signal bool 42 Turning_Loop bool 43 Sunrise_Sunset object 44 Civil_Twilight object 45 Nautical_Twilight object 46 Astronomical_Twilight object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 773.4+ MB . Let&#39;s get Stistical summary of our dataset . df.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 1.101431e+06 | 2.776068e+06 | 2.375699e+06 | 2.772250e+06 | 2.786142e+06 | 2.774796e+06 | 2.687398e+06 | 2.295884e+06 | . mean 2.137572e+00 | 3.624520e+01 | -9.711463e+01 | 3.624532e+01 | -9.711439e+01 | 7.026779e-01 | 8.089408e+03 | 6.179356e+01 | 5.965823e+01 | 6.436545e+01 | 2.947234e+01 | 9.099391e+00 | 7.395044e+00 | 7.016940e-03 | . std 4.787216e-01 | 5.363797e+00 | 1.831782e+01 | 5.363873e+00 | 1.831763e+01 | 1.560361e+00 | 1.836009e+04 | 1.862263e+01 | 2.116097e+01 | 2.287457e+01 | 1.045286e+00 | 2.717546e+00 | 5.527454e+00 | 9.348831e-02 | . min 1.000000e+00 | 2.456603e+01 | -1.245481e+02 | 2.456601e+01 | -1.245457e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.344517e+01 | -1.180331e+02 | 3.344628e+01 | -1.180333e+02 | 5.200000e-02 | 1.270000e+03 | 5.000000e+01 | 4.600000e+01 | 4.800000e+01 | 2.931000e+01 | 1.000000e+01 | 3.500000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.609861e+01 | -9.241808e+01 | 3.609799e+01 | -9.241772e+01 | 2.440000e-01 | 4.007000e+03 | 6.400000e+01 | 6.300000e+01 | 6.700000e+01 | 2.982000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.016024e+01 | -8.037243e+01 | 4.016105e+01 | -8.037338e+01 | 7.640000e-01 | 9.567000e+03 | 7.600000e+01 | 7.600000e+01 | 8.300000e+01 | 3.001000e+01 | 1.000000e+01 | 1.000000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.960000e+02 | 1.960000e+02 | 1.000000e+02 | 5.890000e+01 | 1.400000e+02 | 1.087000e+03 | 2.400000e+01 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; How many numeric values are in our dataset? . Helps to realize how many numerical dataset we are working with . numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;] numeric_df = df.select_dtypes(include=numerics) len(numeric_df.columns) . 14 . Is there any missing values in our dataset? . df.isna().sum().sort_values(ascending = False) . Number 1743911 Precipitation(in) 549458 Wind_Chill(F) 469643 Wind_Speed(mph) 157944 Wind_Direction 73775 Humidity(%) 73092 Weather_Condition 70636 Visibility(mi) 70546 Temperature(F) 69274 Pressure(in) 59200 Weather_Timestamp 50736 Airport_Code 9549 Timezone 3659 Nautical_Twilight 2867 Civil_Twilight 2867 Sunrise_Sunset 2867 Astronomical_Twilight 2867 Zipcode 1319 City 137 Street 2 Country 0 Junction 0 Start_Time 0 End_Time 0 Start_Lat 0 Turning_Loop 0 Traffic_Signal 0 Traffic_Calming 0 Stop 0 Station 0 Roundabout 0 Railway 0 No_Exit 0 Crossing 0 Give_Way 0 Bump 0 Amenity 0 Start_Lng 0 End_Lat 0 End_Lng 0 Distance(mi) 0 Description 0 Severity 0 Side 0 County 0 State 0 ID 0 dtype: int64 . Let&#39;s convert the data into percentile . missing_percentages = df.isna().sum().sort_values(ascending = False)/len(df) missing_percentages . Number 6.129003e-01 Precipitation(in) 1.931079e-01 Wind_Chill(F) 1.650568e-01 Wind_Speed(mph) 5.550967e-02 Wind_Direction 2.592834e-02 Humidity(%) 2.568830e-02 Weather_Condition 2.482514e-02 Visibility(mi) 2.479350e-02 Temperature(F) 2.434646e-02 Pressure(in) 2.080593e-02 Weather_Timestamp 1.783125e-02 Airport_Code 3.356011e-03 Timezone 1.285961e-03 Nautical_Twilight 1.007612e-03 Civil_Twilight 1.007612e-03 Sunrise_Sunset 1.007612e-03 Astronomical_Twilight 1.007612e-03 Zipcode 4.635647e-04 City 4.814887e-05 Street 7.029032e-07 Country 0.000000e+00 Junction 0.000000e+00 Start_Time 0.000000e+00 End_Time 0.000000e+00 Start_Lat 0.000000e+00 Turning_Loop 0.000000e+00 Traffic_Signal 0.000000e+00 Traffic_Calming 0.000000e+00 Stop 0.000000e+00 Station 0.000000e+00 Roundabout 0.000000e+00 Railway 0.000000e+00 No_Exit 0.000000e+00 Crossing 0.000000e+00 Give_Way 0.000000e+00 Bump 0.000000e+00 Amenity 0.000000e+00 Start_Lng 0.000000e+00 End_Lat 0.000000e+00 End_Lng 0.000000e+00 Distance(mi) 0.000000e+00 Description 0.000000e+00 Severity 0.000000e+00 Side 0.000000e+00 County 0.000000e+00 State 0.000000e+00 ID 0.000000e+00 dtype: float64 . type(missing_percentages) . pandas.core.series.Series . let&#39;s remove all the data from missing_percentile that has zero as a value . missing_percentages[missing_percentages !=0] . Number 6.129003e-01 Precipitation(in) 1.931079e-01 Wind_Chill(F) 1.650568e-01 Wind_Speed(mph) 5.550967e-02 Wind_Direction 2.592834e-02 Humidity(%) 2.568830e-02 Weather_Condition 2.482514e-02 Visibility(mi) 2.479350e-02 Temperature(F) 2.434646e-02 Pressure(in) 2.080593e-02 Weather_Timestamp 1.783125e-02 Airport_Code 3.356011e-03 Timezone 1.285961e-03 Nautical_Twilight 1.007612e-03 Civil_Twilight 1.007612e-03 Sunrise_Sunset 1.007612e-03 Astronomical_Twilight 1.007612e-03 Zipcode 4.635647e-04 City 4.814887e-05 Street 7.029032e-07 dtype: float64 . Let&#39;s visualize this in matplotlib . missing_percentages[missing_percentages !=0].plot(kind = &quot;barh&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ae2919750&gt; . Exploratory Data Analysis and Visualization . Col to analyze . 1. City 2. Start Time 3. Temprature 4. Weather Condtions 5. Start Lat, Start lng . How many Cities are in the dataset? . len(df.City.unique()) . 11682 . How many States are represented in the dataset? . len(df.State.unique()) . 49 . cities_by_accident = df.City.value_counts() cities_by_accident . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 ... Ridgedale 1 Sekiu 1 Wooldridge 1 Bullock 1 American Fork-Pleasant Grove 1 Name: City, Length: 11681, dtype: int64 . Ask question . Are there more accident in warmer or colder areas? | which 5 states have the highest number of accident per capita? | cities_by_accident[:20] . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 Charlotte 33152 Sacramento 32559 San Diego 26627 Raleigh 22840 Minneapolis 22768 Portland 20944 Nashville 20267 Austin 18301 Baton Rouge 18182 Phoenix 17143 Saint Paul 16869 New Orleans 16251 Atlanta 15622 Jacksonville 14967 Richmond 14349 Name: City, dtype: int64 . cities_by_accident . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 ... Ridgedale 1 Sekiu 1 Wooldridge 1 Bullock 1 American Fork-Pleasant Grove 1 Name: City, Length: 11681, dtype: int64 . Let&#39;s check the information for the most populated city in the states . Let&#39;s check if our dataset contains information about New York. | . df.State.head(10) . 0 OH 1 OH 2 OH 3 OH 4 OH 5 OH 6 OH 7 OH 8 OH 9 OH Name: State, dtype: object . Is NY State and City in the dataset? . &quot;NY&quot; in df[&quot;State&quot;].values . True . &quot;New York&quot; in df[&quot;City&quot;].values . True . Since we have data for New York as a City and State, let&#39;s dig deep into it. . cities_by_accident[&quot;New York&quot;] . 7068 . Let&#39;s visualize the accident for top 10 cities by accident . cities_by_accident[:10].plot(kind = &quot;barh&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b12cbdbd0&gt; . Let&#39;s get a distribution summary for our data. We can visualize this using seaborn . Find out the top cities with top accident and where they located . sns.set_style(&quot;darkgrid&quot;) sns.distplot(cities_by_accident) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4adb8d9550&gt; . high_accident_cities = cities_by_accident[cities_by_accident &gt;= 1000] len(high_accident_cities) . 496 . low_accident_cities = cities_by_accident[cities_by_accident &lt;= 1000] len(low_accident_cities) . 11187 . It appears our dataset contains more cities with accident &lt; 1000 . Let&#39;s get the destribution summary for both high and low accident cities. . sns.distplot(high_accident_cities) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad9ccd190&gt; . sns.distplot(low_accident_cities) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad53caa90&gt; . Let&#39;s use log scale better understaning of our dataset. . sns.histplot(cities_by_accident, log_scale = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad52c6410&gt; . z = cities_by_accident[cities_by_accident &lt;= 10] len(z) . 4628 . Start Time . df.Start_Time . 0 2016-02-08 00:37:08 1 2016-02-08 05:56:20 2 2016-02-08 06:15:39 3 2016-02-08 06:51:45 4 2016-02-08 07:53:43 ... 2845337 2019-08-23 18:03:25 2845338 2019-08-23 19:11:30 2845339 2019-08-23 19:00:21 2845340 2019-08-23 19:00:21 2845341 2019-08-23 18:52:06 Name: Start_Time, Length: 2845342, dtype: object . Let&#39;s convert the Start Time data type into a date time format . df.Start_Time = pd.to_datetime(df.Start_Time) . Let&#39;s visualize the time of the day when accident happen. . df.Start_Time.dt.hour . 0 0 1 5 2 6 3 6 4 7 .. 2845337 18 2845338 19 2845339 19 2845340 19 2845341 18 Name: Start_Time, Length: 2845342, dtype: int64 . sns.distplot(df.Start_Time.dt.hour,bins = 24) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b261ec890&gt; . sns.distplot(df.Start_Time.dt.hour,bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad547ead0&gt; . From the above result, we can conclude that most accidents happen early morning when people go to work and in the afternoon when they get back home from work. Let&#39;s also find out what days of the week we have more accidents... . sns.distplot(df.Start_Time.dt.dayofweek, bins = 7, kde= False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4e76250&gt; . It appears during the week days(Monday = 0...) accidents incidents are fairly distributed. On the weekend, the accident seems to be less. . Let&#39;s check what time of the day the accident happen for weekend. . sundays_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 6] sns.distplot(sundays_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4aeb3b3dd0&gt; . It appears on Sunday, the accident happens in the afternoon. . let&#39;s visualize the data for each day of the week . Monday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 0] sns.distplot(Monday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4d21a50&gt; . For Monday, the peak time for accidents is in the morning from 7 - 9 and in the afternoon from 3- 5. . Tuesday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 1] sns.distplot(Tuesday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4cd1110&gt; . Tuesday is almost identical to Monday . Wednesday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 2] sns.distplot(Wednesday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4d18ed0&gt; . Thursday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 3] sns.distplot(Thursday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4c04c10&gt; . Friday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 4] sns.distplot(Friday_start_time.dt.hour, bins = 24, kde = True, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4aea24d4d0&gt; . On the weekdays, accidents mostly happen during morning and afternoon rush hours, which is the obvious busiest time of the day for motor vehicles. . Saturday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 5] sns.distplot(Saturday_start_time.dt.hour, bins = 24, kde = True, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad4995310&gt; . However on Saturday, it appears the accident happens during the early hours and peaks 2-3 in the afternoon. . Let&#39;s check in which month of the year the accidents tend to be high. . sns.distplot(df.Start_Time.dt.month, bins = 12, kde = False, norm_hist = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad43fe5d0&gt; . This concludes that winter is when most accidents happen . What are the top 5 cities per accident? . sns.histplot(cities_by_accident, log_scale = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad51f29d0&gt; . TopCitiesPerAcc = cities_by_accident.head(5) TopCitiesPerAcc . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 Name: City, dtype: int64 . plt.plot(TopCitiesPerAcc) . [&lt;matplotlib.lines.Line2D at 0x7f4ad4779810&gt;] . Find out Accident per States . Which State has the highest accident? | . States_by_accident = df.State.value_counts() States_by_accident . CA 795868 FL 401388 TX 149037 OR 126341 VA 113535 NY 108049 PA 99975 MN 97185 NC 91362 SC 89216 MD 65085 AZ 56504 NJ 52902 TN 52613 UT 49193 LA 47232 IL 47105 MI 43843 GA 40086 WA 32554 CT 29762 MO 29633 CO 25340 OH 24409 IN 20850 AL 19322 MT 15964 AR 10935 IA 9607 DC 9133 KS 9033 OK 8806 ID 8544 WI 7896 WV 7632 KY 6638 MA 6392 NV 6197 MS 5320 DE 4842 RI 4451 NH 3866 NE 3320 NM 2370 ND 2258 ME 2193 WY 990 VT 365 SD 201 Name: State, dtype: int64 . Top 5 States by accident . States_by_accident.head() . CA 795868 FL 401388 TX 149037 OR 126341 VA 113535 Name: State, dtype: int64 . California seems to be the worst place when it comes to accidents. . sns.distplot(States_by_accident.head(5)) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad471dc50&gt; . Plot top 5 States with the most accidents . States_by_accident.head(5).plot(xlabel = &quot;State&quot;, kind = &quot;barh&quot;, ylabel = &quot;Num of Accidents&quot;, title = &quot;State vs Accident&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4ad46b3750&gt; . Summary and Conclusions . Insights: . - The number of accidents per city decreases exponentially - Less 5% of cities have more than 1000 yearly accidents - Over 1200 cities reported just one accident(This needs to be further investigated) - Accidents for weekdays seem to be higher than the weekend. - Accidents seem to be high for the winter time. .",
            "url": "https://seifhh.github.io/DS_Projects/2022/03/16/US_CAR_ACCIDENT.html",
            "relUrl": "/2022/03/16/US_CAR_ACCIDENT.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "COVID-19 EXPLANATORY DATA ANALYSIS",
            "content": "Data Source: Johns Hopkins University . Dataset Link: . https://github.com/CSSEGISandData/COVID-19 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE This dataset is updated on daily basis by Johns Hopkins CSSE . Downloading and installing the necessary packages. . !pip install pycountry_convert --quiet !pip install folium --quiet !pip install calmap --quiet . import numpy as np import pandas as pd import requests from bs4 import BeautifulSoup import plotly.graph_objects as go import plotly.express as px import plotly.io as pio pio.templates.default = &quot;plotly_dark&quot; from plotly.subplots import make_subplots import folium from folium import plugins from tqdm.notebook import tqdm as tqdm import seaborn as sns from pathlib import Path import os import warnings warnings.filterwarnings(&#39;ignore&#39;) . Retrieving data from the source (JHU/Worldometers.info) . Cleaned_data = pd.read_csv(&quot;D: Data Science Python Covid-19 Data COVID-19_LatestData.csv&quot;) Cleaned_data . # Country Total Cases Total Deaths Total Recovered Active Cases Total Tests Population . 0 0 | World | 461652411 | 6074898 | 394,906,322 | 60,671,191 | 0 | 0 | . 1 1 | USA | 81259094 | 993757 | 56,461,468 | 23,803,869 | 967505242 | 334305317 | . 2 2 | India | 42998938 | 516103 | 42,450,055 | 32,780 | 779754156 | 1403087655 | . 3 3 | Brazil | 29432274 | 655649 | 27,968,811 | 807,814 | 63776166 | 215130305 | . 4 4 | France | 23649615 | 140440 | 22,327,945 | 1,181,230 | 246629975 | 65519286 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 221 222 | Western Sahara | 10 | 1 | 8 | 1 | 0 | 622306 | . 222 224 | Marshall Islands | 7 | 0 | 7 | 0 | 0 | 59870 | . 223 225 | Saint Helena | 2 | 0 | 2 | 0 | 0 | 6108 | . 224 226 | Micronesia | 1 | 0 | 1 | 0 | 0 | 117074 | . 225 227 | Niue | 1 | 0 | 0 | 1 | 0 | 1645 | . 226 rows × 8 columns . Preprocessing(Cleaning Data) . Cleaned_Data1 = Cleaned_data.drop(0) # Droping Column &quot;#&quot; Cleaned_Data1.drop(columns = [&#39;#&#39;], inplace = True) Cleaned_Data1.head() . Country Total Cases Total Deaths Total Recovered Active Cases Total Tests Population . 1 USA | 81259094 | 993757 | 56,461,468 | 23,803,869 | 967505242 | 334305317 | . 2 India | 42998938 | 516103 | 42,450,055 | 32,780 | 779754156 | 1403087655 | . 3 Brazil | 29432274 | 655649 | 27,968,811 | 807,814 | 63776166 | 215130305 | . 4 France | 23649615 | 140440 | 22,327,945 | 1,181,230 | 246629975 | 65519286 | . 5 UK | 19819810 | 163095 | 18,429,633 | 1,227,082 | 492630733 | 68492797 | . Cleaned_data[[&#39;Country&#39;, &#39;Total Deaths&#39;]].head() . Country Total Deaths . 0 World | 6074898 | . 1 USA | 993757 | . 2 India | 516103 | . 3 Brazil | 655649 | . 4 France | 140440 | . Cleaned_data[[&#39;Country&#39;,&#39;Total Deaths&#39;]].head() . Country Total Deaths . 0 World | 6074898 | . 1 USA | 993757 | . 2 India | 516103 | . 3 Brazil | 655649 | . 4 France | 140440 | . Cleaned_data[[&quot;Country&quot;,&quot;Total Recovered&quot;]].head() . Country Total Recovered . 0 World | 394,906,322 | . 1 USA | 56,461,468 | . 2 India | 42,450,055 | . 3 Brazil | 27,968,811 | . 4 France | 22,327,945 | . Active_Cases = Cleaned_data[[&quot;Country&quot;,&quot;Active Cases&quot;]] Active_Cases . Country Active Cases . 0 World | 60,671,191 | . 1 USA | 23,803,869 | . 2 India | 32,780 | . 3 Brazil | 807,814 | . 4 France | 1,181,230 | . ... ... | ... | . 221 Western Sahara | 1 | . 222 Marshall Islands | 0 | . 223 Saint Helena | 0 | . 224 Micronesia | 0 | . 225 Niue | 1 | . 226 rows × 2 columns . Removing &quot;,&quot; from the ActiveCases Column . Cleaned_Data1.replace(&quot;,&quot;,&quot;&quot;, regex = True, inplace = True) . Cleaned_Data1 . Country Total Cases Total Deaths Total Recovered Active Cases Total Tests Population . 1 USA | 81259094 | 993757 | 56461468 | 23803869 | 967505242 | 334305317 | . 2 India | 42998938 | 516103 | 42450055 | 32780 | 779754156 | 1403087655 | . 3 Brazil | 29432274 | 655649 | 27968811 | 807814 | 63776166 | 215130305 | . 4 France | 23649615 | 140440 | 22327945 | 1181230 | 246629975 | 65519286 | . 5 UK | 19819810 | 163095 | 18429633 | 1227082 | 492630733 | 68492797 | . ... ... | ... | ... | ... | ... | ... | ... | . 221 Western Sahara | 10 | 1 | 8 | 1 | 0 | 622306 | . 222 Marshall Islands | 7 | 0 | 7 | 0 | 0 | 59870 | . 223 Saint Helena | 2 | 0 | 2 | 0 | 0 | 6108 | . 224 Micronesia | 1 | 0 | 1 | 0 | 0 | 117074 | . 225 Niue | 1 | 0 | 0 | 1 | 0 | 1645 | . 225 rows × 7 columns . Cleaned_Data1.dtypes . Country object Total Cases int64 Total Deaths int64 Total Recovered object Active Cases object Total Tests int64 Population int64 dtype: object . Converting Object Datatype into numeric. As we can see, Total Recovered and Active Cases columns are read as an object Datatype. We need to convert them to numeric value. . Use the pd.to_numeric to convert both at the same time! . Cleaned_Data1[[&quot;Total Recovered&quot;, &quot;Active Cases&quot;]] = Cleaned_Data1[[&quot;Total Recovered&quot;, &quot;Active Cases&quot;]].apply(pd.to_numeric) . Cleaned_Data1.dtypes . Country object Total Cases int64 Total Deaths int64 Total Recovered int64 Active Cases int64 Total Tests int64 Population int64 dtype: object . Global_ConfirmedCases = Cleaned_Data1[[&#39;Country&#39;,&#39;Total Cases&#39;]] Global_ActiveCases.index = np.arange(1, 227) fig = px.bar(Global_ConfirmedCases.sort_values(&#39;Total Cases&#39;,ascending=False)[:10][::-1],x=&#39;Total Cases&#39;,y=&#39;Country&#39;,title=&#39;Confirmed Cases Worldwide&#39;,text=&#39;Total Cases&#39;, height=900, orientation=&#39;h&#39;) fig.show() . Global_DeathCases = Cleaned_Data1[[&#39;Country&#39;,&#39;Total Deaths&#39;]] Global_DeathCases.index = np.arange(1, 226) fig = px.bar(Global_DeathCases.sort_values(&#39;Total Deaths&#39;,ascending=False)[:10][::-1],x=&#39;Total Deaths&#39;,y=&#39;Country&#39;,title=&#39;Deaths Cases Worldwide&#39;,text=&#39;Total Deaths&#39;, height=900, orientation=&#39;h&#39;) fig.show() . Cleaned_Data1[&#39;Active Cases&#39;] . 1 23803869 2 32780 3 807814 4 1181230 5 1227082 ... 221 1 222 0 223 0 224 0 225 1 Name: Active Cases, Length: 225, dtype: int64 . Global_RecoveredCases = Cleaned_Data1[[&#39;Country&#39;,&#39;Total Recovered&#39;]] Global_RecoveredCases.index = np.arange(1, 226) fig = px.bar(Global_RecoveredCases.sort_values(&#39;Total Recovered&#39;,ascending=False)[:10][::-1],x=&#39;Total Recovered&#39;,y=&#39;Country&#39;,title=&#39;Recovered Cases Worldwide&#39;,text=&#39;Total Recovered&#39;, height=900, orientation=&#39;h&#39;) fig.show() . Global_ActiveCases = Cleaned_Data1[[&#39;Country&#39;,&#39;Active Cases&#39;]] Global_ActiveCases.index = np.arange(1,226) fig = px.bar(Global_ActiveCases.sort_values(&#39;Active Cases&#39;,ascending=False)[:10][::-1],x=&#39;Active Cases&#39;,y=&#39;Country&#39;,title=&#39;Active Cases Worldwide&#39;,text=&#39;Active Cases&#39;, height=900, orientation=&#39;h&#39;) fig.show() . Let&#8217;s determine Covid-19 Mortality Rate. . Cleaned_Data1 . Country Total Cases Total Deaths Total Recovered Active Cases Total Tests Population . 1 USA | 81259094 | 993757 | 56461468 | 23803869 | 967505242 | 334305317 | . 2 India | 42998938 | 516103 | 42450055 | 32780 | 779754156 | 1403087655 | . 3 Brazil | 29432274 | 655649 | 27968811 | 807814 | 63776166 | 215130305 | . 4 France | 23649615 | 140440 | 22327945 | 1181230 | 246629975 | 65519286 | . 5 UK | 19819810 | 163095 | 18429633 | 1227082 | 492630733 | 68492797 | . ... ... | ... | ... | ... | ... | ... | ... | . 221 Western Sahara | 10 | 1 | 8 | 1 | 0 | 622306 | . 222 Marshall Islands | 7 | 0 | 7 | 0 | 0 | 59870 | . 223 Saint Helena | 2 | 0 | 2 | 0 | 0 | 6108 | . 224 Micronesia | 1 | 0 | 1 | 0 | 0 | 117074 | . 225 Niue | 1 | 0 | 0 | 1 | 0 | 1645 | . 225 rows × 7 columns . flg = Cleaned_Data1.groupby(&#39;Country&#39;)[&#39;Total Cases&#39;, &#39;Total Deaths&#39;, &#39;Total Recovered&#39;, &#39;Active Cases&#39;].sum().reset_index() . flg . Country Total Cases Total Deaths Total Recovered Active Cases . 0 Afghanistan | 176617 | 7649 | 159007 | 9961 | . 1 Albania | 272711 | 3486 | 268563 | 662 | . 2 Algeria | 265478 | 6868 | 178094 | 80516 | . 3 Andorra | 38794 | 152 | 37925 | 717 | . 4 Angola | 98956 | 1900 | 96935 | 121 | . ... ... | ... | ... | ... | ... | . 220 Wallis and Futuna | 454 | 7 | 438 | 9 | . 221 Western Sahara | 10 | 1 | 8 | 1 | . 222 Yemen | 11793 | 2139 | 8879 | 775 | . 223 Zambia | 315381 | 3961 | 309765 | 1655 | . 224 Zimbabwe | 243365 | 5417 | 232787 | 5161 | . 225 rows × 5 columns . flg[&#39;MortalityRate %&#39;] = round((flg[&#39;Total Deaths&#39;]/flg[&#39;Total Cases&#39;])*100, 2) temp = flg[flg[&#39;Total Cases&#39;]&gt;100] temp = temp.sort_values(&#39;MortalityRate %&#39;, ascending=False) temp . Country Total Cases Total Deaths Total Recovered Active Cases MortalityRate MortalityRate % . 222 Yemen | 11793 | 2139 | 8879 | 775 | 18.14 | 18.14 | . 193 Sudan | 61715 | 4874 | 0 | 0 | 7.90 | 7.90 | . 158 Peru | 3537488 | 211619 | 0 | 0 | 5.98 | 5.98 | . 130 Mexico | 5607845 | 321115 | 4900680 | 386050 | 5.73 | 5.73 | . 197 Syria | 55431 | 3118 | 50181 | 2132 | 5.63 | 5.63 | . ... ... | ... | ... | ... | ... | ... | ... | . 91 Iceland | 165365 | 83 | 75685 | 89597 | 0.05 | 0.05 | . 22 Bhutan | 19435 | 7 | 14112 | 5316 | 0.04 | 0.04 | . 144 New Zealand | 399342 | 117 | 205975 | 193250 | 0.03 | 0.03 | . 46 Cook Islands | 476 | 0 | 132 | 344 | 0.00 | 0.00 | . 67 Falkland Islands | 118 | 0 | 0 | 0 | 0.00 | 0.00 | . 217 rows × 7 columns . fig = px.bar(temp.sort_values(by=&quot;MortalityRate %&quot;, ascending=False)[:20][::-1], x = &#39;MortalityRate %&#39;, y = &#39;Country&#39;, title=&#39;Deaths per 100 Confirmed Cases&#39;, text=&#39;MortalityRate %&#39;, height=800, orientation=&#39;h&#39;, color_discrete_sequence=[&#39;darkred&#39;] ) fig.show() . This is astonishing and mind boggling at the same time. The people of Yemen&#39;s mortality rate is extremely high and it needs to be further invetigated. As a war-torn country, it’s understandable that it might not be easy to get a vaccination to them, but this should not be acceptable by any standard. There is 18% chance of dying from Covid in Yemen, which is as high as 5 times the average Global Mortality Rate registered by WHO. If we ever going to eradicate this disease, vaccinating the underdeveloped countries become a must. . Let&#39;s check mortality rate for: USA, AUSTRALIA, ETHIOPIA, UK, AND NORWAY . temp.set_index(&#39;Country&#39;, inplace = True) . temp.loc[&quot;Australia&quot;][&quot;MortalityRate&quot;] . 0.15 . temp.loc[&quot;Ethiopia&quot;][&quot;MortalityRate&quot;] . 1.6 . temp.loc[&quot;USA&quot;][&quot;MortalityRate&quot;] . 1.22 . temp.loc[&quot;Norway&quot;][&quot;MortalityRate&quot;] . 0.13 . temp.loc[&quot;UK&quot;][&quot;MortalityRate&quot;] . 0.82 .",
            "url": "https://seifhh.github.io/DS_Projects/2022/03/16/COVID-19-2022-02-01Explanatory-Data-Analysis-and-Visualizations.html",
            "relUrl": "/2022/03/16/COVID-19-2022-02-01Explanatory-Data-Analysis-and-Visualizations.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Worldwide Natural Gas Reserve Data Analysis.",
            "content": "import requests import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns url = &#39;https://www.worldometers.info/gas/&#39; html = requests.get(url).content df_list = pd.read_html(html) Oil_dataset = df_list[-1] print(Oil_dataset) Oil_dataset.to_csv(&#39;my data.csv&#39;) . # Country Gas Reserves (MMcf) World Share 0 1 Russia 1688228000 24.3% 1 2 Iran 1201382000 17.3% 2 3 Qatar 871585000 12.5% 3 4 United States 368704000 5.3% 4 5 Saudi Arabia 294205000 4.2% .. .. ... ... ... 94 95 Benin 40000 0.0006% 95 96 Greece 35000 0.0005% 96 97 DR Congo 35000 0.0005% 97 98 Albania 29000 0.0004% 98 99 Barbados 5000 0.0001% [99 rows x 4 columns] . Oil_dataset.columns . Index([&#39;#&#39;, &#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;, &#39;World Share&#39;], dtype=&#39;object&#39;) . Oil_dataset.head() . # Country Gas Reserves (MMcf) World Share . 0 1 | Russia | 1688228000 | 24.3% | . 1 2 | Iran | 1201382000 | 17.3% | . 2 3 | Qatar | 871585000 | 12.5% | . 3 4 | United States | 368704000 | 5.3% | . 4 5 | Saudi Arabia | 294205000 | 4.2% | . Oil_dataset.tail() . # Country Gas Reserves (MMcf) World Share . 94 95 | Benin | 40000 | 0.0006% | . 95 96 | Greece | 35000 | 0.0005% | . 96 97 | DR Congo | 35000 | 0.0005% | . 97 98 | Albania | 29000 | 0.0004% | . 98 99 | Barbados | 5000 | 0.0001% | . Oil_dataset.shape . (99, 3) . y = Oil_dataset[&quot;Gas Reserves (MMcf)&quot;] . . TopTen_GasReserve = y.head(10) . TopTen_GasReserve . 0 1688228000 1 1201382000 2 871585000 3 368704000 4 294205000 5 265000000 6 215098000 7 197087000 8 180490000 9 163959000 Name: Gas Reserves (MMcf), dtype: int64 . PercentShare = Oil_dataset[&quot;World Share&quot;] . PercentShare = x.head(10) . PercentShare . 0 24.3% 1 17.3% 2 12.5% 3 5.3% 4 4.2% 5 3.8% 6 3.1% 7 2.8% 8 2.6% 9 2.4% Name: World Share, dtype: object . TopCountries = Oil_dataset[&quot;Country&quot;] . TopTenCountries = TopCountries.head(10) . TopTenCountries . 0 Russia 1 Iran 2 Qatar 3 United States 4 Saudi Arabia 5 Turkmenistan 6 United Arab Emirates 7 Venezuela 8 Nigeria 9 China Name: Country, dtype: object . Let&#39;s split the dataset into 2 different dataset for analysis. . 1 . Countries Per their Gas reserve 2. Countries pee their percentile Gas reserve . CtryPerShare = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;World Share&#39;]) CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerReserve = pd.DataFrame(Oil_dataset.head(10), columns= [&#39;Country&#39;, &#39;Gas Reserves (MMcf)&#39;]) . CtryPerReserve . Country Gas Reserves (MMcf) . 0 Russia | 1688228000 | . 1 Iran | 1201382000 | . 2 Qatar | 871585000 | . 3 United States | 368704000 | . 4 Saudi Arabia | 294205000 | . 5 Turkmenistan | 265000000 | . 6 United Arab Emirates | 215098000 | . 7 Venezuela | 197087000 | . 8 Nigeria | 180490000 | . 9 China | 163959000 | . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerReserve[&#39;Country&#39;], CtryPerReserve[&quot;Gas Reserves (MMcf)&quot;]) . CtryPerShare . Country World Share . 0 Russia | 24.3% | . 1 Iran | 17.3% | . 2 Qatar | 12.5% | . 3 United States | 5.3% | . 4 Saudi Arabia | 4.2% | . 5 Turkmenistan | 3.8% | . 6 United Arab Emirates | 3.1% | . 7 Venezuela | 2.8% | . 8 Nigeria | 2.6% | . 9 China | 2.4% | . CtryPerShare.dtypes . Country object World Share object dtype: object . s = CtryPerShare[&#39;World Share&#39;].str.replace(r&#39;%&#39;, r&#39;&#39;).astype(&#39;float&#39;)/100 CtryPerShare[&#39;World Share&#39;] = pd.to_numeric(CtryPerShare[&#39;World Share&#39;], errors=&#39;coerce&#39;).fillna(s) . CtryPerShare . Country World Share . 0 Russia | 0.243 | . 1 Iran | 0.173 | . 2 Qatar | 0.125 | . 3 United States | 0.053 | . 4 Saudi Arabia | 0.042 | . 5 Turkmenistan | 0.038 | . 6 United Arab Emirates | 0.031 | . 7 Venezuela | 0.028 | . 8 Nigeria | 0.026 | . 9 China | 0.024 | . fig = plt.figure(figsize = (10, 7)) plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&quot;Country&quot;], shadow = True) plt.show() . C: Users Admin AppData Local Temp/ipykernel_3308/3530909758.py:4: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False plt.pie(CtryPerShare[&#39;World Share&#39;], labels = CtryPerShare[&#34;Country&#34;], shadow = True) . fig = plt.figure(figsize = (18,7)) chart = plt.bar(CtryPerShare[&#39;Country&#39;], CtryPerShare[&quot;World Share&quot;]) . name = CtryPerShare[&#39;Country&#39;] price = CtryPerShare[&#39;World Share&#39;] # Figure Size fig, ax = plt.subplots(figsize =(16, 9)) # Horizontal Bar Plot ax.barh(name, price) #Remove axes splines for s in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[s].set_visible(False) #Remove x, y Ticks ax.xaxis.set_ticks_position(&#39;none&#39;) ax.yaxis.set_ticks_position(&#39;none&#39;) # Add padding between axes and labels ax.xaxis.set_tick_params(pad = 5) ax.yaxis.set_tick_params(pad = 10) #Add x, y gridlines ax.grid(b = True, color =&#39;grey&#39;, linestyle =&#39;-.&#39;, linewidth = 0.5, alpha = 0.2) # Show top values ax.invert_yaxis() # Add annotation to bars for i in ax.patches: plt.text(i.get_width(), i.get_y()+0.5, str(round((i.get_width()), 2)), fontsize = 10, fontweight =&#39;bold&#39;, color =&#39;grey&#39;) #Add Plot Title ax.set_title(&#39;Top 10 Countries Per % Natural Gas Reseve &#39;, loc =&#39;left&#39;, ) # # # Add Text watermark # fig.text(0.9, 0.15, &#39;Jeeteshgavande30&#39;, fontsize = 12, # color =&#39;grey&#39;, ha =&#39;right&#39;, va =&#39;bottom&#39;, # alpha = 0.7) # Show Plot plt.show() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/02/28/Worldwide-Natural-Gas-Reserve.html",
            "relUrl": "/2022/02/28/Worldwide-Natural-Gas-Reserve.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "US Car Accident Exploratory Data Analysis",
            "content": "US NATION-WIDE CAR ACCIDENT EXPLORATORY DATA ANALYSIS. . - Exploring the dataset - General overview of the data - Cleaning(Checking missing values, fixing datatypes, and joining variables) - Performing Statistical Analysis - Comparing and contrasting variables - Data visualization both in Matplotlib and Seaborn. - Driving insight from the conclusions. . Our dataset has over 2.8 million rows of data and 47 different variables collected over the span of 5 years from 49 States in the US and it can be obtained from the following source: . https://www.kaggle.com/sobhanmoosavi/us-accidents . !pip install opendatasets --upgrade --quiet . Downloading the dataset from Kaggle . import opendatasets as od download_url = &quot;https://www.kaggle.com/sobhanmoosavi/us-accidents&quot; od.download(download_url) . Skipping, found downloaded files in &#34;. us-accidents&#34; (use force=True to force download) . Data Preparation and Cleaning . Load the files using Pandas | Look at some information about the dataset. Get generation stat about the dataset | Fix any mnissing or incorrect values. | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . data_filename = &quot;./us-accidents/US_Accidents_Dec21_updated.csv&quot; df = pd.read_csv(data_filename) . df . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description ... Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-1 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.108910 | -83.092860 | 40.112060 | -83.031870 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | ... | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.865420 | -84.062800 | 39.865010 | -84.048730 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | ... | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-3 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.102660 | -84.524680 | 39.102090 | -84.523960 | 0.055 | At I-71/US-50/Exit 1 - Accident. | ... | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-4 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.062130 | -81.537840 | 41.062170 | -81.535470 | 0.123 | At Dart Ave/Exit 21 - Accident. | ... | False | False | False | False | False | False | Night | Night | Day | Day | . 4 A-5 | 3 | 2016-02-08 07:53:43 | 2016-02-08 13:53:43 | 39.172393 | -84.492792 | 39.170476 | -84.501798 | 0.500 | At Mitchell Ave/Exit 6 - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2845337 A-2845338 | 2 | 2019-08-23 18:03:25 | 2019-08-23 18:32:01 | 34.002480 | -117.379360 | 33.998880 | -117.370940 | 0.543 | At Market St - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845338 A-2845339 | 2 | 2019-08-23 19:11:30 | 2019-08-23 19:38:23 | 32.766960 | -117.148060 | 32.765550 | -117.153630 | 0.338 | At Camino Del Rio/Mission Center Rd - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845339 A-2845340 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:28:49 | 33.775450 | -117.847790 | 33.777400 | -117.857270 | 0.561 | At Glassell St/Grand Ave - Accident. in the ri... | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845340 A-2845341 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:29:42 | 33.992460 | -118.403020 | 33.983110 | -118.395650 | 0.772 | At CA-90/Marina Fwy/Jefferson Blvd - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845341 A-2845342 | 2 | 2019-08-23 18:52:06 | 2019-08-23 19:21:31 | 34.133930 | -117.230920 | 34.137360 | -117.239340 | 0.537 | At Highland Ave/Arden Ave - Accident. | ... | False | False | False | False | False | False | Day | Day | Day | Day | . 2845342 rows × 47 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2845342 entries, 0 to 2845341 Data columns (total 47 columns): # Column Dtype -- 0 ID object 1 Severity int64 2 Start_Time object 3 End_Time object 4 Start_Lat float64 5 Start_Lng float64 6 End_Lat float64 7 End_Lng float64 8 Distance(mi) float64 9 Description object 10 Number float64 11 Street object 12 Side object 13 City object 14 County object 15 State object 16 Zipcode object 17 Country object 18 Timezone object 19 Airport_Code object 20 Weather_Timestamp object 21 Temperature(F) float64 22 Wind_Chill(F) float64 23 Humidity(%) float64 24 Pressure(in) float64 25 Visibility(mi) float64 26 Wind_Direction object 27 Wind_Speed(mph) float64 28 Precipitation(in) float64 29 Weather_Condition object 30 Amenity bool 31 Bump bool 32 Crossing bool 33 Give_Way bool 34 Junction bool 35 No_Exit bool 36 Railway bool 37 Roundabout bool 38 Station bool 39 Stop bool 40 Traffic_Calming bool 41 Traffic_Signal bool 42 Turning_Loop bool 43 Sunrise_Sunset object 44 Civil_Twilight object 45 Nautical_Twilight object 46 Astronomical_Twilight object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 773.4+ MB . Let&#39;s get Stistical summary of our dataset . df.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 2.845342e+06 | 1.101431e+06 | 2.776068e+06 | 2.375699e+06 | 2.772250e+06 | 2.786142e+06 | 2.774796e+06 | 2.687398e+06 | 2.295884e+06 | . mean 2.137572e+00 | 3.624520e+01 | -9.711463e+01 | 3.624532e+01 | -9.711439e+01 | 7.026779e-01 | 8.089408e+03 | 6.179356e+01 | 5.965823e+01 | 6.436545e+01 | 2.947234e+01 | 9.099391e+00 | 7.395044e+00 | 7.016940e-03 | . std 4.787216e-01 | 5.363797e+00 | 1.831782e+01 | 5.363873e+00 | 1.831763e+01 | 1.560361e+00 | 1.836009e+04 | 1.862263e+01 | 2.116097e+01 | 2.287457e+01 | 1.045286e+00 | 2.717546e+00 | 5.527454e+00 | 9.348831e-02 | . min 1.000000e+00 | 2.456603e+01 | -1.245481e+02 | 2.456601e+01 | -1.245457e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.344517e+01 | -1.180331e+02 | 3.344628e+01 | -1.180333e+02 | 5.200000e-02 | 1.270000e+03 | 5.000000e+01 | 4.600000e+01 | 4.800000e+01 | 2.931000e+01 | 1.000000e+01 | 3.500000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.609861e+01 | -9.241808e+01 | 3.609799e+01 | -9.241772e+01 | 2.440000e-01 | 4.007000e+03 | 6.400000e+01 | 6.300000e+01 | 6.700000e+01 | 2.982000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.016024e+01 | -8.037243e+01 | 4.016105e+01 | -8.037338e+01 | 7.640000e-01 | 9.567000e+03 | 7.600000e+01 | 7.600000e+01 | 8.300000e+01 | 3.001000e+01 | 1.000000e+01 | 1.000000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.960000e+02 | 1.960000e+02 | 1.000000e+02 | 5.890000e+01 | 1.400000e+02 | 1.087000e+03 | 2.400000e+01 | . Check for numerical values in the Dataset . numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;] numeric_df = df.select_dtypes(include=numerics) len(numeric_df.columns) . 14 . Check for missing values . df.isna().sum().sort_values(ascending = False) . Number 1743911 Precipitation(in) 549458 Wind_Chill(F) 469643 Wind_Speed(mph) 157944 Wind_Direction 73775 Humidity(%) 73092 Weather_Condition 70636 Visibility(mi) 70546 Temperature(F) 69274 Pressure(in) 59200 Weather_Timestamp 50736 Airport_Code 9549 Timezone 3659 Nautical_Twilight 2867 Civil_Twilight 2867 Sunrise_Sunset 2867 Astronomical_Twilight 2867 Zipcode 1319 City 137 Street 2 Country 0 Junction 0 Start_Time 0 End_Time 0 Start_Lat 0 Turning_Loop 0 Traffic_Signal 0 Traffic_Calming 0 Stop 0 Station 0 Roundabout 0 Railway 0 No_Exit 0 Crossing 0 Give_Way 0 Bump 0 Amenity 0 Start_Lng 0 End_Lat 0 End_Lng 0 Distance(mi) 0 Description 0 Severity 0 Side 0 County 0 State 0 ID 0 dtype: int64 . Let&#39;s convert the above result to percentile to better understand missing data . missing_percentages = df.isna().sum().sort_values(ascending = False)/len(df) missing_percentages . Number 6.129003e-01 Precipitation(in) 1.931079e-01 Wind_Chill(F) 1.650568e-01 Wind_Speed(mph) 5.550967e-02 Wind_Direction 2.592834e-02 Humidity(%) 2.568830e-02 Weather_Condition 2.482514e-02 Visibility(mi) 2.479350e-02 Temperature(F) 2.434646e-02 Pressure(in) 2.080593e-02 Weather_Timestamp 1.783125e-02 Airport_Code 3.356011e-03 Timezone 1.285961e-03 Nautical_Twilight 1.007612e-03 Civil_Twilight 1.007612e-03 Sunrise_Sunset 1.007612e-03 Astronomical_Twilight 1.007612e-03 Zipcode 4.635647e-04 City 4.814887e-05 Street 7.029032e-07 Country 0.000000e+00 Junction 0.000000e+00 Start_Time 0.000000e+00 End_Time 0.000000e+00 Start_Lat 0.000000e+00 Turning_Loop 0.000000e+00 Traffic_Signal 0.000000e+00 Traffic_Calming 0.000000e+00 Stop 0.000000e+00 Station 0.000000e+00 Roundabout 0.000000e+00 Railway 0.000000e+00 No_Exit 0.000000e+00 Crossing 0.000000e+00 Give_Way 0.000000e+00 Bump 0.000000e+00 Amenity 0.000000e+00 Start_Lng 0.000000e+00 End_Lat 0.000000e+00 End_Lng 0.000000e+00 Distance(mi) 0.000000e+00 Description 0.000000e+00 Severity 0.000000e+00 Side 0.000000e+00 County 0.000000e+00 State 0.000000e+00 ID 0.000000e+00 dtype: float64 . type(missing_percentages) . pandas.core.series.Series . let&#39;s remove all the data from missing_percentile that has zero as a value . missing_percentages[missing_percentages !=0] . Number 6.129003e-01 Precipitation(in) 1.931079e-01 Wind_Chill(F) 1.650568e-01 Wind_Speed(mph) 5.550967e-02 Wind_Direction 2.592834e-02 Humidity(%) 2.568830e-02 Weather_Condition 2.482514e-02 Visibility(mi) 2.479350e-02 Temperature(F) 2.434646e-02 Pressure(in) 2.080593e-02 Weather_Timestamp 1.783125e-02 Airport_Code 3.356011e-03 Timezone 1.285961e-03 Nautical_Twilight 1.007612e-03 Civil_Twilight 1.007612e-03 Sunrise_Sunset 1.007612e-03 Astronomical_Twilight 1.007612e-03 Zipcode 4.635647e-04 City 4.814887e-05 Street 7.029032e-07 dtype: float64 . Let&#39;s visualize this in matplotlib . missing_percentages[missing_percentages !=0].plot(kind = &quot;barh&quot;) . &lt;AxesSubplot:&gt; . Exploratory Data Analysis and Visualization . Col to analyze . 1. City 2. Start Time 3. Temprature 4. Weather Condtions 5. Start Lat, Start lng . How many Cities are in the dataset? . len(df.City.unique()) . 11682 . How many States are represented in the dataset? . len(df.State.unique()) . 49 . Car Accident Per Cities . CarAccidentPerCities = df.City.value_counts() CarAccidentPerCities . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 ... Ridgedale 1 Sekiu 1 Wooldridge 1 Bullock 1 American Fork-Pleasant Grove 1 Name: City, Length: 11681, dtype: int64 . CarAccidentPerCities[:20] . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 Charlotte 33152 Sacramento 32559 San Diego 26627 Raleigh 22840 Minneapolis 22768 Portland 20944 Nashville 20267 Austin 18301 Baton Rouge 18182 Phoenix 17143 Saint Paul 16869 New Orleans 16251 Atlanta 15622 Jacksonville 14967 Richmond 14349 Name: City, dtype: int64 . Miami seems to be the city with the highest accident and followed by Los Angeles and Orlando respectively. . Considering the New-York is the most populated city in the US, let&#39;s check the accident report for the city of New-York. | . &quot;NY&quot; in df[&quot;State&quot;].values . True . CarAccidentPerCities[&quot;New York&quot;] . 7068 . Let&#39;s check Minneapolis(My home City) . CarAccidentPerCities[&quot;Minneapolis&quot;] . 22768 . It make sense as the weather is extremely bad during winter. . CarAccidentPerCities[:10].plot(kind = &quot;barh&quot;) CarAccidentPerCities.head(10) . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 Charlotte 33152 Sacramento 32559 San Diego 26627 Raleigh 22840 Minneapolis 22768 Name: City, dtype: int64 . Let&#39;s get a Guassian Distribution Summary for our data to get a better understing about how different varaibles result in an accident. We can visualize this using seaborn . Find out the top cities with top accident and where they located . sns.set_style(&quot;darkgrid&quot;) sns.distplot(CarAccidentPerCities) . &lt;AxesSubplot:xlabel=&#39;City&#39;, ylabel=&#39;Density&#39;&gt; . How many cities report over a 1000 accidents during this period? and plot the distrubition . high_accident_cities = CarAccidentPerCities[CarAccidentPerCities &gt;= 1000] len(high_accident_cities) . 496 . sns.distplot(high_accident_cities) . C: Users Admin anaconda3 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;City&#39;, ylabel=&#39;Density&#39;&gt; . And how many cities reported less than a 1000? and plot the distribution . low_accident_cities = CarAccidentPerCities[CarAccidentPerCities &lt;= 1000] len(low_accident_cities) . 11187 . sns.distplot(low_accident_cities) . C: Users Admin anaconda3 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;City&#39;, ylabel=&#39;Density&#39;&gt; . Let&#39;s use log scale to better understand of our dataset. . sns.histplot(CarAccidentPerCities, log_scale = True) . &lt;AxesSubplot:xlabel=&#39;City&#39;, ylabel=&#39;Count&#39;&gt; . From the above graph, we can see that there seem to be many cities with fewer than 10 accidents. Verify this: . z = CarAccidentPerCities[CarAccidentPerCities &lt;= 10] len(z) . 4628 . Now let&#39;s dig deep and analyze our data based on different Variables... . Start Time . df.Start_Time . 0 2016-02-08 00:37:08 1 2016-02-08 05:56:20 2 2016-02-08 06:15:39 3 2016-02-08 06:51:45 4 2016-02-08 07:53:43 ... 2845337 2019-08-23 18:03:25 2845338 2019-08-23 19:11:30 2845339 2019-08-23 19:00:21 2845340 2019-08-23 19:00:21 2845341 2019-08-23 18:52:06 Name: Start_Time, Length: 2845342, dtype: object . First of all, we can see here the datatype for Time is Object. We need to convert that into Date datatype. . df.Start_Time = pd.to_datetime(df.Start_Time) #Verify is this df.Start_Time . 0 2016-02-08 00:37:08 1 2016-02-08 05:56:20 2 2016-02-08 06:15:39 3 2016-02-08 06:51:45 4 2016-02-08 07:53:43 ... 2845337 2019-08-23 18:03:25 2845338 2019-08-23 19:11:30 2845339 2019-08-23 19:00:21 2845340 2019-08-23 19:00:21 2845341 2019-08-23 18:52:06 Name: Start_Time, Length: 2845342, dtype: datetime64[ns] . Let&#39;s visualize the time of the day when more accidents happen to get better understanding. . df.Start_Time.dt.hour . 0 0 1 5 2 6 3 6 4 7 .. 2845337 18 2845338 19 2845339 19 2845340 19 2845341 18 Name: Start_Time, Length: 2845342, dtype: int64 . sns.distplot(df.Start_Time.dt.hour,bins = 24) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;, ylabel=&#39;Density&#39;&gt; . sns.distplot(df.Start_Time.dt.hour,bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . From the above result, we can conclude that most accidents happen early in the morning when people go to work and in the afternoon when they get back home from work. Let&#39;s also find out what days of the week we have more accidents... . 3# . sns.distplot(df.Start_Time.dt.dayofweek, bins = 7, kde= False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . It appears during the weekdays(Monday = 0...) accidents incidents are fairly distributed. On the weekend, the number of accidents seems to be less. . Check what time of the day accident happens for the weekend. . Saturday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 5] sns.distplot(Saturday_start_time.dt.hour, bins = 24, kde = True, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;, ylabel=&#39;Density&#39;&gt; . Sunday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 6] sns.distplot(Sunday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . The result for the weekend is almost identical. The peak for the accident is in the afternoon around 3: 00 pm . Let&#39;s check the rest of the weekdays. . Mondayday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 0] sns.distplot(Monday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . Tuesday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 1] sns.distplot(Tuesday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . Wednesday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 2] sns.distplot(Wednesday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . Thursday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 3] sns.distplot(Thursday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . Friday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 4] sns.distplot(Friday_start_time.dt.hour, bins = 24, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . On the weekdays, accidents mostly happen during morning and afternoon rush hours, which is the obvious busiest time of the day for motor vehicles. . Check in which months of the year the accidents tend to be high. . sns.distplot(df.Start_Time.dt.month, bins = 12, kde = False, norm_hist = False) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . Wintertime (Oct-Dec) is the worst time. It makes sense as it snows in places in the winter and heavy rains where it doesn&#8217;t snow. . What are top 5 cities per accident? . plt.plot(TopCitiesPerAcc) TopCitiesPerAcc = CarAccidentPerCities.head(5) TopCitiesPerAcc . Miami 106966 Los Angeles 68956 Orlando 54691 Dallas 41979 Houston 39448 Name: City, dtype: int64 . Check accident report per State and what are the 7 top States with the highest accident? . AccidentPerState = df.State.value_counts() AccidentPerState . CA 795868 FL 401388 TX 149037 OR 126341 VA 113535 NY 108049 PA 99975 MN 97185 NC 91362 SC 89216 MD 65085 AZ 56504 NJ 52902 TN 52613 UT 49193 LA 47232 IL 47105 MI 43843 GA 40086 WA 32554 CT 29762 MO 29633 CO 25340 OH 24409 IN 20850 AL 19322 MT 15964 AR 10935 IA 9607 DC 9133 KS 9033 OK 8806 ID 8544 WI 7896 WV 7632 KY 6638 MA 6392 NV 6197 MS 5320 DE 4842 RI 4451 NH 3866 NE 3320 NM 2370 ND 2258 ME 2193 WY 990 VT 365 SD 201 Name: State, dtype: int64 . AccidentPerState.head(7).plot(xlabel = &quot;State&quot;, kind = &quot;barh&quot;, ylabel = &quot;Num of Accidents&quot;, title = &quot;State vs Accident&quot;) AccidentPerState.head(7) . CA 795868 FL 401388 TX 149037 OR 126341 VA 113535 NY 108049 PA 99975 Name: State, dtype: int64 . Accident Per Specific Year . df.Start_Time.dt.year . 0 2016 1 2016 2 2016 3 2016 4 2016 ... 2845337 2019 2845338 2019 2845339 2019 2845340 2019 2845341 2019 Name: Start_Time, Length: 2845342, dtype: int64 . df_2019 = df[df.Start_Time.dt.year == 2019] sns.distplot(df_2019.Start_Time.dt.month, bins = 12, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . df_2018 = df[df.Start_Time.dt.year == 2018] sns.distplot(df_2018.Start_Time.dt.month, bins = 12, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . df_2017 = df[df.Start_Time.dt.year == 2017] sns.distplot(df_2017.Start_Time.dt.month, bins = 12, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . df_2016 = df[df.Start_Time.dt.year == 2016] sns.distplot(df_2016.Start_Time.dt.month, bins = 12, kde = False, norm_hist = True) . &lt;AxesSubplot:xlabel=&#39;Start_Time&#39;&gt; . df.columns . Index([&#39;ID&#39;, &#39;Severity&#39;, &#39;Start_Time&#39;, &#39;End_Time&#39;, &#39;Start_Lat&#39;, &#39;Start_Lng&#39;, &#39;End_Lat&#39;, &#39;End_Lng&#39;, &#39;Distance(mi)&#39;, &#39;Description&#39;, &#39;Number&#39;, &#39;Street&#39;, &#39;Side&#39;, &#39;City&#39;, &#39;County&#39;, &#39;State&#39;, &#39;Zipcode&#39;, &#39;Country&#39;, &#39;Timezone&#39;, &#39;Airport_Code&#39;, &#39;Weather_Timestamp&#39;, &#39;Temperature(F)&#39;, &#39;Wind_Chill(F)&#39;, &#39;Humidity(%)&#39;, &#39;Pressure(in)&#39;, &#39;Visibility(mi)&#39;, &#39;Wind_Direction&#39;, &#39;Wind_Speed(mph)&#39;, &#39;Precipitation(in)&#39;, &#39;Weather_Condition&#39;, &#39;Amenity&#39;, &#39;Bump&#39;, &#39;Crossing&#39;, &#39;Give_Way&#39;, &#39;Junction&#39;, &#39;No_Exit&#39;, &#39;Railway&#39;, &#39;Roundabout&#39;, &#39;Station&#39;, &#39;Stop&#39;, &#39;Traffic_Calming&#39;, &#39;Traffic_Signal&#39;, &#39;Turning_Loop&#39;, &#39;Sunrise_Sunset&#39;, &#39;Civil_Twilight&#39;, &#39;Nautical_Twilight&#39;, &#39;Astronomical_Twilight&#39;], dtype=&#39;object&#39;) . Summary and Conclusions . Insights: . - The number of accidents per city decreases exponentially - Less 5% of cities have more than 1000 yearly accidents - Over 1200 cities reported just one accident(This needs to be further investigated) - Accidents for weekdays seem to be higher than the weekend. - Accidents seem to be high for the winter time. . Further Analysis to be continued... .",
            "url": "https://seifhh.github.io/DS_Projects/2022/01/29/US-NATION-WIDE-CAR-ACCIDENT-EDA-REPORT.html",
            "relUrl": "/2022/01/29/US-NATION-WIDE-CAR-ACCIDENT-EDA-REPORT.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "We will be performing EDA on FIFA 2018 DATASET. We will be using both SQL and Python combination to do the analysis in Jupyter Notebook. The right, right from Jupyter IDE. . To be able to run SQL in Jupyter Notebook, we need to download the necessary modules and access the dataset from a local database. . The Dataset we are using can be accessed from: https://public.tableau.com/en-us/s/resources . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import pyodbc conn_str = ( r&#39;DRIVER={SQL Server};&#39; r&#39;SERVER=SEIFEDIN SQLEXPRESS;&#39; r&#39;DATABASE=Fifa 18;&#39; r&#39;Trusted_Connection=yes;&#39; ) cnxn = pyodbc.connect(conn_str) query = &quot;SELECT * FROM Fifa18&quot; cursor = cnxn.cursor() # cursor.execute(&quot;SELECT * FROM Fifa18&quot;) df = pd.read_sql(query, cnxn) df . Wage (€) Value (€) Name Age Photo Nationality Flag Overall Potential Club ... RB RCB RCM RDM RF RM RS RW RWB ST . 0 565000.0 | 95500000.0 | Cristiano Ronaldo | 32.0 | https://cdn.sofifa.org/48/18/players/20801.png | Portugal | https://cdn.sofifa.org/flags/38.png | 94.0 | 94.0 | Real Madrid CF | ... | 61.0 | 53.0 | 82.0 | 62.0 | 91.0 | 89.0 | 92.0 | 91.0 | 66.0 | 92.0 | . 1 565000.0 | 105000000.0 | L. Messi | 30.0 | https://cdn.sofifa.org/48/18/players/158023.png | Argentina | https://cdn.sofifa.org/flags/52.png | 93.0 | 93.0 | FC Barcelona | ... | 57.0 | 45.0 | 84.0 | 59.0 | 92.0 | 90.0 | 88.0 | 91.0 | 62.0 | 88.0 | . 2 280000.0 | 123000000.0 | Neymar | 25.0 | https://cdn.sofifa.org/48/18/players/190871.png | Brazil | https://cdn.sofifa.org/flags/54.png | 92.0 | 94.0 | Paris Saint-Germain | ... | 59.0 | 46.0 | 79.0 | 59.0 | 88.0 | 87.0 | 84.0 | 89.0 | 64.0 | 84.0 | . 3 510000.0 | 97000000.0 | L. Suárez | 30.0 | https://cdn.sofifa.org/48/18/players/176580.png | Uruguay | https://cdn.sofifa.org/flags/60.png | 92.0 | 92.0 | FC Barcelona | ... | 64.0 | 58.0 | 80.0 | 65.0 | 88.0 | 85.0 | 88.0 | 87.0 | 68.0 | 88.0 | . 4 230000.0 | 61000000.0 | M. Neuer | 31.0 | https://cdn.sofifa.org/48/18/players/167495.png | Germany | https://cdn.sofifa.org/flags/21.png | 92.0 | 92.0 | FC Bayern Munich | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 17976 5000.0 | 625000.0 | A. Muric | 18.0 | https://cdn.sofifa.org/48/18/players/233164.png | Switzerland | https://cdn.sofifa.org/flags/47.png | 63.0 | 81.0 | Manchester City | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 17977 4000.0 | 375000.0 | W. Atkinson | 28.0 | https://cdn.sofifa.org/48/18/players/186572.png | England | https://cdn.sofifa.org/flags/14.png | 63.0 | 63.0 | Mansfield Town | ... | 60.0 | 57.0 | 59.0 | 60.0 | 59.0 | 62.0 | 58.0 | 60.0 | 62.0 | 58.0 | . 17978 4000.0 | 675000.0 | M. Beerman | 18.0 | https://cdn.sofifa.org/48/18/players/233165.png | Malta | https://cdn.sofifa.org/flags/32.png | 63.0 | 80.0 | Rangers | ... | 62.0 | 57.0 | 56.0 | 58.0 | 53.0 | 59.0 | 48.0 | 58.0 | 63.0 | 48.0 | . 17979 4000.0 | 625000.0 | P. Burner | 21.0 | https://cdn.sofifa.org/48/18/players/235981.png | France | https://cdn.sofifa.org/flags/18.png | 63.0 | 76.0 | OGC Nice | ... | 62.0 | 60.0 | 60.0 | 62.0 | 56.0 | 60.0 | 53.0 | 58.0 | 63.0 | 53.0 | . 17980 4000.0 | 220000.0 | A. Al Mazaidi | 31.0 | https://cdn.sofifa.org/48/18/players/191437.png | Saudi Arabia | https://cdn.sofifa.org/flags/183.png | 63.0 | 63.0 | Al Fateh | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 17981 rows × 74 columns . df.head(5) . Wage (€) Value (€) Name Age Photo Nationality Flag Overall Potential Club ... RB RCB RCM RDM RF RM RS RW RWB ST . 0 565000.0 | 95500000.0 | Cristiano Ronaldo | 32.0 | https://cdn.sofifa.org/48/18/players/20801.png | Portugal | https://cdn.sofifa.org/flags/38.png | 94.0 | 94.0 | Real Madrid CF | ... | 61.0 | 53.0 | 82.0 | 62.0 | 91.0 | 89.0 | 92.0 | 91.0 | 66.0 | 92.0 | . 1 565000.0 | 105000000.0 | L. Messi | 30.0 | https://cdn.sofifa.org/48/18/players/158023.png | Argentina | https://cdn.sofifa.org/flags/52.png | 93.0 | 93.0 | FC Barcelona | ... | 57.0 | 45.0 | 84.0 | 59.0 | 92.0 | 90.0 | 88.0 | 91.0 | 62.0 | 88.0 | . 2 280000.0 | 123000000.0 | Neymar | 25.0 | https://cdn.sofifa.org/48/18/players/190871.png | Brazil | https://cdn.sofifa.org/flags/54.png | 92.0 | 94.0 | Paris Saint-Germain | ... | 59.0 | 46.0 | 79.0 | 59.0 | 88.0 | 87.0 | 84.0 | 89.0 | 64.0 | 84.0 | . 3 510000.0 | 97000000.0 | L. Suárez | 30.0 | https://cdn.sofifa.org/48/18/players/176580.png | Uruguay | https://cdn.sofifa.org/flags/60.png | 92.0 | 92.0 | FC Barcelona | ... | 64.0 | 58.0 | 80.0 | 65.0 | 88.0 | 85.0 | 88.0 | 87.0 | 68.0 | 88.0 | . 4 230000.0 | 61000000.0 | M. Neuer | 31.0 | https://cdn.sofifa.org/48/18/players/167495.png | Germany | https://cdn.sofifa.org/flags/21.png | 92.0 | 92.0 | FC Bayern Munich | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 74 columns . Let&#39;s check how many countries are represented in the dataset. . Nationality = &quot;SELECT COUNT(DISTINCT Nationality) AS Nationality FROM Fifa18&quot; df= pd.read_sql(Nationality, cnxn) df . Nationality . 0 165 | . Let&#39;s chech how many players are in the FIFA . Num_players = &quot;SELECT COUNT(*) AS Num_Of_Players FROM Fifa18&quot; df= pd.read_sql(Num_players, cnxn) df . Num_Of_Players . 0 17981 | . There are 17981 players in the FIFA . Find out how many players are from each country... . NumOfPlayerPerCuntry = &quot;SELECT COUNT(*) AS NumOfPlayerPerCuntry, Nationality FROM Fifa18 GROUP BY Nationality ORDER BY NumOfPlayerPerCuntry DESC&quot; df= pd.read_sql(NumOfPlayerPerCuntry, cnxn) df.head(10) . NumOfPlayerPerCuntry Nationality . 0 1630 | England | . 1 1140 | Germany | . 2 1019 | Spain | . 3 978 | France | . 4 965 | Argentina | . 5 812 | Brazil | . 6 799 | Italy | . 7 592 | Colombia | . 8 469 | Japan | . 9 429 | Netherlands | . What&#39;s the highest Salary paid to a player? . Highest_Salary = &quot;SELECT MAX([Wage (€)]) AS Max_Salary FROM Fifa18&quot; df= pd.read_sql(Highest_Salary, cnxn) df . Max_Salary . 0 565000.0 | . Which player is getting paid the highest and where is he from? . HighestPaidPlayer = &quot;SELECT Name, Nationality FROM Fifa18 WHERE [Wage (€)] = 565000&quot; df= pd.read_sql(HighestPaidPlayer, cnxn) df . Name Nationality . 0 Cristiano Ronaldo | Portugal | . 1 L. Messi | Argentina | . It appears we have ties. The two GOATS(CR7 and L.Messi Make the same amount) . HighestPaidPlayer = &quot;SELECT Name, [Wage (€)], Nationality FROM Fifa18 WHERE [Wage (€)] = (SELECT MAX([Wage (€)]) FROM Fifa18)&quot; df= pd.read_sql(HighestPaidPlayer, cnxn) df . Name Wage (€) Nationality . 0 Cristiano Ronaldo | 565000.0 | Portugal | . 1 L. Messi | 565000.0 | Argentina | . What&#39;s the minimum Salary paid? . Min_Salary = &quot;SELECT Min([Wage (€)]) FROM Fifa18&quot; df= pd.read_sql(Min_Salary, cnxn) df . . 0 0.0 | . It appears the are some people who are just volunteers. . Which player is got the highest rating and from which club? USE THE NESTED QUERY METHOD FOR THIS. . Top_player = &quot;SELECT Name, Overall, Club FROM Fifa18 WHERE Overall = (SELECT MAX(Overall) FROM Fifa18)&quot; df = pd.read_sql(Top_player, cnxn) df . Name Overall Club . 0 Cristiano Ronaldo | 94.0 | Real Madrid CF | . Cristiano Ronaldo is the highest rated player in FIFA . Which are top 3 clubs based on overall rating ? . Top_Clubs = &quot;SELECT TOP 4 AVG(Overall) AS Avg_Overall_Rating, Club FROM Fifa18 GROUP BY Club ORDER BY AVG(Overall) DESC&quot; df = pd.read_sql(Top_Clubs, cnxn) df . Avg_Overall_Rating Club . 0 82.560000 | FC Barcelona | . 1 81.653846 | Juventus | . 2 81.038462 | Real Madrid CF | . 3 79.423077 | FC Bayern Munich | . We can even combine with Viz from here. We can run both Matplotlib or Seaborn right here at the same time . fig = plt.figure() ax = fig.add_axes([5,5,1,1]) x_ax = df[&quot;Club&quot;] y_ax = df[&quot;Avg_Overall_Rating&quot;] ax.bar(x_ax, y_ax) ax.set_xlabel(&quot;Clubs&quot;) ax.set_ylabel(&quot;Overall Rating&quot;) ax.set_title(&quot;Highest Rated Clubs in Fifa&quot;) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . IF YOU WANT TO USE JUST SQL TO PERFORM YOUR EDA, BELOW IS THE CODE. . /* We will be performing Explatory Data Analysis on a FIFA 2018 DataSet. . DateSet can be found here if further research/Exploration needs to be done. . Data Source: https://public.tableau.com/en-us/s/resources . */ . -- Check our dataset. . SELECT * . FROM . Fifa18 . --Check how many countries are represented . SELECT COUNT(DISTINCT Nationality) . FROM . DBO.Fifa18 . --Check the number of players in FIFA . SELECT COUNT(*) as Number_of_Players FROM Fifa18 . --How many players from each country. Check the top 5 . SELECT TOP 5 COUNT(*) AS NumOfPlayer_FromEach_Country, Nationality . FROM Fifa18 . GROUP BY nationality ORDER BY NumOfPlayer_FromEach_Country DESC . --England appears to have the most players in FIFA. . --What&#39;s the highest Salary paid to a player . SELECT . MAX([Wage (€)]) AS Maximum_Salary . FROM Fifa18 . -- Which player is getting paid the highest and where is he from? . SELECT Name, Nationality . FROM Fifa18 . WHERE [Wage (€)] = 565000 --Interestingly, we have two players getting the highest salary. The two goats(CR7, and Messi) . --We can also do it this way. Nesting a query within a query(Outer query) . SELECT name, [Wage (€)] . FROM Fifa18 WHERE [Wage (€)] = (SELECT max([Wage (€)]) FROM Fifa18) . What&#39;s the minimum Salary paid? . SELECT min([Wage (€)]) FROM Fifa18 . --It appears some people work in FIFA volunterly. . --Which player is got the highest rating and from which club? USE THE NESTED QUERY METHOD FOR THIS. . SELECT NAME, OVERALL, CLUB FROM Fifa18 WHERE OVERALL = (SELECT MAX(OVERALL) FROM Fifa18) --My main man Critiano Ronaldo(CR7) Comes on top. . --Which are top 3 clubs based on overall rating ? . SELECT TOP 5 AVG(overall) as Average_Overall_rating, CLUB FROM Fifa18 GROUP BY CLUB ORDER BY Average_Overall_rating DESC .",
            "url": "https://seifhh.github.io/DS_Projects/2022/01/23/SQL-IN-PYTHON.html",
            "relUrl": "/2022/01/23/SQL-IN-PYTHON.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "COVID-19 WORLWIDE DATASET ANALYSIS.",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt print(&quot;Modules are imported&quot;) . Modules are imported . corona_dataset_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set/covid19_Confirmed_dataset.csv&quot;) . corona_dataset_csv.head(100) #Let&#39;s check what our data looks like. . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 NaN | Djibouti | 11.8251 | 42.5903 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 945 | 974 | 986 | 999 | 1008 | 1023 | 1035 | 1072 | 1077 | 1089 | . 96 NaN | Dominican Republic | 18.7357 | -70.1627 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5044 | 5300 | 5543 | 5749 | 5926 | 6135 | 6293 | 6416 | 6652 | 6972 | . 97 NaN | Ecuador | -1.8312 | -78.1834 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10398 | 10850 | 11183 | 22719 | 22719 | 22719 | 23240 | 24258 | 24675 | 24934 | . 98 NaN | Egypt | 26.0000 | 30.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3490 | 3659 | 3891 | 4092 | 4319 | 4534 | 4782 | 5042 | 5268 | 5537 | . 99 NaN | El Salvador | 13.7942 | -88.8965 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 225 | 237 | 250 | 274 | 274 | 298 | 323 | 345 | 377 | 395 | . 100 rows × 104 columns . corona_dataset_csv.shape #Always good to check the shape( #of rows and column) of our data_set. . (266, 104) . . Adjusted_df = corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;], axis = 1) #Using drop method, we can drop the two col we don&#39;t need. Make sure to identify the axis. . Adjusted_df . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 261 NaN | Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . 262 NaN | Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8 | 14 | . 263 NaN | Yemen | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . 264 NaN | Comoros | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 265 NaN | Tajikistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | . 266 rows × 102 columns . # Using drop method will not delete the two column from the original data set. It just won&#39;t show in our new data from. to remove it from the original data set, # we can do as follows: corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;],axis = 1,inplace = True) #Inplace will drop the two unwatnted col. . corona_dataset_csv.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . 5 rows × 102 columns . Aggregating the rows by country: . Instead of having multiple data from the same country based on the province/region, we can combine them and get a signle dataset for each country. . corona_dataset_aggregated= corona_dataset_csv.groupby(&#39;Country/Region&#39;).sum() # This will group data from each region and sum up the total to result in a single output for each country corona_dataset_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 466 | 474 | 480 | 484 | 342 | 342 | 342 | 343 | 344 | 344 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 70 | 74 | 76 | 84 | 84 | 88 | 88 | 95 | 97 | 106 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 29 | 31 | 31 | 32 | 32 | 32 | 40 | . 187 rows × 100 columns . corona_dataset_aggregated.shape . (187, 100) . . Performing Visualisations . corona_dataset_aggregated.loc[&#39;Australia&#39;] #Showing data for Australia . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 4 ... 4/26/20 6714 4/27/20 6721 4/28/20 6744 4/29/20 6752 4/30/20 6766 Name: Australia, Length: 100, dtype: int64 . . corona_dataset_aggregated.loc[&#39;Australia&#39;].plot() . &lt;AxesSubplot:&gt; . corona_dataset_aggregated.loc[&#39;China&#39;].plot() corona_dataset_aggregated.loc[&#39;Italy&#39;].plot() corona_dataset_aggregated.loc[&#39;Spain&#39;].plot() plt.legend() #This will add the legend to make it easy to identify . &lt;matplotlib.legend.Legend at 0x2985f665f10&gt; . Calculating a good measure to do the analysis. Let&#39;s findout the spread of the virus in each country . . corona_dataset_aggregated.loc[&#39;China&#39;][:7].plot() . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;Australia&#39;].diff().plot() #This will show the change in infection rate day by day. . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;China&#39;].diff().max() #This is 24 hours change in China. . 15136.0 . corona_dataset_aggregated.loc[&#39;Italy&#39;].diff().max() . 6557.0 . corona_dataset_aggregated.loc[&#39;Spain&#39;].diff().max() . 9630.0 . #Since the indexes of our dataset is a list of coutries,we can do as follow: countries = list(corona_dataset_aggregated.index) #Create an empty list and append the result of each countries infection rate into the new list max_infection_rates = [] for c in countries: max_infection_rates.append(corona_dataset_aggregated.loc[c].diff().max()) max_infection_rates . [232.0, 34.0, 199.0, 43.0, 5.0, 6.0, 291.0, 134.0, 497.0, 1321.0, 105.0, 7.0, 301.0, 641.0, 12.0, 1485.0, 2454.0, 4.0, 19.0, 1.0, 104.0, 92.0, 7.0, 7502.0, 26.0, 137.0, 41.0, 21.0, 6.0, 45.0, 31.0, 203.0, 2778.0, 31.0, 21.0, 1138.0, 15136.0, 353.0, 1.0, 57.0, 81.0, 37.0, 113.0, 96.0, 63.0, 58.0, 381.0, 391.0, 99.0, 156.0, 5.0, 371.0, 11536.0, 269.0, 32.0, 130.0, 7.0, 134.0, 20.0, 9.0, 5.0, 267.0, 26849.0, 38.0, 5.0, 42.0, 6933.0, 403.0, 156.0, 6.0, 68.0, 167.0, 132.0, 12.0, 10.0, 3.0, 72.0, 210.0, 99.0, 1893.0, 436.0, 3186.0, 91.0, 1515.0, 1131.0, 6557.0, 52.0, 1161.0, 40.0, 264.0, 29.0, 851.0, 289.0, 300.0, 69.0, 3.0, 48.0, 61.0, 17.0, 13.0, 21.0, 90.0, 234.0, 7.0, 14.0, 10.0, 235.0, 190.0, 58.0, 52.0, 2.0, 41.0, 1425.0, 222.0, 12.0, 13.0, 30.0, 281.0, 19.0, 3.0, 14.0, 1346.0, 89.0, 2.0, 69.0, 208.0, 107.0, 386.0, 144.0, 1292.0, 357.0, 5.0, 27.0, 3683.0, 538.0, 545.0, 1516.0, 957.0, 523.0, 7099.0, 22.0, 5.0, 6.0, 4.0, 54.0, 6.0, 1351.0, 87.0, 2379.0, 2.0, 20.0, 1426.0, 114.0, 70.0, 73.0, 354.0, 28.0, 9630.0, 65.0, 67.0, 3.0, 812.0, 1321.0, 6.0, 27.0, 15.0, 181.0, 188.0, 10.0, 14.0, 40.0, 82.0, 5138.0, 36188.0, 11.0, 578.0, 552.0, 8733.0, 48.0, 167.0, 29.0, 19.0, 66.0, 4.0, 5.0, 9.0, 8.0] . . corona_dataset_aggregated[&#39;max_infection_rate&#39;] = max_infection_rates . corona_dataset_aggregated.columns #Our new col is included in the end . Index([&#39;1/22/20&#39;, &#39;1/23/20&#39;, &#39;1/24/20&#39;, &#39;1/25/20&#39;, &#39;1/26/20&#39;, &#39;1/27/20&#39;, &#39;1/28/20&#39;, &#39;1/29/20&#39;, &#39;1/30/20&#39;, &#39;1/31/20&#39;, ... &#39;4/22/20&#39;, &#39;4/23/20&#39;, &#39;4/24/20&#39;, &#39;4/25/20&#39;, &#39;4/26/20&#39;, &#39;4/27/20&#39;, &#39;4/28/20&#39;, &#39;4/29/20&#39;, &#39;4/30/20&#39;, &#39;max_infection_rate&#39;], dtype=&#39;object&#39;, length=101) . corona_dataset_aggregated.index . Index([&#39;Afghanistan&#39;, &#39;Albania&#39;, &#39;Algeria&#39;, &#39;Andorra&#39;, &#39;Angola&#39;, &#39;Antigua and Barbuda&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;, &#39;Austria&#39;, ... &#39;United Kingdom&#39;, &#39;Uruguay&#39;, &#39;Uzbekistan&#39;, &#39;Venezuela&#39;, &#39;Vietnam&#39;, &#39;West Bank and Gaza&#39;, &#39;Western Sahara&#39;, &#39;Yemen&#39;, &#39;Zambia&#39;, &#39;Zimbabwe&#39;], dtype=&#39;object&#39;, name=&#39;Country/Region&#39;, length=187) . corona_dataset_aggregated.head() . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 max_infection_rate . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | 232.0 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | 34.0 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | 199.0 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | 43.0 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | 5.0 | . 5 rows × 101 columns . . corona_data = pd.DataFrame(corona_dataset_aggregated[&#39;max_infection_rate&#39;]) corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . . happiness_report_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set worldwide_happiness_report.csv&quot;) . happiness_report_csv.head() . Overall rank Country or region Score GDP per capita Social support Healthy life expectancy Freedom to make life choices Generosity Perceptions of corruption . 0 1 | Finland | 7.769 | 1.340 | 1.587 | 0.986 | 0.596 | 0.153 | 0.393 | . 1 2 | Denmark | 7.600 | 1.383 | 1.573 | 0.996 | 0.592 | 0.252 | 0.410 | . 2 3 | Norway | 7.554 | 1.488 | 1.582 | 1.028 | 0.603 | 0.271 | 0.341 | . 3 4 | Iceland | 7.494 | 1.380 | 1.624 | 1.026 | 0.591 | 0.354 | 0.118 | . 4 5 | Netherlands | 7.488 | 1.396 | 1.522 | 0.999 | 0.557 | 0.322 | 0.298 | . happiness_report_csv.head() . Overall rank Country or region Score GDP per capita Social support Healthy life expectancy Freedom to make life choices Generosity Perceptions of corruption . 0 1 | Finland | 7.769 | 1.340 | 1.587 | 0.986 | 0.596 | 0.153 | 0.393 | . 1 2 | Denmark | 7.600 | 1.383 | 1.573 | 0.996 | 0.592 | 0.252 | 0.410 | . 2 3 | Norway | 7.554 | 1.488 | 1.582 | 1.028 | 0.603 | 0.271 | 0.341 | . 3 4 | Iceland | 7.494 | 1.380 | 1.624 | 1.026 | 0.591 | 0.354 | 0.118 | . 4 5 | Netherlands | 7.488 | 1.396 | 1.522 | 0.999 | 0.557 | 0.322 | 0.298 | . . useless_cols = [&quot;Overall rank&quot;, &quot;Score&quot;,&quot;Generosity&quot;, &quot;Perceptions of corruption&quot;] . happiness_report_csv.drop(useless_cols, axis = 1, inplace = True) . happiness_report_csv.head() . Country or region GDP per capita Social support Healthy life expectancy Freedom to make life choices . 0 Finland | 1.340 | 1.587 | 0.986 | 0.596 | . 1 Denmark | 1.383 | 1.573 | 0.996 | 0.592 | . 2 Norway | 1.488 | 1.582 | 1.028 | 0.603 | . 3 Iceland | 1.380 | 1.624 | 1.026 | 0.591 | . 4 Netherlands | 1.396 | 1.522 | 0.999 | 0.557 | . . happiness_report_csv.set_index(&quot;Country or region&quot;, inplace = True) . happiness_report_csv.head() . GDP per capita Social support Healthy life expectancy Freedom to make life choices . Country or region . Finland 1.340 | 1.587 | 0.986 | 0.596 | . Denmark 1.383 | 1.573 | 0.996 | 0.592 | . Norway 1.488 | 1.582 | 1.028 | 0.603 | . Iceland 1.380 | 1.624 | 1.026 | 0.591 | . Netherlands 1.396 | 1.522 | 0.999 | 0.557 | . Now let&#39;s join the world happiness dataset with the corona dataset.* . corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . corona_data.shape . (187, 1) . happiness_report_csv . GDP per capita Social support Healthy life expectancy Freedom to make life choices . Country or region . Finland 1.340 | 1.587 | 0.986 | 0.596 | . Denmark 1.383 | 1.573 | 0.996 | 0.592 | . Norway 1.488 | 1.582 | 1.028 | 0.603 | . Iceland 1.380 | 1.624 | 1.026 | 0.591 | . Netherlands 1.396 | 1.522 | 0.999 | 0.557 | . ... ... | ... | ... | ... | . Rwanda 0.359 | 0.711 | 0.614 | 0.555 | . Tanzania 0.476 | 0.885 | 0.499 | 0.417 | . Afghanistan 0.350 | 0.517 | 0.361 | 0.000 | . Central African Republic 0.026 | 0.000 | 0.105 | 0.225 | . South Sudan 0.306 | 0.575 | 0.295 | 0.010 | . 156 rows × 4 columns . happiness_report_csv.shape . (156, 4) . Since the number of countries in the happiness dataset is smaller than that of Corona dataset, we need to perform inner join to combine them on country col. . data = corona_data.join(happiness_report_csv, how = &quot;inner&quot;) #Performing inner join. data.head() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . Afghanistan 232.0 | 0.350 | 0.517 | 0.361 | 0.000 | . Albania 34.0 | 0.947 | 0.848 | 0.874 | 0.383 | . Algeria 199.0 | 1.002 | 1.160 | 0.785 | 0.086 | . Argentina 291.0 | 1.092 | 1.432 | 0.881 | 0.471 | . Armenia 134.0 | 0.850 | 1.055 | 0.815 | 0.283 | . Let&#39;s check if there&#39;s any correlation between the factors in the above dataset. . data.corr() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . max_infection_rate 1.000000 | 0.250118 | 0.191958 | 0.289263 | 0.078196 | . GDP per capita 0.250118 | 1.000000 | 0.759468 | 0.863062 | 0.394603 | . Social support 0.191958 | 0.759468 | 1.000000 | 0.765286 | 0.456246 | . Healthy life expectancy 0.289263 | 0.863062 | 0.765286 | 1.000000 | 0.427892 | . Freedom to make life choices 0.078196 | 0.394603 | 0.456246 | 0.427892 | 1.000000 | . Now let&#39;s visualize the correlation and the bigger picture. Better way to do this is using seaborn. . data.head() . max_infection_rate GDP per capita Social support Healthy life expectancy Freedom to make life choices . Afghanistan 232.0 | 0.350 | 0.517 | 0.361 | 0.000 | . Albania 34.0 | 0.947 | 0.848 | 0.874 | 0.383 | . Algeria 199.0 | 1.002 | 1.160 | 0.785 | 0.086 | . Argentina 291.0 | 1.092 | 1.432 | 0.881 | 0.471 | . Armenia 134.0 | 0.850 | 1.055 | 0.815 | 0.283 | . Let&#39;s plot GDP vs Maximum infection rate. . x = data[&quot;GDP per capita&quot;] y = data[&quot;max_infection_rate&quot;] sns.scatterplot(x = x, y = y) . &lt;AxesSubplot:xlabel=&#39;GDP per capita&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . . sns.scatterplot(x = x, y = np.log(y)) #This will apply log scaling into the y-axis. . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . sns.regplot(x = x, y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . . x = data[&quot;Healthy life expectancy&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x = x,y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Healthy life expectancy&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . x = data[&quot;Social support&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x = x, y = np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Social support&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . x = data[&quot;Freedom to make life choices&quot;] y = data[&quot;max_infection_rate&quot;] sns.regplot(x=x, y= np.log(y)) . &lt;AxesSubplot:xlabel=&#39;Freedom to make life choices&#39;, ylabel=&#39;max_infection_rate&#39;&gt; . Surprisingly, there seems to be a positive correlation in developed country than the rest, which indicates, people in a developed countries are prone to catching Covid than those in the developing country. In order to prove this, let&#39;s compare the result to the confirmed death dataset . covid_death_dataset = pd.read_csv(&quot;D: Data Science Covid_Data_Set covid19_deaths_dataset.csv&quot;) . covid_death_dataset.head(10) . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . 5 NaN | Antigua and Barbuda | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 6 NaN | Argentina | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 147 | 152 | 165 | 176 | 185 | 192 | 197 | 207 | 214 | 218 | . 7 NaN | Armenia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 24 | 24 | 27 | 28 | 28 | 29 | 30 | 30 | 32 | . 8 Australian Capital Territory | Australia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 9 New South Wales | Australia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 26 | 31 | 33 | 33 | 34 | 34 | 39 | 40 | 41 | . 10 rows × 102 columns . covid_death_dataset.shape . (266, 104) . covid_death_dataset.drop([&quot;Lat&quot;, &quot;Long&quot;], axis = 1, inplace = True) . covid_death_dataset.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . 5 rows × 102 columns . Let&#39;s perform some cleanup on death dataset. . covid_death_dataset.shape . (266, 102) . Let&#39;s aggregate data into a country. . covid_death_aggregated = covid_death_dataset.groupby(&quot;Country/Region&quot;).sum() covid_death_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 36 | 40 | 42 | 43 | 47 | 50 | 57 | 58 | 60 | 64 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 27 | 27 | 27 | 27 | 28 | 28 | 30 | 30 | 31 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 392 | 402 | 407 | 415 | 419 | 425 | 432 | 437 | 444 | 450 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 37 | 37 | 37 | 40 | 40 | 40 | 40 | 41 | 42 | 42 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 2 | 2 | 2 | 2 | 2 | 2 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | . 187 rows × 100 columns . covid_death_aggregated.shape . (187, 100) . . Covid_death_aggregated.loc[&quot;Australia&quot;] . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 0 .. 4/26/20 83 4/27/20 83 4/28/20 89 4/29/20 91 4/30/20 93 Name: Australia, Length: 100, dtype: int64 . Let&#39;s perform visualisations on death dataset. . Covid_death_aggregated.loc[&quot;Australia&quot;].plot() . &lt;AxesSubplot:&gt; . Covid_death_aggregated.loc[&quot;Australia&quot;].plot() Covid_death_aggregated.loc[&quot;Spain&quot;].plot() Covid_death_aggregated.loc[&quot;China&quot;].plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x2986139b850&gt; . Let&#39;s calculate change in death rate . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().plot() . &lt;AxesSubplot:&gt; . Let&#39;s calculate the max death rate in 24 hrs . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().max() . 8.0 . Covid_death_aggregated.loc[&quot;Germany&quot;].diff().max() . 510.0 . Covid_death_aggregated.loc[&quot;Australia&quot;].diff().max() .",
            "url": "https://seifhh.github.io/DS_Projects/2022/01/22/COVID-19-Data-Analysis.html",
            "relUrl": "/2022/01/22/COVID-19-Data-Analysis.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "COVID-19 WORLWIDE DATASET ANALYSIS.",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt print(&quot;Modules are imported&quot;) . Modules are imported . corona_dataset_csv = pd.read_csv(&quot;D: Data Science Covid_Data_Set/covid19_Confirmed_dataset.csv&quot;) . corona_dataset_csv.head(100) #Let&#39;s check what our data looks like. . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 NaN | Djibouti | 11.8251 | 42.5903 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 945 | 974 | 986 | 999 | 1008 | 1023 | 1035 | 1072 | 1077 | 1089 | . 96 NaN | Dominican Republic | 18.7357 | -70.1627 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5044 | 5300 | 5543 | 5749 | 5926 | 6135 | 6293 | 6416 | 6652 | 6972 | . 97 NaN | Ecuador | -1.8312 | -78.1834 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10398 | 10850 | 11183 | 22719 | 22719 | 22719 | 23240 | 24258 | 24675 | 24934 | . 98 NaN | Egypt | 26.0000 | 30.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3490 | 3659 | 3891 | 4092 | 4319 | 4534 | 4782 | 5042 | 5268 | 5537 | . 99 NaN | El Salvador | 13.7942 | -88.8965 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 225 | 237 | 250 | 274 | 274 | 298 | 323 | 345 | 377 | 395 | . 100 rows × 104 columns . corona_dataset_csv.shape #Always good to check the shape( #of rows and column) of our data_set. . (266, 104) . . Adjusted_df = corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;], axis = 1) #Using drop method, we can drop the two col we don&#39;t need. Make sure to identify the axis. . Adjusted_df . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 261 NaN | Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . 262 NaN | Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8 | 14 | . 263 NaN | Yemen | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . 264 NaN | Comoros | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 265 NaN | Tajikistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | . 266 rows × 102 columns . # Using drop method will not delete the two column from the original data set. It just won&#39;t show in our new data from. to remove it from the original data set, # we can do as follows: corona_dataset_csv.drop([&quot;Lat&quot;,&quot;Long&quot;],axis = 1,inplace = True) #Inplace will drop the two unwatnted col. . corona_dataset_csv.head() . Province/State Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . 0 NaN | Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . 1 NaN | Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . 2 NaN | Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . 3 NaN | Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . 4 NaN | Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . 5 rows × 102 columns . Aggregating the rows by country: . Instead of having multiple data from the same country based on the province/region, we can combine them and get a signle dataset for each country. . corona_dataset_aggregated= corona_dataset_csv.groupby(&#39;Country/Region&#39;).sum() # This will group data from each region and sum up the total to result in a single output for each country corona_dataset_aggregated . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/21/20 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1092 | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 609 | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2811 | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 717 | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 24 | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . West Bank and Gaza 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 466 | 474 | 480 | 484 | 342 | 342 | 342 | 343 | 344 | 344 | . Western Sahara 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | . Yemen 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 6 | . Zambia 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 70 | 74 | 76 | 84 | 84 | 88 | 88 | 95 | 97 | 106 | . Zimbabwe 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 29 | 31 | 31 | 32 | 32 | 32 | 40 | . 187 rows × 100 columns . corona_dataset_aggregated.shape . (187, 100) . . Performing Visualisations . corona_dataset_aggregated.loc[&#39;Australia&#39;] #Showing data for Australia . 1/22/20 0 1/23/20 0 1/24/20 0 1/25/20 0 1/26/20 4 ... 4/26/20 6714 4/27/20 6721 4/28/20 6744 4/29/20 6752 4/30/20 6766 Name: Australia, Length: 100, dtype: int64 . . corona_dataset_aggregated.loc[&#39;Australia&#39;].plot() . &lt;AxesSubplot:&gt; . corona_dataset_aggregated.loc[&#39;China&#39;].plot() corona_dataset_aggregated.loc[&#39;Italy&#39;].plot() corona_dataset_aggregated.loc[&#39;Spain&#39;].plot() plt.legend() #This will add the legend to make it easy to identify . &lt;matplotlib.legend.Legend at 0x2985f665f10&gt; . Calculating a good measure to do the analysis. Let&#39;s findout the spread of the virus in each country . . corona_dataset_aggregated.loc[&#39;China&#39;][:7].plot() . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;Australia&#39;].diff().plot() #This will show the change in infection rate day by day. . &lt;AxesSubplot:&gt; . . corona_dataset_aggregated.loc[&#39;China&#39;].diff().max() #This is 24 hours change in China. . 15136.0 . corona_dataset_aggregated.loc[&#39;Italy&#39;].diff().max() . 6557.0 . corona_dataset_aggregated.loc[&#39;Spain&#39;].diff().max() . 9630.0 . #Since the indexes of our dataset is a list of coutries,we can do as follow: countries = list(corona_dataset_aggregated.index) #Create an empty list and append the result of each countries infection rate into the new list max_infection_rates = [] for c in countries: max_infection_rates.append(corona_dataset_aggregated.loc[c].diff().max()) max_infection_rates . [232.0, 34.0, 199.0, 43.0, 5.0, 6.0, 291.0, 134.0, 497.0, 1321.0, 105.0, 7.0, 301.0, 641.0, 12.0, 1485.0, 2454.0, 4.0, 19.0, 1.0, 104.0, 92.0, 7.0, 7502.0, 26.0, 137.0, 41.0, 21.0, 6.0, 45.0, 31.0, 203.0, 2778.0, 31.0, 21.0, 1138.0, 15136.0, 353.0, 1.0, 57.0, 81.0, 37.0, 113.0, 96.0, 63.0, 58.0, 381.0, 391.0, 99.0, 156.0, 5.0, 371.0, 11536.0, 269.0, 32.0, 130.0, 7.0, 134.0, 20.0, 9.0, 5.0, 267.0, 26849.0, 38.0, 5.0, 42.0, 6933.0, 403.0, 156.0, 6.0, 68.0, 167.0, 132.0, 12.0, 10.0, 3.0, 72.0, 210.0, 99.0, 1893.0, 436.0, 3186.0, 91.0, 1515.0, 1131.0, 6557.0, 52.0, 1161.0, 40.0, 264.0, 29.0, 851.0, 289.0, 300.0, 69.0, 3.0, 48.0, 61.0, 17.0, 13.0, 21.0, 90.0, 234.0, 7.0, 14.0, 10.0, 235.0, 190.0, 58.0, 52.0, 2.0, 41.0, 1425.0, 222.0, 12.0, 13.0, 30.0, 281.0, 19.0, 3.0, 14.0, 1346.0, 89.0, 2.0, 69.0, 208.0, 107.0, 386.0, 144.0, 1292.0, 357.0, 5.0, 27.0, 3683.0, 538.0, 545.0, 1516.0, 957.0, 523.0, 7099.0, 22.0, 5.0, 6.0, 4.0, 54.0, 6.0, 1351.0, 87.0, 2379.0, 2.0, 20.0, 1426.0, 114.0, 70.0, 73.0, 354.0, 28.0, 9630.0, 65.0, 67.0, 3.0, 812.0, 1321.0, 6.0, 27.0, 15.0, 181.0, 188.0, 10.0, 14.0, 40.0, 82.0, 5138.0, 36188.0, 11.0, 578.0, 552.0, 8733.0, 48.0, 167.0, 29.0, 19.0, 66.0, 4.0, 5.0, 9.0, 8.0] . . corona_dataset_aggregated[&#39;max_infection_rate&#39;] = max_infection_rates . corona_dataset_aggregated.columns #Our new col is included in the end . Index([&#39;1/22/20&#39;, &#39;1/23/20&#39;, &#39;1/24/20&#39;, &#39;1/25/20&#39;, &#39;1/26/20&#39;, &#39;1/27/20&#39;, &#39;1/28/20&#39;, &#39;1/29/20&#39;, &#39;1/30/20&#39;, &#39;1/31/20&#39;, ... &#39;4/22/20&#39;, &#39;4/23/20&#39;, &#39;4/24/20&#39;, &#39;4/25/20&#39;, &#39;4/26/20&#39;, &#39;4/27/20&#39;, &#39;4/28/20&#39;, &#39;4/29/20&#39;, &#39;4/30/20&#39;, &#39;max_infection_rate&#39;], dtype=&#39;object&#39;, length=101) . corona_dataset_aggregated.index . Index([&#39;Afghanistan&#39;, &#39;Albania&#39;, &#39;Algeria&#39;, &#39;Andorra&#39;, &#39;Angola&#39;, &#39;Antigua and Barbuda&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;, &#39;Austria&#39;, ... &#39;United Kingdom&#39;, &#39;Uruguay&#39;, &#39;Uzbekistan&#39;, &#39;Venezuela&#39;, &#39;Vietnam&#39;, &#39;West Bank and Gaza&#39;, &#39;Western Sahara&#39;, &#39;Yemen&#39;, &#39;Zambia&#39;, &#39;Zimbabwe&#39;], dtype=&#39;object&#39;, name=&#39;Country/Region&#39;, length=187) . corona_dataset_aggregated.head() . 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 ... 4/22/20 4/23/20 4/24/20 4/25/20 4/26/20 4/27/20 4/28/20 4/29/20 4/30/20 max_infection_rate . Country/Region . Afghanistan 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1176 | 1279 | 1351 | 1463 | 1531 | 1703 | 1828 | 1939 | 2171 | 232.0 | . Albania 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 634 | 663 | 678 | 712 | 726 | 736 | 750 | 766 | 773 | 34.0 | . Algeria 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2910 | 3007 | 3127 | 3256 | 3382 | 3517 | 3649 | 3848 | 4006 | 199.0 | . Andorra 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 723 | 723 | 731 | 738 | 738 | 743 | 743 | 743 | 745 | 43.0 | . Angola 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 25 | 25 | 25 | 25 | 26 | 27 | 27 | 27 | 27 | 5.0 | . 5 rows × 101 columns . . corona_data = pd.DataFrame(corona_dataset_aggregated[&#39;max_infection_rate&#39;]) corona_data.head() . max_infection_rate . Country/Region . Afghanistan 232.0 | . Albania 34.0 | . Algeria 199.0 | . Andorra 43.0 | . Angola 5.0 | . print(len(corona_dataset_csv)) . 266 .",
            "url": "https://seifhh.github.io/DS_Projects/2021/05/28/Data-Analysis.html",
            "relUrl": "/2021/05/28/Data-Analysis.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://seifhh.github.io/DS_Projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Used Car  Data Analysis",
            "content": ". import pandas as pd #Two dimentional table import numpy as np #Array and metrices import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . path = (r&quot;D: Data Science Python IBM Data_Analysis automobileEDA.csv&quot;) df = pd.read_csv(path) . df.head(5) . symboling normalized-losses make aspiration num-of-doors body-style drive-wheels engine-location wheel-base length ... compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km horsepower-binned diesel gas . 0 3 | 122 | alfa-romero | std | two | convertible | rwd | front | 88.6 | 0.811148 | ... | 9.0 | 111.0 | 5000.0 | 21 | 27 | 13495.0 | 11.190476 | Medium | 0 | 1 | . 1 3 | 122 | alfa-romero | std | two | convertible | rwd | front | 88.6 | 0.811148 | ... | 9.0 | 111.0 | 5000.0 | 21 | 27 | 16500.0 | 11.190476 | Medium | 0 | 1 | . 2 1 | 122 | alfa-romero | std | two | hatchback | rwd | front | 94.5 | 0.822681 | ... | 9.0 | 154.0 | 5000.0 | 19 | 26 | 16500.0 | 12.368421 | Medium | 0 | 1 | . 3 2 | 164 | audi | std | four | sedan | fwd | front | 99.8 | 0.848630 | ... | 10.0 | 102.0 | 5500.0 | 24 | 30 | 13950.0 | 9.791667 | Medium | 0 | 1 | . 4 2 | 164 | audi | std | four | sedan | 4wd | front | 99.4 | 0.848630 | ... | 8.0 | 115.0 | 5500.0 | 18 | 22 | 17450.0 | 13.055556 | Medium | 0 | 1 | . 5 rows × 29 columns . Analyzing Individual Feature Patterns Using Visualization . df.dtypes . symboling int64 normalized-losses int64 make object aspiration object num-of-doors object body-style object drive-wheels object engine-location object wheel-base float64 length float64 width float64 height float64 curb-weight int64 engine-type object num-of-cylinders object engine-size int64 fuel-system object bore float64 stroke float64 compression-ratio float64 horsepower float64 peak-rpm float64 city-mpg int64 highway-mpg int64 price float64 city-L/100km float64 horsepower-binned object diesel int64 gas int64 dtype: object . df.dtypes[[&quot;peak-rpm&quot;, &quot;horsepower&quot;, &quot;engine-type&quot;]] . peak-rpm float64 horsepower float64 engine-type object dtype: object . Calculate the correlation between variables of type &quot;int64&quot; or &quot;float64&quot; using the method &quot;corr&quot;: . df.corr() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . symboling 1.000000 | 0.466264 | -0.535987 | -0.365404 | -0.242423 | -0.550160 | -0.233118 | -0.110581 | -0.140019 | -0.008245 | -0.182196 | 0.075819 | 0.279740 | -0.035527 | 0.036233 | -0.082391 | 0.066171 | -0.196735 | 0.196735 | . normalized-losses 0.466264 | 1.000000 | -0.056661 | 0.019424 | 0.086802 | -0.373737 | 0.099404 | 0.112360 | -0.029862 | 0.055563 | -0.114713 | 0.217299 | 0.239543 | -0.225016 | -0.181877 | 0.133999 | 0.238567 | -0.101546 | 0.101546 | . wheel-base -0.535987 | -0.056661 | 1.000000 | 0.876024 | 0.814507 | 0.590742 | 0.782097 | 0.572027 | 0.493244 | 0.158502 | 0.250313 | 0.371147 | -0.360305 | -0.470606 | -0.543304 | 0.584642 | 0.476153 | 0.307237 | -0.307237 | . length -0.365404 | 0.019424 | 0.876024 | 1.000000 | 0.857170 | 0.492063 | 0.880665 | 0.685025 | 0.608971 | 0.124139 | 0.159733 | 0.579821 | -0.285970 | -0.665192 | -0.698142 | 0.690628 | 0.657373 | 0.211187 | -0.211187 | . width -0.242423 | 0.086802 | 0.814507 | 0.857170 | 1.000000 | 0.306002 | 0.866201 | 0.729436 | 0.544885 | 0.188829 | 0.189867 | 0.615077 | -0.245800 | -0.633531 | -0.680635 | 0.751265 | 0.673363 | 0.244356 | -0.244356 | . height -0.550160 | -0.373737 | 0.590742 | 0.492063 | 0.306002 | 1.000000 | 0.307581 | 0.074694 | 0.180449 | -0.062704 | 0.259737 | -0.087027 | -0.309974 | -0.049800 | -0.104812 | 0.135486 | 0.003811 | 0.281578 | -0.281578 | . curb-weight -0.233118 | 0.099404 | 0.782097 | 0.880665 | 0.866201 | 0.307581 | 1.000000 | 0.849072 | 0.644060 | 0.167562 | 0.156433 | 0.757976 | -0.279361 | -0.749543 | -0.794889 | 0.834415 | 0.785353 | 0.221046 | -0.221046 | . engine-size -0.110581 | 0.112360 | 0.572027 | 0.685025 | 0.729436 | 0.074694 | 0.849072 | 1.000000 | 0.572609 | 0.209523 | 0.028889 | 0.822676 | -0.256733 | -0.650546 | -0.679571 | 0.872335 | 0.745059 | 0.070779 | -0.070779 | . bore -0.140019 | -0.029862 | 0.493244 | 0.608971 | 0.544885 | 0.180449 | 0.644060 | 0.572609 | 1.000000 | -0.055390 | 0.001263 | 0.566936 | -0.267392 | -0.582027 | -0.591309 | 0.543155 | 0.554610 | 0.054458 | -0.054458 | . stroke -0.008245 | 0.055563 | 0.158502 | 0.124139 | 0.188829 | -0.062704 | 0.167562 | 0.209523 | -0.055390 | 1.000000 | 0.187923 | 0.098462 | -0.065713 | -0.034696 | -0.035201 | 0.082310 | 0.037300 | 0.241303 | -0.241303 | . compression-ratio -0.182196 | -0.114713 | 0.250313 | 0.159733 | 0.189867 | 0.259737 | 0.156433 | 0.028889 | 0.001263 | 0.187923 | 1.000000 | -0.214514 | -0.435780 | 0.331425 | 0.268465 | 0.071107 | -0.299372 | 0.985231 | -0.985231 | . horsepower 0.075819 | 0.217299 | 0.371147 | 0.579821 | 0.615077 | -0.087027 | 0.757976 | 0.822676 | 0.566936 | 0.098462 | -0.214514 | 1.000000 | 0.107885 | -0.822214 | -0.804575 | 0.809575 | 0.889488 | -0.169053 | 0.169053 | . peak-rpm 0.279740 | 0.239543 | -0.360305 | -0.285970 | -0.245800 | -0.309974 | -0.279361 | -0.256733 | -0.267392 | -0.065713 | -0.435780 | 0.107885 | 1.000000 | -0.115413 | -0.058598 | -0.101616 | 0.115830 | -0.475812 | 0.475812 | . city-mpg -0.035527 | -0.225016 | -0.470606 | -0.665192 | -0.633531 | -0.049800 | -0.749543 | -0.650546 | -0.582027 | -0.034696 | 0.331425 | -0.822214 | -0.115413 | 1.000000 | 0.972044 | -0.686571 | -0.949713 | 0.265676 | -0.265676 | . highway-mpg 0.036233 | -0.181877 | -0.543304 | -0.698142 | -0.680635 | -0.104812 | -0.794889 | -0.679571 | -0.591309 | -0.035201 | 0.268465 | -0.804575 | -0.058598 | 0.972044 | 1.000000 | -0.704692 | -0.930028 | 0.198690 | -0.198690 | . price -0.082391 | 0.133999 | 0.584642 | 0.690628 | 0.751265 | 0.135486 | 0.834415 | 0.872335 | 0.543155 | 0.082310 | 0.071107 | 0.809575 | -0.101616 | -0.686571 | -0.704692 | 1.000000 | 0.789898 | 0.110326 | -0.110326 | . city-L/100km 0.066171 | 0.238567 | 0.476153 | 0.657373 | 0.673363 | 0.003811 | 0.785353 | 0.745059 | 0.554610 | 0.037300 | -0.299372 | 0.889488 | 0.115830 | -0.949713 | -0.930028 | 0.789898 | 1.000000 | -0.241282 | 0.241282 | . diesel -0.196735 | -0.101546 | 0.307237 | 0.211187 | 0.244356 | 0.281578 | 0.221046 | 0.070779 | 0.054458 | 0.241303 | 0.985231 | -0.169053 | -0.475812 | 0.265676 | 0.198690 | 0.110326 | -0.241282 | 1.000000 | -1.000000 | . gas 0.196735 | 0.101546 | -0.307237 | -0.211187 | -0.244356 | -0.281578 | -0.221046 | -0.070779 | -0.054458 | -0.241303 | -0.985231 | 0.169053 | 0.475812 | -0.265676 | -0.198690 | -0.110326 | 0.241282 | -1.000000 | 1.000000 | . Let&#39;s find the correlation between the following columns: bore, stroke, compression-ratio, and horsepower. . df[[&#39;bore&#39;, &#39;stroke&#39;, &#39;compression-ratio&#39;, &#39;horsepower&#39;]].corr() . bore stroke compression-ratio horsepower . bore 1.000000 | -0.055390 | 0.001263 | 0.566936 | . stroke -0.055390 | 1.000000 | 0.187923 | 0.098462 | . compression-ratio 0.001263 | 0.187923 | 1.000000 | -0.214514 | . horsepower 0.566936 | 0.098462 | -0.214514 | 1.000000 | . #Fitted regression line on the data #Calculate linear relationship between engine-size and pirce . sns.regplot(x = &quot;engine-size&quot;, y = &quot;price&quot;, data = df) plt.ylim(0,) . (0.0, 53653.66212817468) . As the slope of the regression line is positive, this correlation indicate there exist positive relationship between engine-size and price. The higher the engine-size, the higher the price. . df[[&quot;engine-size&quot;, &quot;price&quot;]].corr() . engine-size price . engine-size 1.000000 | 0.872335 | . price 0.872335 | 1.000000 | . . sns.regplot(x = &quot;highway-mpg&quot;, y = &quot;price&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;highway-mpg&#39;, ylabel=&#39;price&#39;&gt; . As highway-mpg goes up, the price goes down: this indicates an inverse/negative relationship between these two variables. Highway mpg could potentially be a predictor of price. Let&#39;s examine the correlation. . df[[&quot;highway-mpg&quot;,&quot;price&quot;]].corr() . highway-mpg price . highway-mpg 1.000000 | -0.704692 | . price -0.704692 | 1.000000 | . As we can see, the correlation between &#39;highway-mpg&#39; and &#39;price&#39; and see it&#39;s approximately -0.704. . . sns.regplot(x = &quot;peak-rpm&quot;, y = &quot;price&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;peak-rpm&#39;, ylabel=&#39;price&#39;&gt; . Peak rpm does not seem like a good predictor of the price at all since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore, it&#39;s not a reliable variable. This is a weak linear relationship. The slope of the regression line is almost the same(zero) at all point and it can&#39;t be used to predictive analysis. . df[[&#39;peak-rpm&#39;,&#39;price&#39;]].corr() . peak-rpm price . peak-rpm 1.000000 | -0.101616 | . price -0.101616 | 1.000000 | . sns.regplot(x = &#39;stroke&#39;, y = &#39;price&#39;, data = df) . &lt;AxesSubplot:xlabel=&#39;stroke&#39;, ylabel=&#39;price&#39;&gt; . df[[&#39;stroke&#39;,&#39;price&#39;]].corr() . stroke price . stroke 1.00000 | 0.08231 | . price 0.08231 | 1.00000 | . . Let&#39;s examine categorical variables. The categorical variables can have the type &quot;object&quot; or &quot;int64&quot;. Best way to visualize this is using boxplot. . sns.boxplot(x=&quot;body-style&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;body-style&#39;, ylabel=&#39;price&#39;&gt; . We see that the distributions of price between the different body-style categories have a significant overlap, so body-style would not be a good predictor of price. Let&#39;s examine engine &quot;engine-location&quot; and &quot;price&quot; . sns.boxplot(x=&quot;engine-location&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;engine-location&#39;, ylabel=&#39;price&#39;&gt; . Here we see that the distribution of price between these two engine-location categories, front and rear, are distinct enough to take engine-location as a potential good predictor of price. . Let&#39;s examine &quot;drive-wheels&quot; and &quot;price&quot; . sns.boxplot(x=&quot;drive-wheels&quot;, y=&quot;price&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;drive-wheels&#39;, ylabel=&#39;price&#39;&gt; . Here we see that the distribution of price between the different drive-wheels categories differs. As such, drive-wheels could potentially be a predictor of price. . Let&#39;s compute descriptive Statistical Analysis. NaN value will be skipped when we do descriptive analysis. . df.describe() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . count 201.000000 | 201.00000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 197.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | 201.000000 | . mean 0.840796 | 122.00000 | 98.797015 | 0.837102 | 0.915126 | 53.766667 | 2555.666667 | 126.875622 | 3.330692 | 3.256904 | 10.164279 | 103.405534 | 5117.665368 | 25.179104 | 30.686567 | 13207.129353 | 9.944145 | 0.099502 | 0.900498 | . std 1.254802 | 31.99625 | 6.066366 | 0.059213 | 0.029187 | 2.447822 | 517.296727 | 41.546834 | 0.268072 | 0.319256 | 4.004965 | 37.365700 | 478.113805 | 6.423220 | 6.815150 | 7947.066342 | 2.534599 | 0.300083 | 0.300083 | . min -2.000000 | 65.00000 | 86.600000 | 0.678039 | 0.837500 | 47.800000 | 1488.000000 | 61.000000 | 2.540000 | 2.070000 | 7.000000 | 48.000000 | 4150.000000 | 13.000000 | 16.000000 | 5118.000000 | 4.795918 | 0.000000 | 0.000000 | . 25% 0.000000 | 101.00000 | 94.500000 | 0.801538 | 0.890278 | 52.000000 | 2169.000000 | 98.000000 | 3.150000 | 3.110000 | 8.600000 | 70.000000 | 4800.000000 | 19.000000 | 25.000000 | 7775.000000 | 7.833333 | 0.000000 | 1.000000 | . 50% 1.000000 | 122.00000 | 97.000000 | 0.832292 | 0.909722 | 54.100000 | 2414.000000 | 120.000000 | 3.310000 | 3.290000 | 9.000000 | 95.000000 | 5125.369458 | 24.000000 | 30.000000 | 10295.000000 | 9.791667 | 0.000000 | 1.000000 | . 75% 2.000000 | 137.00000 | 102.400000 | 0.881788 | 0.925000 | 55.500000 | 2926.000000 | 141.000000 | 3.580000 | 3.410000 | 9.400000 | 116.000000 | 5500.000000 | 30.000000 | 34.000000 | 16500.000000 | 12.368421 | 0.000000 | 1.000000 | . max 3.000000 | 256.00000 | 120.900000 | 1.000000 | 1.000000 | 59.800000 | 4066.000000 | 326.000000 | 3.940000 | 4.170000 | 23.000000 | 262.000000 | 6600.000000 | 49.000000 | 54.000000 | 45400.000000 | 18.076923 | 1.000000 | 1.000000 | . df.describe(include=[&#39;object&#39;]) . make aspiration num-of-doors body-style drive-wheels engine-location engine-type num-of-cylinders fuel-system horsepower-binned . count 201 | 201 | 201 | 201 | 201 | 201 | 201 | 201 | 201 | 200 | . unique 22 | 2 | 2 | 5 | 3 | 2 | 6 | 7 | 8 | 3 | . top toyota | std | four | sedan | fwd | front | ohc | four | mpfi | Low | . freq 32 | 165 | 115 | 94 | 118 | 198 | 145 | 157 | 92 | 115 | . Value counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the &quot;value_counts&quot; method on the column &quot;drive-wheels&quot;. N.B, the method &quot;value_counts&quot; only works on pandas series, not pandas dataframes. As a result, we only include one bracket df[&#39;drive-wheels&#39;], not two brackets df[[&#39;drive-wheels&#39;]]. . df[&#39;drive-wheels&#39;].value_counts() . fwd 118 rwd 75 4wd 8 Name: drive-wheels, dtype: int64 . df[&#39;drive-wheels&#39;].value_counts().to_frame() . drive-wheels . fwd 118 | . rwd 75 | . 4wd 8 | . . drive_wheels_counts = df[&#39;drive-wheels&#39;].value_counts().to_frame() drive_wheels_counts.rename(columns={&#39;drive-wheels&#39;: &#39;value_counts&#39;}, inplace=True) drive_wheels_counts . value_counts . fwd 118 | . rwd 75 | . 4wd 8 | . . drive_wheels_counts.index.name = &quot;drive-wheels&quot; drive_wheels_counts . value_counts . drive-wheels . fwd 118 | . rwd 75 | . 4wd 8 | . . engine_loc_counts = df[&#39;engine-location&#39;].value_counts().to_frame() engine_loc_counts.rename(columns={&#39;engine-location&#39;: &#39;value_counts&#39;}, inplace=True) engine_loc_counts.index.name = &#39;engine-location&#39; engine_loc_counts.head() . value_counts . engine-location . front 198 | . rear 3 | . After examining the value counts of the engine location, we see that engine location would not be a good predictor variable for the price. This is because we only have three cars with a rear engine and 198 with an engine in the front, so this result is skewed. Thus, we are not able to draw any conclusions about the engine location. . #Groupby method groups data into different categories. . df[&#39;drive-wheels&#39;].unique() . array([&#39;rwd&#39;, &#39;fwd&#39;, &#39;4wd&#39;], dtype=object) . . . df_group_one = df[[&#39;drive-wheels&#39;,&#39;body-style&#39;,&#39;price&#39;]] df_group_one.head(10) . drive-wheels body-style price . 0 rwd | convertible | 13495.0 | . 1 rwd | convertible | 16500.0 | . 2 rwd | hatchback | 16500.0 | . 3 fwd | sedan | 13950.0 | . 4 4wd | sedan | 17450.0 | . 5 fwd | sedan | 15250.0 | . 6 fwd | sedan | 17710.0 | . 7 fwd | wagon | 18920.0 | . 8 fwd | sedan | 23875.0 | . 9 rwd | sedan | 16430.0 | . df_group_one = df_group_one.groupby([&#39;drive-wheels&#39;],as_index=False).mean() df_group_one . drive-wheels price . 0 4wd | 10241.000000 | . 1 fwd | 9244.779661 | . 2 rwd | 19757.613333 | . From our data, it seems rear-wheel drive vehicles are, on average, the most expensive, while 4-wheel and front-wheel are approximately the same in price. . df_gptest = df[[&#39;drive-wheels&#39;,&#39;body-style&#39;,&#39;price&#39;]] grouped_data = df_gptest.groupby([&#39;drive-wheels&#39;,&#39;body-style&#39;],as_index=False).mean() grouped_data . drive-wheels body-style price . 0 4wd | hatchback | 7603.000000 | . 1 4wd | sedan | 12647.333333 | . 2 4wd | wagon | 9095.750000 | . 3 fwd | convertible | 11595.000000 | . 4 fwd | hardtop | 8249.000000 | . 5 fwd | hatchback | 8396.387755 | . 6 fwd | sedan | 9811.800000 | . 7 fwd | wagon | 9997.333333 | . 8 rwd | convertible | 23949.600000 | . 9 rwd | hardtop | 24202.714286 | . 10 rwd | hatchback | 14337.777778 | . 11 rwd | sedan | 21711.833333 | . 12 rwd | wagon | 16994.222222 | . . grouped_data_pivoted = grouped_data.pivot(index=&#39;drive-wheels&#39;,columns=&#39;body-style&#39;) grouped_data_pivoted . price . body-style convertible hardtop hatchback sedan wagon . drive-wheels . 4wd NaN | NaN | 7603.000000 | 12647.333333 | 9095.750000 | . fwd 11595.0 | 8249.000000 | 8396.387755 | 9811.800000 | 9997.333333 | . rwd 23949.6 | 24202.714286 | 14337.777778 | 21711.833333 | 16994.222222 | . grouped_data_pivoted = grouped_data_pivoted.fillna(0) grouped_data_pivoted . price . body-style convertible hardtop hatchback sedan wagon . drive-wheels . 4wd 0.0 | 0.000000 | 7603.000000 | 12647.333333 | 9095.750000 | . fwd 11595.0 | 8249.000000 | 8396.387755 | 9811.800000 | 9997.333333 | . rwd 23949.6 | 24202.714286 | 14337.777778 | 21711.833333 | 16994.222222 | . df_avg_bodystyle = df[[&#39;body-style&#39;,&#39;price&#39;]] grouped_data_bodystyle = df_avg_bodystyle.groupby([&#39;body-style&#39;],as_index= False).mean() grouped_data_bodystyle . body-style price . 0 convertible | 21890.500000 | . 1 hardtop | 22208.500000 | . 2 hatchback | 9957.441176 | . 3 sedan | 14459.755319 | . 4 wagon | 12371.960000 | . Let&#39;s use a heat map to visualize the relationship between Body Style vs Price. . fig, ax = plt.subplots() im = ax.pcolor(grouped_data_pivoted, cmap=&#39;RdBu&#39;) #label names row_labels = grouped_data_pivoted.columns.levels[1] col_labels = grouped_data_pivoted.index #move ticks and labels to the center ax.set_xticks(np.arange(grouped_data_pivoted.shape[1]) + 0.5, minor=False) ax.set_yticks(np.arange(grouped_data_pivoted.shape[0]) + 0.5, minor=False) #insert labels ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(col_labels, minor=False) #rotate label if too long plt.xticks(rotation=90) fig.colorbar(im) plt.show() . The heatmap plots the target variable (price) proportional to colour with respect to the variables &#39;drive-wheel&#39; and &#39;body-style&#39; on the vertical and horizontal axis, respectively. This allows us to visualize how the price is related to &#39;drive-wheel&#39; and &#39;body-style. . . df.corr() . symboling normalized-losses wheel-base length width height curb-weight engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price city-L/100km diesel gas . symboling 1.000000 | 0.466264 | -0.535987 | -0.365404 | -0.242423 | -0.550160 | -0.233118 | -0.110581 | -0.140019 | -0.008245 | -0.182196 | 0.075819 | 0.279740 | -0.035527 | 0.036233 | -0.082391 | 0.066171 | -0.196735 | 0.196735 | . normalized-losses 0.466264 | 1.000000 | -0.056661 | 0.019424 | 0.086802 | -0.373737 | 0.099404 | 0.112360 | -0.029862 | 0.055563 | -0.114713 | 0.217299 | 0.239543 | -0.225016 | -0.181877 | 0.133999 | 0.238567 | -0.101546 | 0.101546 | . wheel-base -0.535987 | -0.056661 | 1.000000 | 0.876024 | 0.814507 | 0.590742 | 0.782097 | 0.572027 | 0.493244 | 0.158502 | 0.250313 | 0.371147 | -0.360305 | -0.470606 | -0.543304 | 0.584642 | 0.476153 | 0.307237 | -0.307237 | . length -0.365404 | 0.019424 | 0.876024 | 1.000000 | 0.857170 | 0.492063 | 0.880665 | 0.685025 | 0.608971 | 0.124139 | 0.159733 | 0.579821 | -0.285970 | -0.665192 | -0.698142 | 0.690628 | 0.657373 | 0.211187 | -0.211187 | . width -0.242423 | 0.086802 | 0.814507 | 0.857170 | 1.000000 | 0.306002 | 0.866201 | 0.729436 | 0.544885 | 0.188829 | 0.189867 | 0.615077 | -0.245800 | -0.633531 | -0.680635 | 0.751265 | 0.673363 | 0.244356 | -0.244356 | . height -0.550160 | -0.373737 | 0.590742 | 0.492063 | 0.306002 | 1.000000 | 0.307581 | 0.074694 | 0.180449 | -0.062704 | 0.259737 | -0.087027 | -0.309974 | -0.049800 | -0.104812 | 0.135486 | 0.003811 | 0.281578 | -0.281578 | . curb-weight -0.233118 | 0.099404 | 0.782097 | 0.880665 | 0.866201 | 0.307581 | 1.000000 | 0.849072 | 0.644060 | 0.167562 | 0.156433 | 0.757976 | -0.279361 | -0.749543 | -0.794889 | 0.834415 | 0.785353 | 0.221046 | -0.221046 | . engine-size -0.110581 | 0.112360 | 0.572027 | 0.685025 | 0.729436 | 0.074694 | 0.849072 | 1.000000 | 0.572609 | 0.209523 | 0.028889 | 0.822676 | -0.256733 | -0.650546 | -0.679571 | 0.872335 | 0.745059 | 0.070779 | -0.070779 | . bore -0.140019 | -0.029862 | 0.493244 | 0.608971 | 0.544885 | 0.180449 | 0.644060 | 0.572609 | 1.000000 | -0.055390 | 0.001263 | 0.566936 | -0.267392 | -0.582027 | -0.591309 | 0.543155 | 0.554610 | 0.054458 | -0.054458 | . stroke -0.008245 | 0.055563 | 0.158502 | 0.124139 | 0.188829 | -0.062704 | 0.167562 | 0.209523 | -0.055390 | 1.000000 | 0.187923 | 0.098462 | -0.065713 | -0.034696 | -0.035201 | 0.082310 | 0.037300 | 0.241303 | -0.241303 | . compression-ratio -0.182196 | -0.114713 | 0.250313 | 0.159733 | 0.189867 | 0.259737 | 0.156433 | 0.028889 | 0.001263 | 0.187923 | 1.000000 | -0.214514 | -0.435780 | 0.331425 | 0.268465 | 0.071107 | -0.299372 | 0.985231 | -0.985231 | . horsepower 0.075819 | 0.217299 | 0.371147 | 0.579821 | 0.615077 | -0.087027 | 0.757976 | 0.822676 | 0.566936 | 0.098462 | -0.214514 | 1.000000 | 0.107885 | -0.822214 | -0.804575 | 0.809575 | 0.889488 | -0.169053 | 0.169053 | . peak-rpm 0.279740 | 0.239543 | -0.360305 | -0.285970 | -0.245800 | -0.309974 | -0.279361 | -0.256733 | -0.267392 | -0.065713 | -0.435780 | 0.107885 | 1.000000 | -0.115413 | -0.058598 | -0.101616 | 0.115830 | -0.475812 | 0.475812 | . city-mpg -0.035527 | -0.225016 | -0.470606 | -0.665192 | -0.633531 | -0.049800 | -0.749543 | -0.650546 | -0.582027 | -0.034696 | 0.331425 | -0.822214 | -0.115413 | 1.000000 | 0.972044 | -0.686571 | -0.949713 | 0.265676 | -0.265676 | . highway-mpg 0.036233 | -0.181877 | -0.543304 | -0.698142 | -0.680635 | -0.104812 | -0.794889 | -0.679571 | -0.591309 | -0.035201 | 0.268465 | -0.804575 | -0.058598 | 0.972044 | 1.000000 | -0.704692 | -0.930028 | 0.198690 | -0.198690 | . price -0.082391 | 0.133999 | 0.584642 | 0.690628 | 0.751265 | 0.135486 | 0.834415 | 0.872335 | 0.543155 | 0.082310 | 0.071107 | 0.809575 | -0.101616 | -0.686571 | -0.704692 | 1.000000 | 0.789898 | 0.110326 | -0.110326 | . city-L/100km 0.066171 | 0.238567 | 0.476153 | 0.657373 | 0.673363 | 0.003811 | 0.785353 | 0.745059 | 0.554610 | 0.037300 | -0.299372 | 0.889488 | 0.115830 | -0.949713 | -0.930028 | 0.789898 | 1.000000 | -0.241282 | 0.241282 | . diesel -0.196735 | -0.101546 | 0.307237 | 0.211187 | 0.244356 | 0.281578 | 0.221046 | 0.070779 | 0.054458 | 0.241303 | 0.985231 | -0.169053 | -0.475812 | 0.265676 | 0.198690 | 0.110326 | -0.241282 | 1.000000 | -1.000000 | . gas 0.196735 | 0.101546 | -0.307237 | -0.211187 | -0.244356 | -0.281578 | -0.221046 | -0.070779 | -0.054458 | -0.241303 | -0.985231 | 0.169053 | 0.475812 | -0.265676 | -0.198690 | -0.110326 | 0.241282 | -1.000000 | 1.000000 | . Let&#39;s calculate the significant of correlation(P-value) here. Remember, the P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant. p-value is &lt; 0.001: we say there is strong evidence that the correlation is significant. p-value is &lt; 0.05: there is moderate evidence that the correlation is significant. p-value is &lt; 0.1: there is weak evidence that the correlation is significant. p-value is &gt; 0.1: there is no evidence that the correlation is significant. . from scipy import stats . pearson_coef, p_value = stats.pearsonr(df[&#39;wheel-base&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.5846418222655081 with a P-value of P = 8.076488270732989e-20 . Since the p-value is &lt; 0.001, the correlation between wheel-base and price is statistically significant, although the linear relationship isn&#39;t extremely strong (~0.585). . pearson_coef, p_value = stats.pearsonr(df[&#39;horsepower&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.809574567003656 with a P-value of P = 6.369057428259557e-48 . Since the p-value is &lt; 0.001, the correlation between horsepower and price is statistically significant, and the linear relationship is quite strong (~0.809, close to 1). . pearson_coef, p_value = stats.pearsonr(df[&#39;length&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.690628380448364 with a P-value of P = 8.016477466158986e-30 . Since the p-value is &lt; 0.001, the correlation between length and price is statistically significant, and the linear relationship is moderately strong (~0.691). . pearson_coef, p_value = stats.pearsonr(df[&#39;width&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value ) . The Pearson Correlation Coefficient is 0.7512653440522674 with a P-value of P = 9.200335510481516e-38 . Since the p-value is &lt; 0.001, the correlation between width and price is statistically significant, and the linear relationship is quite strong (~0.751). . pearson_coef, p_value = stats.pearsonr(df[&#39;curb-weight&#39;], df[&#39;price&#39;]) print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is 0.8344145257702846 with a P-value of P = 2.1895772388936914e-53 . Since the p-value is &lt; 0.001, the correlation between curb-weight and price is statistically significant, and the linear relationship is quite strong (~0.834). . pearson_coef, p_value = stats.pearsonr(df[&#39;engine-size&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.8723351674455185 with a P-value of P = 9.265491622198389e-64 . Since the p-value is &lt; 0.001, the correlation between engine-size and price is statistically significant, and the linear relationship is very strong (~0.872). . pearson_coef, p_value = stats.pearsonr(df[&#39;bore&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value ) . The Pearson Correlation Coefficient is 0.5431553832626602 with a P-value of P = 8.049189483935489e-17 . Since the p-value is &lt; 0.001, the correlation between bore and price is statistically significant, but the linear relationship is only moderate (~0.521). . pearson_coef, p_value = stats.pearsonr(df[&#39;city-mpg&#39;], df[&#39;price&#39;]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value) . The Pearson Correlation Coefficient is -0.6865710067844677 with a P-value of P = 2.321132065567674e-29 . Since the p-value is &lt; 0.001, the correlation between city-mpg and price is statistically significant, and the coefficient of about -0.687 shows that the relationship is negative and moderately strong. . pearson_coef, p_value = stats.pearsonr(df[&#39;highway-mpg&#39;], df[&#39;price&#39;]) print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value ) . The Pearson Correlation Coefficient is -0.7046922650589529 with a P-value of P = 1.7495471144477352e-31 . Since the p-value is &lt; 0.001, the correlation between highway-mpg and price is statistically significant, and the coefficient of about -0.705 shows that the relationship is negative and moderately strong. . We can use ANOVA(Analysis of Variance) to test whether there are significant differences between the means of two more groups.F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means. . P-value:P-value tells how statistically significant our calculated score value is. If our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to return a sizeable F-test score and a small p-value. . grouped_test2=df_gptest[[&#39;drive-wheels&#39;, &#39;price&#39;]].groupby([&#39;drive-wheels&#39;]) grouped_test2.head(2) . drive-wheels price . 0 rwd | 13495.0 | . 1 rwd | 16500.0 | . 3 fwd | 13950.0 | . 4 4wd | 17450.0 | . 5 fwd | 15250.0 | . 136 4wd | 7603.0 | . df_gptest . drive-wheels body-style price . 0 rwd | convertible | 13495.0 | . 1 rwd | convertible | 16500.0 | . 2 rwd | hatchback | 16500.0 | . 3 fwd | sedan | 13950.0 | . 4 4wd | sedan | 17450.0 | . ... ... | ... | ... | . 196 rwd | sedan | 16845.0 | . 197 rwd | sedan | 19045.0 | . 198 rwd | sedan | 21485.0 | . 199 rwd | sedan | 22470.0 | . 200 rwd | sedan | 22625.0 | . 201 rows × 3 columns . grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;] . 4 17450.0 136 7603.0 140 9233.0 141 11259.0 144 8013.0 145 11694.0 150 7898.0 151 8778.0 Name: price, dtype: float64 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 67.95406500780399 , P = 3.3945443577151245e-23 . This is a great result with a large F-test score showing a strong correlation and a P-value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated? . Let&#39;s examine them separately. . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val ) . ANOVA results: F= 130.5533160959111 , P = 2.2355306355677845e-23 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;]) print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 8.580681368924756 , P = 0.004411492211225333 . f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;]) print(&quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) . ANOVA results: F= 0.665465750252303 , P = 0.41620116697845666 . We now have a better idea of what our data looks like and which variables are important to take into account when predicting the car price. We have narrowed it down to the following variables: . Continuous numerical variables: . Length Width Curb-weight Engine-size Horsepower City-mpg Highway-mpg Wheel-base Bore Categorical variables: . Drive-wheels As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model&#39;s prediction performance. .",
            "url": "https://seifhh.github.io/DS_Projects/2020/01/28/Used-Car-Data-Analysis-with-Python.html",
            "relUrl": "/2020/01/28/Used-Car-Data-Analysis-with-Python.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://seifhh.github.io/DS_Projects/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "An Electrical Engineer, passionate about the field of Data Science and Blockchain Technology. .",
          "url": "https://seifhh.github.io/DS_Projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://seifhh.github.io/DS_Projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}